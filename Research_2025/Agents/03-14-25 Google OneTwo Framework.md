We just released ð—¢ð—»ð—²ð—§ð˜„ð—¼ ð˜ƒðŸ¬.ðŸ®.ðŸ­ ([https://lnkd.in/dYMkGVKT](https://lnkd.in/dYMkGVKT))!
 
This is the latest version of our open-source Python library for interacting with large language models like Gemini and ChatGPT. If you're not familiar with OneTwo, it has many commonalities with other frameworks for LLM prompting and orchestration, but is particularly optimized for the research use case. We use it heavily, for example, in our own research team in Google DeepMind. A key design motivation is that programming with LLMs should be as easy and flexible as ordinary Python programming. The core of OneTwo thus provides a set of function decorators and related libraries that hide under the hood a lot of the complexities of dealing with LLMs (threading, batching, tracing, differences in APIs between different models, etc.) so that you don't need to worry about them. We then provide a "standard library" of clean implementations of various reusable and composable techniques from the published literature.
 
If you are an AI researcher or practitioner, or a Python programmer who is interested in learning more about programming with LLMs, you may be interested in checking out our tutorial colab ([https://lnkd.in/dibwczMC](https://lnkd.in/dibwczMC)), which guides you step-by-step through many of the well-known techniques, starting from individual LLM calls, chain-of-thought, few-shot prompting, and other basics, up through agentic strategies like ReAct and Python Planning, and ensembling approaches like self-consistency. In the latest version (v0.2.1), we added examples of multimodal prompting (e.g., for QA about the contents of an image) and best practices for dealing with LLMs trained on chat semantics (the most common paradigm currently) vs. traditional LLMs based on text completion semantics.
 
The original version of OneTwo was implemented by Olivier Bousquet, Ilya Tolstikhin, Nathanael SchÃ¤rli and myself. For the latest version, I would like to give a special shout-out to Sachit Menon, who contributed significantly to strengthening the multimodal capabilities of OneTwo, including enabling the ReAct and Python Planning agents to support multimodal inputs. Many thanks also to Aleh Lishayou, Bhavesh Jain, Cezary Czernecki, Delia Georgiana S., Gil Fidel, Haolin Yu, Hootan Nakhost, Jie Bu,Parth Kothari, Pete Aykroyd, and Rui Zhang for their contributions to v0.2.1, as well as to the many people who contributed to earlier versions of the library!
 \> From \<[https://www.linkedin.com/my-items/saved-posts/](https://www.linkedin.com/my-items/saved-posts/)\>