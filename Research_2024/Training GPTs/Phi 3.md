Microsoft has launched the Phi-3 series, which includes three sizes: mini (3.8 billion parameters), small (7 billion parameters), and medium (14 billion parameters). These models are designed to run efficiently on both mobile devices and PCs, using advanced datasets to achieve high performance.
 
**Technical Details and Performance:**

- **Architecture:** All models feature a transformer decoder architecture.
- **Performance:** The mini model achieves 69% on the MMLU and 8.38 on MT-bench, showing performance on par with larger models such as Mixtral 8x7B and GPT-3.5.
- **Context Support:** Supports a default 4K context length, expandable to 128K through LongRope technology.
 \> From \<[https://mail.google.com/mail/u/0/#inbox/FMfcgzGxStqWptTKcgffFTqpHQCLclCM](https://mail.google.com/mail/u/0/#inbox/FMfcgzGxStqWptTKcgffFTqpHQCLclCM)\>