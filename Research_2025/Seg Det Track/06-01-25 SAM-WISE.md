SAM2 is great at video object tracking using visual prompts, but it does not understand text. In my demos, I often showed how combining SAM2 with VLMs enabled language-guided image segmentation, but SAMWISE takes this even further by allowing direct text-driven video object segmentation without large VLMs or extra fine-tuning.
 
Developed by Claudia Cuttano, Gabriele Trivigno, Gabriele Rosi, Carlo Masone, and Giuseppe Averta, SAMWISE adds language understanding and temporal reasoning to SAM2.
 
Now you can segment and track objects in videos just by describing them, for example "the person in red who enters from the left." The model follows your prompt, tracks the right object as it appears, and can even fix its own tracking mistakes.
 
SAMWISE achieves this by adding a lightweight adapter that lets SAM2 combine visual and language information and model changes over time. It can recognize when it is tracking the wrong object and automatically switch focus to the correct one, even during occlusions or complex motion. All this works efficiently without retraining SAM2 or using external models, making it practical for real-world, long video scenarios.
 
â®‘ ðŸ”— top CVPR 2025 papers: [https://lnkd.in/dbNRXHW2](https://lnkd.in/dbNRXHW2)
 
Links to the paper, code and poster are in the comments below. ðŸ‘‡ðŸ»
 
#cvpr2025 #computervision #deeplearning #opensource
 \> From \<[https://www.linkedin.com/my-items/saved-posts/](https://www.linkedin.com/my-items/saved-posts/)\>