ZeroGrasp: Zero-Shot Shape Reconstruction Enabled Robotic Grasping  
ðŸ“„ Paper: [https://lnkd.in/gqicnFFt](https://lnkd.in/gqicnFFt)  
ðŸ’» Code: [https://lnkd.in/g9QA7-u3](https://lnkd.in/g9QA7-u3)  
ðŸŒ Website: [https://sh8.io/#/zerograsp](https://sh8.io/#/zerograsp)
 
We propose a unified framework for 3D shape reconstruction and grasp pose prediction from a single RGB-D image â€” designed to generalize to unseen objects in the wild. And yes, ZeroGrasp sets a new state-of-the-art on the GraspNet-1B benchmark ðŸ†
 
As part of this work, weâ€™re also releasing:  
âœ¨ ZeroGrasp-11B dataset â€“ a large-scale synthetic dataset for grasp prediction and reconstruction, built from 12K 3D models in the Objaverse-LVIS dataset  
âœ¨ ReOcS dataset â€“ a benchmark for evaluating 3D reconstruction under different levels of occlusion
 
ðŸ“ Join us at our poster session on Saturday, June 14, from 5-7 p.m. If youâ€™re working on 3D vision, grasping, or generalizable robotics, Iâ€™d love to discuss and connect!
 
This project was made possible through a fantastic collaboration with TRI and Woven by Toyota â€” huge thanks for the support! ðŸ™  
Sergey Zakharov Katherine Liu Zubair Irshad, PhD Vitor Guizilini Robert Lee Takuya Ikeda Koichi Nishiwaki Kris Kitani RareÈ™ AmbruÈ™
 
Also, Iâ€™ll be on the job market starting this October â€” feel free to reach out!
 \> From \<[https://www.linkedin.com/my-items/saved-posts/](https://www.linkedin.com/my-items/saved-posts/)\>