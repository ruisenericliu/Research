ðŸš€ Thrilled to share our latest work: PointArena: Probing Multimodal Grounding Through Language-Guided Pointing.
 
Pointing is one of the most intuitive ways humans ground language in visual contextâ€”yet it's often overlooked in multimodal evaluation. PointArena is the first unified benchmark that systematically evaluates how well multimodal models can point based on language.
 
ðŸ” Whatâ€™s in PointArena?  
Point-Bench: 982 language-image pairs across 5 reasoning typesâ€”spatial, affordance, counting, steerability, and abstract reasoning.
 
Point-Battle: A live, crowd-sourced arena with 4,500+ human votes comparing model predictions head-to-head.
 
Point-Act: A real-world robot manipulation setup to test if models can turn pointing into action.
 
ðŸŽ¯ Key Findings  
Open models like Molmo-72B outperform proprietary giants like GPT-4o and Gemini.
 
Pointing-specific supervision yields major performance gains.
 
Static benchmark accuracy strongly correlates with human preference and real-world success.
 
Bigger â‰  better â€” model size alone has limited impact on pointing accuracy.
 
ðŸ§  Why it matters  
Pointing isn't just a benchmark taskâ€”itâ€™s fundamental for robotics, assistive tech, and spatially grounded AI.
 
PointArena provides a scalable, rigorous way to evaluate these capabilities.
 
ðŸ‘ Big thanks to our incredible team at UW and AI2 for making this possible!
 
ðŸ“‚ Weâ€™ve open-sourced everything:  
Paper: [https://lnkd.in/g-u8u46R](https://lnkd.in/g-u8u46R)
 
Code: [https://lnkd.in/gAPC3sj5](https://lnkd.in/gAPC3sj5)
 
Dataset: [https://lnkd.in/gqnaFWmY](https://lnkd.in/gqnaFWmY)
 
Point-Battle: tinyurl.com/Point-Battle
 
#PointArena #MLLM #MultimodalAI #VisionLanguageModels #VisualGrounding #Robotics #Benchmarking #OpenSourceAI
 \> From \<[https://www.linkedin.com/my-items/saved-posts/](https://www.linkedin.com/my-items/saved-posts/)\>