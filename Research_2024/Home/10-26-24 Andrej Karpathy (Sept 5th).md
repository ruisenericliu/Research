[No Priors Ep. 80 | With Andrej Karpathy from OpenAI and Tesla](https://m.youtube.com/watch?v=hM_h0UA7upI)
 
We've reached "AGI" for self-driving via Waymo
 
Waymo, 2014 was a perfect demo drive, and now is a product; but globalization hasn't happened
 
Thinks personally Tesla is ahead of Waymo  
Tesla has a software problem; Waymo has a hardware problem (scaling)
 
Tesla uses expensive sensors during training time, but distilling to vision only during test time
 
Now there's very little C++ code  
Eat through it incrementally via pre-training
 
In 10 years, end-to-end neural net
 
Tesla Humanoid?
 
Everything transfers from self-driving  
Tesla is a robotics at scale company
 
Things just show up (in-house expertise to building robots)
 
B2C is not the right start point
 
B2self, then B2B;  
A lot of materials handling
 
Large fixed cost of single platform humanoid  
Want it to be transferrable
 
Data collection very helpful via teleoperation
 
What about Mobile Aloha? Cheaper platform (maybe it takes you down to a local minimum)
 
**Lower body is angular physics; Upper body is imitation**
 
**Transformers**
 
Very little change in 5 years
 
Scaling laws are largely attributable to the transformer
 
Innovations - a piece of tissue:  
Residuals, Layer norm, attention block, Tanh's
 
Architecture is not a bottleneck
 
Activity has moved  
**Loss function + dataset matter more now**  
**Need 1B examples of trajectories of reasoning traces**  
**Refactoring dataset into inner monologue; synthetic data plays a part**  
**Only way to work;**  
**Models have collapsed on singular examples (e.g. GPT jokes)**  
**NEED ENTROPY**  
Someone release dataset of 1B personalities (Persona)
 
Human Brain
 
**Transformers are more efficient thank human brains?**  
E.g. sequence memorization via backprop
 
Backprop not real
 
ExCortex- Computers are an extension of the brain?
 
Why can't you talk to all things?  
Doors open themselves?  
Demolition man; iRobot
 
Democratization?  
Current oligopoly - Small number of large labs
 
NOT YOUR WEIGHTS; NOT YOUR BRAIN
 
Open source stuff is the fallback
 
Smallest performant model -  
Get to cognitive core that can look up things  
1B may suffice (maybe less?)  
Distillation works
 
Maybe want a parallel process of distilled models  
(swarm robotics?)  
(Agentic workflows)
 
Why Education?  
Love learning; love teaching  
Want to empower people instead of displace  
Empowered state of automation
 
How far can someone go if they have the perfect tutor?
 
1-1 tutoring increases people's performance by 2 std
 
How do you scale classes? To 8 billion people?
 
Teachers structure the curriculum  
Models are the front-end  
TA analogy?  
Tractable today
 
Companies don't understand what is available today
 
DEMO IS NEAR; PRODUCT IS FAR
 
Low hanging fruit:  
Different languages  
Mid:  
Adapt to people's background  
Understand what people know
 
**KEY : PERSONALIZATION**
 
**AI Community**  
Don't want lineage to matter  
Cluster effect - group of excellent people  
Bing part of a community  
WHAT ARE PEOPLE MOTIVATED BY?  
Toronto is not entrepreneurial
 
Cultural aspect may be a dominant variable
 
LEARNING SHOULD BE HARD  
MOTIVATION SHOULD BE BEYOND STATUS
 
POST-AGI - EDUCATION IS ENTERTAINMENT?  
People who were scientist were nobility
   

Eureka Labs:  
First course may be for undergrad
 
People will come back to school more frequently as technology accelerates
 
What should kids study? Math, Physics, CS  
Really, thinking skills  
Symbol manipulation rather than memorization