Alibaba DAMO Academy just dropped a game-changer for multimodal reasoning on Hugging Face!
 
Say hello to MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources. This work tackles two critical bottlenecks in multimodal AI.
 
First, the scarcity of large-scale, high-quality long Chain-of-Thought (CoT) data has hindered progress.
 
Second, the instability of reinforcement learning (RL) algorithms, particularly gradient vanishing in GRPO, has made post-training a challenge.
 
MMR1 introduces Variance-Aware Sampling (VAS), a novel data selection strategy guided by a Variance Promotion Score. This stabilizes policy optimization and ensures stronger, more consistent learning signals.
 
But that's not all! They've also released an incredible suite of open resources:  
- A massive ~1.6M Supervised Fine-Tuning (SFT) dataset with long CoT trajectories.  
- A ~15k Reinforcement Learning (RL) dataset, carefully curated for quality and diversity.
 
On top of this, a family of open-source MMR1 models (3B, 7B, and 32B scale checkpoints) are now available on the Hugging Face Hub, setting new baselines for the community.
 
These contributions have led to state-of-the-art performance on mathematical reasoning benchmarks, with significant improvements in training efficiency and robustness.
 
Check out the qualitative demo below to see MMR1 in action, solving complex problems with detailed, step-by-step reasoning. This commitment to open science truly accelerates research for everyone!
 
Discover the full paper, models, and datasets:
 
Paper: [https://lnkd.in/etTy2rvC](https://lnkd.in/etTy2rvC)  
Models: [https://lnkd.in/exT4YHV5](https://lnkd.in/exT4YHV5)  
Datasets: [https://lnkd.in/etPu3uAw](https://lnkd.in/etPu3uAw)  
Code: [https://lnkd.in/e5BGAwjT](https://lnkd.in/e5BGAwjT)
 \> From \<[https://www.linkedin.com/preload/](https://www.linkedin.com/preload/)\>