|   |
|---|
|**Training LLMs over Neurally Compressed Text**|
|Google introduces Equal-Info Windows, a method for training large language models on neurally compressed text. It outperforms byte-level baselines in perplexity and inference speed but has higher perplexity than subword tokenizers.Â <br><br>  <br><br>This method reduces sequence lengths, leading to lower latency. At 2 billion parameters, it achieves perplexity close to that of SentencePiece tokenization with reduced computational demands for inference.|
|**Yi Tay:** _"Holy. This idea is so outrageously cool and insane."_|
 \> From \<[https://mail.google.com/mail/u/0/#inbox/FMfcgzGxSbrtLSGRMzLFfSxqLTJMlQrW](https://mail.google.com/mail/u/0/#inbox/FMfcgzGxSbrtLSGRMzLFfSxqLTJMlQrW)\>