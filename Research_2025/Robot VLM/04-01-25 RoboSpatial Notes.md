ðŸ” Introducing RoboSpatial!  
We developed a data pipeline that automatically annotates real 3D indoor/tabletop scenes with spatial questions tailored to robotics:  
âœ… 1M images  
âœ… 5k 3D scans  
âœ… 3M+ spatial QAs  
âœ… Robotics-relevant spatial tasks.  
âœ… Multiple reference frames (ego, object, world)  
ðŸ‘‰ To support evaluation, we also introduce RoboSpatial-Home â€” a manually annotated benchmark focused on spatial reasoning in realistic home settings.
 
ðŸ“ˆ Does it work?  
Yes â€” training on RoboSpatial improves spatial reasoning across:  
- Spatial benchmarks (RoboSpatial-Home, BLINK, SpatialBench)  
- Real-world robot manipulation tasks  
- Domain transfer (e.g., indoor â†” tabletop)  
It helps build stronger spatial priors for foundation models that control physical agents.
 
ðŸ§© Whatâ€™s next?  
We see RoboSpatial as a stepping stone toward:  
- General-purpose robot foundation models (e.g., Google DeepMind Gemini-ER, NVIDIA GR00T, Physical Intelligence Hi Robot, Figure Helix)  
- Modular robotic systems  
- AR/VR applications with spatial grounding  
But spatial understanding is hard â€” our results show that even with scale, we still need better model architectures and learning paradigms to bridge the gap between VLMs and embodied agents.
 
ðŸ”— Learn more:  
ðŸ“‚ Benchmark: [https://lnkd.in/gYckbd49](https://lnkd.in/gYckbd49)  
ðŸ“– Paper: [https://lnkd.in/gzVpzEG4](https://lnkd.in/gzVpzEG4)  
Huge thanks to Valts Blukis, Jonathan Tremblay, Stephen Tyree, Stan Birchfield at NVIDIA, and Yu Su.
 \> From \<[https://www.linkedin.com/my-items/saved-posts/](https://www.linkedin.com/my-items/saved-posts/)\>