[![Image preview](Exported%20image%2020260222122718-0.jpeg)](https://www.linkedin.com/feed/update/urn:li:activity:7336784493183152128?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7336784493183152128%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29)

ðŸš¨ New CVPR 2025 Work â€” Can Vision-Language Models Self-Correct Semantic Grounding Errors?
 
**What if we could get large vision-language models (VLMs) to improve their own answers without retraining â€” just by thinking twice?**
 
ðŸ“ In our upcoming CVPR 2025 poster, we show that open-source VLMs, GPT-4V, and GPT-4o can self-correct semantic grounding mistakes through a training-free, agentic approach. That is:  
ðŸ¤– The model gives an answer... then turns around and verifies its own response.  
Why this matters:  
Prior work suggests LLMs can't reliably self-correct (ICLRâ€™24, EMNLPâ€™24)  
We challenge this: With well-structured, even binary feedback, test-time scaling emerges.
 
ðŸ‘€ Curious about system-2 thinking, feedback loops, or semantic grounding?  
ðŸª§ Come see our poster to discuss:  
1. Self-correction works in VLMs/LLMs  
2. Why verification beats generation for reliable feedback  
3. How we automate the process â€” no humans in the loop!
 
ðŸ§  Related projects:  
- EMNLP'24: Training-free spatial reasoning in VLMs [https://lnkd.in/gHWE_YZy](https://lnkd.in/gHWE_YZy)  
- arXiv'25: Synthetic System-2 reasoning for System-1 perception [https://lnkd.in/g6fYvp72](https://lnkd.in/g6fYvp72)
 
ðŸ‘‰ Project page â€“ [https://lnkd.in/gF6aYnsg](https://lnkd.in/gF6aYnsg)  
ðŸ•’ Poster time â€“ Saturday, June 14â‹…10:30 â€“ 12:30 CDT  
ðŸ“ Poster location â€“ ExHall D Poster #385
 
Shout out to my amazing collaborators/advisors: Rafid Mahmood, Sanja Fidler, and David Acuna
 
#CVPR2025 #VisionLanguageModels #SelfCorrection #MultimodalAI #System2Thinking #SemanticGrounding
 \> From \<[https://www.linkedin.com/my-items/saved-posts/](https://www.linkedin.com/my-items/saved-posts/)\>