**Summary - Generative AI:** [https://arxiv.org/abs/2312.10868](https://arxiv.org/abs/2312.10868)  
**FB:**  
New research from Meta FAIR: Unibench is a unified implementation of 50+ VLM benchmarks spanning a comprehensive range of carefully categorized capabilities from object recognition to spatial awareness, counting and much more.
 
Research paper ➡️ [https://go.fb.me/fa97z9](https://go.fb.me/fa97z9)  
UniBench repo ➡️ [https://go.fb.me/dxxz4w](https://go.fb.me/dxxz4w)
 \> From \<[https://www.linkedin.com/feed/](https://www.linkedin.com/feed/)\>   
Veo 2  
Phi 4  
Florence VL
 
DINO WM VLM: [https://openreview.net/pdf?id=GARbxyCV13](https://openreview.net/pdf?id=GARbxyCV13)
 
PaliGemma2
 
JEPA explanation post: [https://www.turingpost.com/p/jepa](https://www.turingpost.com/p/jepa)  
H-JEPA: [From Machine Learning to Autonomous Intelligence – AI-Talk by Prof. Dr. Yann LeCun](https://www.youtube.com/live/pd0JmT6rYcI?app=desktop&si=s9MzvtfKHKxWEzOL)
 
[https://www.linkedin.com/posts/barathsa_%F0%9D%97%A7%F0%9D%97%B5%F0%9D%97%B2-%F0%9D%97%94%F0%9D%97%B9%F0%9D%97%B9%F0%9D%97%B2%F0%9D%97%BB-%F0%9D%97%9C%F0%9D%97%BB%F0%9D%98%80%F0%9D%98%81%F0%9D%97%B6%F0%9D%98%81%F0%9D%98%82%F0%9D%98%81%F0%9D%97%B2-%F0%9D%97%B3%F0%9D%97%BC-activity-7244739466559455234-pI0F?utm_source=share&utm_medium=member_ios](https://www.linkedin.com/posts/barathsa_%F0%9D%97%A7%F0%9D%97%B5%F0%9D%97%B2-%F0%9D%97%94%F0%9D%97%B9%F0%9D%97%B9%F0%9D%97%B2%F0%9D%97%BB-%F0%9D%97%9C%F0%9D%97%BB%F0%9D%98%80%F0%9D%98%81%F0%9D%97%B6%F0%9D%98%81%F0%9D%98%82%F0%9D%98%81%F0%9D%97%B2-%F0%9D%97%B3%F0%9D%97%BC-activity-7244739466559455234-pI0F?utm_source=share&utm_medium=member_ios)
 
NVLM: Open Frontier-Class Multimodal LLMs [https://arxiv.org/pdf/2409.11402](https://arxiv.org/pdf/2409.11402)  
[https://huggingface.co/nvidia/NVLM-D-72B](https://huggingface.co/nvidia/NVLM-D-72B)
 
New research from Meta FAIR: Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model.
 
This recipe combines next token prediction with diffusion to train a single transformer over mixed-modality sequences. Our experiments show that Transfusion scales significantly better than traditional approaches and demonstrate that scaling to 7B parameters and 2T multi-modal tokens produces a model that is on par with similar scale language and diffusion models.
 
More details in the full research paper ➡️ [https://go.fb.me/4vnybn](https://go.fb.me/4vnybn)
 
VILA^2:  
[https://arxiv.org/html/2407.17453v1](https://arxiv.org/html/2407.17453v1)
 
Meta-prompt: [https://github.com/jmiemirza/Meta-Prompting](https://github.com/jmiemirza/Meta-Prompting)
 
Tutorial for Diffusion Models: [https://arxiv.org/abs/2403.18103](https://arxiv.org/abs/2403.18103)  
**Quick test on tuning VLMS vs CNNs; Note, this is for small models:** [https://www.picsellia.com/post/vlms-vs-cnns-a-new-era-dawning-in-computer-vision-performance](https://www.picsellia.com/post/vlms-vs-cnns-a-new-era-dawning-in-computer-vision-performance)  
**LVMS:** [Introducing Domain-Specific Large Vision Models (LVMs)](https://www.youtube.com/watch?utm_campaign=The+Batch&utm_medium=email&_hsmi=285502361&utm_content=285502361&utm_source=hs_email&v=29USE4U5IXo&feature=youtu.be)
 
Apple MM1: [https://arxiv.org/abs/2403.09611](https://arxiv.org/abs/2403.09611)  
. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance
 
**Cog VLM2**
 
Video Language planning [https://arxiv.org/abs/2310.10625](https://arxiv.org/abs/2310.10625)
 
ReALM (Apple): [https://arxiv.org/html/2403.20329v1](https://arxiv.org/html/2403.20329v1)
 
VLMS for embodied question answering: [https://arxiv.org/abs/2403.15941](https://arxiv.org/abs/2403.15941)
 
Ferret-UI (UI for multimodal): [https://arxiv.org/abs/2404.05719](https://arxiv.org/abs/2404.05719)
 
[Mora](https://link.alphasignal.ai/Pxk5cS): an open-source version of video generation model Sora.
 
Dream Machine: [https://lumalabs.ai/dream-machine](https://lumalabs.ai/dream-machine)
 
Vidu: [https://arxiv.org/pdf/2405.04233](https://arxiv.org/pdf/2405.04233)
 
Chain of Thought for VLMs: [https://arxiv.org/abs/2405.10292](https://arxiv.org/abs/2405.10292)
 
[https://arxiv.org/pdf/2405.10300](https://arxiv.org/pdf/2405.10300) grounding Dino 1.5
 
New paper from FAIR, Chameleon: Mixed-Modal Early-Fusion Foundation Models.
 
Chameleon: [https://arxiv.org/abs/2405.09818](https://arxiv.org/abs/2405.09818)
 
While some LLMs have separate image and text encoders or decoders, this research presents a family of early-fusion token-based mixed-modal models capable of understanding & generating images & text in any arbitrary sequence.
 
Paper ➡️ [https://go.fb.me/7rb19n](https://go.fb.me/7rb19n)
 
The paper includes details on the full modeling approach and training — we hope that sharing this work will help the community further research on mixed-modal models.
 \> From \<[https://www.linkedin.com/feed/](https://www.linkedin.com/feed/)\>   
Fine tune Paligemma: [https://blog.roboflow.com/how-to-fine-tune-paligemma/](https://blog.roboflow.com/how-to-fine-tune-paligemma/)
 
PaliGemma: [https://developers.googleblog.com/en/gemma-family-and-toolkit-expansion-io-2024/](https://developers.googleblog.com/en/gemma-family-and-toolkit-expansion-io-2024/)
 
VILA: [https://arxiv.org/abs/2312.07533](https://arxiv.org/abs/2312.07533)  
[https://developer.nvidia.com/blog/visual-language-intelligence-and-edge-ai-2-0/](https://developer.nvidia.com/blog/visual-language-intelligence-and-edge-ai-2-0/)  
AM-RADIO: [https://arxiv.org/abs/2312.06709](https://arxiv.org/abs/2312.06709)
 
Open Sora - [https://github.com/hpcaitech/Open-Sora](https://github.com/hpcaitech/Open-Sora)
 
State change: [https://haraduka.github.io/continuous-state-recognition/](https://haraduka.github.io/continuous-state-recognition/)
 
Large World Model (Long term context storage): [https://largeworldmodel.github.io/](https://largeworldmodel.github.io/)  
[https://arxiv.org/pdf/2310.01889.pdf](https://arxiv.org/pdf/2310.01889.pdf)￼  
Image to Text - Imagen2
 
Dataset: [https://ai.meta.com/blog/ego-exo4d-video-learning-perception/](https://ai.meta.com/blog/ego-exo4d-video-learning-perception/)
 
Video for planning: [https://arxiv.org/pdf/2402.17139.pdf](https://arxiv.org/pdf/2402.17139.pdf)
 
**i-JEPA:** [https://openaccess.thecvf.com/content/CVPR2023/papers/Assran_Self-Supervised_Learning_From_Images_With_a_Joint-Embedding_Predictive_Architecture_CVPR_2023_paper.pdf?utm_campaign=The%20Batch&utm_medium=email&_hsmi=286401222&utm_content=286401222&utm_source=hs_email](https://openaccess.thecvf.com/content/CVPR2023/papers/Assran_Self-Supervised_Learning_From_Images_With_a_Joint-Embedding_Predictive_Architecture_CVPR_2023_paper.pdf?utm_campaign=The%20Batch&utm_medium=email&_hsmi=286401222&utm_content=286401222&utm_source=hs_email)
 
V-JEPA: [https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/?utm_source=linkedin&utm_medium=organic_social&utm_campaign=vjepa&utm_content=video](https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/?utm_source=linkedin&utm_medium=organic_social&utm_campaign=vjepa&utm_content=video)
 
ConvLORA: [https://github.com/aleemsidra/ConvLoRA](https://github.com/aleemsidra/ConvLoRA)
 
**Visual sequence:** [https://arxiv.org/abs/2312.00785](https://arxiv.org/abs/2312.00785)
 
**Fast Whisper:** [https://github.com/Vaibhavs10/insanely-fast-whisper](https://github.com/Vaibhavs10/insanely-fast-whisper)