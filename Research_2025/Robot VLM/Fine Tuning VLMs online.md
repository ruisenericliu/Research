What's the best way to fine-tune open multimodal LLMs and VLMs? Open Multimodal LLMs have been making tremendous progress recently, including Meta AI's Llama 3.2 Vision, Mistral AI's Pixtral, Qwen 2 VL, and Allen AI's Molmo.
 
Those are great and already work for a number of out of the box use cases, but what if you need to customize them? I am excited to share â€œHow to Fine-Tune Multimodal Models or VLMs with Hugging Face TRL,â€ showing how to fine-tune Qwen 2 7B VL on generating product descriptions based on a title and image from Amazon. ðŸ–¼ï¸
 
TL;DR;  
ðŸ’» Set up fine-tuning environment including PyTorch, Hugging Face TRL, Transformers, and Datasets libraries.  
ðŸ’° Train on consumer GPUs (24GB) for just $1.4â€”three epochs on ~1k samples.  
ðŸ§  Leverage Q-LoRA and 4-bit quantization to reduce memory consumption and speed up training.  
â˜ï¸ Seamlessly push models, logs, and other information to the Hugging Face Hub.  
ðŸ›’ Generate compelling product descriptions from images and metadata to enhance e-commerce platforms.
 
Blog: [https://lnkd.in/eUuiYxuv](https://lnkd.in/eUuiYxuv)  
Code: [https://lnkd.in/eRJzwjn4](https://lnkd.in/eRJzwjn4)
 \> From \<[https://www.linkedin.com/my-items/saved-posts/](https://www.linkedin.com/my-items/saved-posts/)\>