Talks and Speeches :
 
Articulation based relative representations in manipulation - midnight bathroom trip
 
Hugging Face Agent Course:  
[https://huggingface.us17.list-manage.com/subscribe?u=7f57e683fa28b51bfc493d048&id=9ed45a3ef6](https://huggingface.us17.list-manage.com/subscribe?u=7f57e683fa28b51bfc493d048&id=9ed45a3ef6)
 
Cross Embodiment workshop:  
[https://sites.google.com/view/xembodimentworkshop](https://sites.google.com/view/xembodimentworkshop)
 
Nvidia Fundamentals Course: [https://www.nvidia.com/en-us/learn/learning-path/robotics/](https://www.nvidia.com/en-us/learn/learning-path/robotics/)
 
Coding an LLM from scratch: [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch)
 
Guide to Building VLMs from scratch: [https://medium.com/@mbn312](https://medium.com/@mbn312)
 
History of efficiency in Deep Learning: [https://alexzhang13.github.io/blog/2024/efficient-dl/](https://alexzhang13.github.io/blog/2024/efficient-dl/)
 
Summary 50: [https://www.latent.space/p/2025-papers](https://www.latent.space/p/2025-papers)
   

Versus AI - AXIOM: [https://arxiv.org/pdf/2505.24784](https://arxiv.org/pdf/2505.24784)  
[Gemini Robotics on-device in simulation](https://m.youtube.com/watch?v=nVMY3-kWhOc)
 
[https://seed.bytedance.com/en/GR3](https://seed.bytedance.com/en/GR3)  
[https://arxiv.org/pdf/2507.16815](https://arxiv.org/pdf/2507.16815)  
Sort: [https://arxiv.org/pdf/2507.07986](https://arxiv.org/pdf/2507.07986)  
[https://exbody2.github.io/resources/exbody2.pdf](https://exbody2.github.io/resources/exbody2.pdf)  
Villa-X: [https://arxiv.org/abs/2507.23682](https://arxiv.org/abs/2507.23682)  
[https://rewind-reward.github.io/](https://rewind-reward.github.io/)  
[https://arxiv.org/abs/2505.10911](https://arxiv.org/abs/2505.10911)  
[https://www.pair.toronto.edu/Adapt3R/](https://www.pair.toronto.edu/Adapt3R/)  
```ReWiND enables sample-efficient adaptation to new tasks by training a language-conditioned reward model and policy from a small set of demonstrations to learn new tasks without additional per-task demonstrations. We beat baselines by 2X in simulation and improve real-world pre-trained policies by 5X in just 1 hour.```
 
New survey: [https://arxiv.org/abs/2508.13073](https://arxiv.org/abs/2508.13073)  
Pi 0.5 open source now  
[https://link.alphasignal.ai/LGJrVm](https://link.alphasignal.ai/LGJrVm)
   

[https://jaraxxus-me.github.io/IVNTR/](https://jaraxxus-me.github.io/IVNTR/)
 
For anyone interested in atomic tasks in VLAs and the limitations of current robotics foundation models:  
First paper: [https://sites.google.com/view/rh20t-primitive/main](https://sites.google.com/view/rh20t-primitive/main)  
Second paper: [https://arxiv.org/abs/2504.00420](https://arxiv.org/abs/2504.00420)  
Blog post about limitations in current robotics foundation models: [https://roboticalpha.substack.com/p/reborn-robotics-for-the-people-by](https://roboticalpha.substack.com/p/reborn-robotics-for-the-people-by)
 
In the blog post they state:  
"Then thereâ€™s the **model gap**. Even with advances in imitation learning and vision-language-action models, todayâ€™s embodied AI is still brittle. **The models are often overfitted, struggle in unfamiliar settings, and are tightly coupled to specific tasks or platforms.** "
 
I am curious to hear you opinion on this problem with VLAs and VLMs as a whole. The other day I listened the most recent podcast of Lex Fridman and Yann Lecun. There Yann says that right now we are sort of "cheating" this perception logic of models through VLMs by providing visual observations to an LLM. I think you already know his take on the limitations of LLMs and that text is a low-bandwith type of data that is not efficient for training systems that are actually intelligent. Their JEPA architecture sounds really interesting and promising so me and my team are going to try to play with the V-JEPA 2 model for perception and planning in robotics these days. Has anyone here tried it already and what is their opinion? Would be awesome to hear your experience! ðŸ˜ƒ
 
[https://medium.com/correll-lab/grow-your-own-expert-for-free-with-langchain-local-llava-rag-add630abbe6f](https://medium.com/correll-lab/grow-your-own-expert-for-free-with-langchain-local-llava-rag-add630abbe6f)
 
Animate Anyone: [https://humanaigc.github.io/animate-anyone/](https://humanaigc.github.io/animate-anyone/)
 
Zero-shot Image Editing with Reference Imitation  
[https://arxiv.org/abs/2406.07547](https://arxiv.org/abs/2406.07547)  
[https://huggingface.co/papers/2406.07547](https://huggingface.co/papers/2406.07547)
 
LORA vs Fine tuning: [https://arxiv.org/pdf/2410.21228v1](https://arxiv.org/pdf/2410.21228v1)
 
Thinking LLMs: General Instruction Following with Thought Generation  
[https://arxiv.org/abs/2410.10630](https://arxiv.org/abs/2410.10630)
 
Project Sid: [https://arxiv.org/abs/2411.00114](https://arxiv.org/abs/2411.00114)
 
Byte Latent Transformer: [https://arxiv.org/abs/2412.09871](https://arxiv.org/abs/2412.09871)
 
MCTS rollouts for LLM policies: [https://deepmind.google/research/publications/139455/](https://deepmind.google/research/publications/139455/)
 
Quantum photo taking: [https://opg.optica.org/optcon/fulltext.cfm?uri=optcon-2-11-2386&id=541465](https://opg.optica.org/optcon/fulltext.cfm?uri=optcon-2-11-2386&id=541465)