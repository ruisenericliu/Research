**Mamba-2: A new state space model architecture that outperforms Mamba and Transformer++**
   

|   |
|---|
|**What's New**|
|After the success of Mamba-1, which accumulated over 10k stars on GitHub, researchers Tri Dao and Albert Gu introduce Mamba-2.<br><br>  <br><br>Mamba is a new state space model architecture showing promising performance on information-dense data such as language modeling, where previous subquadratic models fall short of transformers.<br><br>  <br><br>You can use Mamba-2 with PyTorch to build and train neural networks handling information-dense data efficiently.<br><br>  <br><br>**Core Innovation: Structured State Space Duality (SSD)**  <br>Mamba-2's core innovation, SSD, combines SSMs and attention mechanisms. SSD constrains the recurrent matrix ùê¥_A_ to a scalar-times-identity structure, simplifying computations and enhancing hardware efficiency.¬†<br><br>  <br><br>This design enables multi-head SSMs, increasing the state size from 16 in Mamba-1 to 64-256 in Mamba-2, and uses matrix multiplications optimized for GPUs/TPUs.<br><br>  <br><br>**Performance Improvements**  <br>Mamba-2 trains 50% faster than Mamba-1 and handles larger state dimensions. At the 3B scale, Mamba-2, trained on 300B tokens, surpasses Mamba-1 and older transformers in performance. The model significantly outperforms Mamba-1 on tasks like multi-query associative recall (MQAR) due to its larger state sizes.<br><br>  <br><br>**Architectural Changes**  <br>Mamba-2 introduces parallel parameter generation for ùê¥_A_, ùêµ_B_, and ùê∂_C_, enabling tensor parallelism and scaling. It maintains efficient memory usage with a constant state size per channel and leverages matrix multiplications for faster computation. These changes simplify the architecture and improve scalability.<br><br>  <br><br>**Empirical Results**  <br>In language modeling tasks, Mamba-2 shows slightly better scaling compared to Mamba-1, adhering to Chinchilla laws, with faster training times. Pretrained models ranging from 130M to 2.8B parameters are available, trained on datasets like Pile and SlimPajama. Performance is consistent across architectures, with minor variations in zero-shot evaluation results due to evaluation noise.  <br>**Key Specifications**<br><br>- **State Size**: Increased from 16 (Mamba-1) to 64-256 (Mamba-2)<br>- **Training Speed**: 50% faster than Mamba-1<br>- **Scale**: Models range from 130M to 2.8B parameters<br>- **Datasets**: Trained on Pile and SlimPajama<br>- **Evaluation Tasks**: Includes MQAR and zero-shot evaluations on various benchmarks|