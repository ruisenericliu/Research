[https://cohere.com/research/papers/back-to-basics-revisiting-reinforce-style-optimization-for-learning-from-human-feedback-in-llms-2024-02-23](https://cohere.com/research/papers/back-to-basics-revisiting-reinforce-style-optimization-for-learning-from-human-feedback-in-llms-2024-02-23)
 
Reinforcement Learning from Human Feedback (RLHF) has established itself as a cornerstone in improving LLMs after fine-tuning (SFT). ðŸ§¬Â I am excited to share my â€œRLHF in 2024 with DPO & Hugging Faceâ€ guide teaching you how to apply RLHF on open LLMs using DPO, including Flash Attention & Q-LoRA, all built with [Hugging Face](https://www.linkedin.com/company/huggingface/) TRL. ðŸš€
 
The guide covers full end-to-end DPO example with:  
ðŸ§‘ðŸ»â€ðŸ’»Â Setup of the development environment  
ðŸ§®Â Create and prepare the preference dataset  
ðŸ‹ï¸â€â™€ï¸Â Align LLM with TRL and the DPOTrainer  
ðŸ˜ŽÂ Test LLM (vibe-check)  
ðŸ¥‡Â Evaluate open LLMs on MT-Bench
 
ðŸ‘‰Â  [https://lnkd.in/eWfTtiVP](https://lnkd.in/eWfTtiVP)
 
RLHF in 2024 with DPO & Hugging Face is the continuation of my â€œHow to Fine-Tune LLMs in 2024 with Hugging Faceâ€ Blog post. Coming next: Scaling SFT and DPO in multi-GPU/multi-Node environments.ðŸ”œ
 \> From \<[https://www.linkedin.com/feed/](https://www.linkedin.com/feed/)\>      
[https://github.com/huggingface/alignment-handbook](https://github.com/huggingface/alignment-handbook)
 
Aligning LLMs with Human Preferences is one of the most active research areas. ðŸ§ªÂ RLHF, DPO, and SLiC are all techniques for aligning LLMs, but they come with limitations and challenges. ðŸ¥·Â DeepMind proposed a new method, â€œStatistical Rejection Sampling Optimization (RSO)â€ to align and learn from Human Preferences.
 
ð—£ð—¿ð—²ð—¿ð—²ð—¾ð˜‚ð—¶ð˜€ð—¶ð˜ð—²ð˜€:  
ðŸ“Â Collect pairwise human preferences data, with each example containing a prompt, an accepted response, and a rejected response.  
ðŸ§ Â Train a pairwise Reward Model on the preference dataset to predict which response is â€œacceptedâ€ for a given prompt.  
ðŸ¤–Â Train an SFT Model on a different instruction dataset
 
ð—¥ð—¦ð—¢ ð—¶ð—ºð—½ð—¹ð—²ð—ºð—²ð—»ð˜ð—®ð˜ð—¶ð—¼ð—»:  
ð—¦Ì²ð˜Ì²ð—²Ì²ð—½Ì² Ì²ðŸ­Ì²:Ì² Sample responses using the SFT Model and rank each response with the Reward Model  
ð—¦Ì²ð˜Ì²ð—²Ì²ð—½Ì² Ì²ðŸ®Ì²:Ì² Use Statistical rejection sampling to conduct samples favoring higher rewards. Label the selected responses as preference pairs based on the relative reward.  
ð—¦Ì²ð˜Ì²ð—²Ì²ð—½Ì² Ì²ðŸ¯Ì²:Ì² Iteratively train the SFT Model using the new labeled preference pairs with logistic or hinge loss.
 
ð—£ð—®ð—½ð—²ð—¿ ð—œð—»ð˜€ð—¶ð—´ð—µð˜ð˜€:  
ðŸ¥‡Â A good Reward Model is the most important and crucial component of RSO  
ðŸ†Â RSO outperforms other methods like SLiC and DPO across diverse tasks.  
âœ…Â Easier to implement than RLHF to learn from human preference  
ðŸ”¢Â Rejection sampling unifies top-k selection and statistical sampling.  
ðŸ‘©â€ðŸ”§Â Used Amazon Mechanical Turk for human evaluation.  
âš–ï¸Â Shows Hinge (SLiC) and logistic Loss (DPO) perform equally well.  
ðŸ¤–Â Used T5 models
 
Check out the full paper: [https://lnkd.in/eDK-PXxs](https://lnkd.in/eDK-PXxs)
 
Remember that these are just my personal findings. Make sure always to conduct your own research and analysis. ðŸ¤—
 ![diagram](Exported%20image%2020260222203119-0.jpeg)

_It is only rarely that, after reading a research paper, I feel like giving the authors a standing ovation. But I felt that way after finishing_ _Direct Preference Optimization_ _(DPO) by Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Chris Manning, and Chelsea Finn._
 
_RLHF became a key algorithm for LLM training thanks to the_ _InstructGPT_ _paper, which adapted the technique to that purpose. A typical implementation of the algorithm works as follows:_Â 

- _Get humans to compare pairs of LLM outputs, generated in response to the same prompt, to specify which one they prefer. For example, humans typically prefer the more helpful, less toxic output._
- _Use the human preferences to learn a reward function. The reward function, typically represented using a transformer network, is trained to give a higher reward (or score) to the outputs that the humans preferred._
- _Finally, using the learned reward, run a reinforcement learning algorithm to tune the LLM to (i) maximize the reward of the answers generated, while (ii) not letting the LLM change too much (as a form of regularization)._

_This is a relatively complex algorithm. It needs to separately represent a reward function and an LLM. Also, the final, reinforcement learning step is well known to be finicky to the choice of hyperparameters._
 
_DPO dramatically simplifies the whole thing. Rather than needing separate transformer networks to represent a reward function and an LLM, the authors show how, given an LLM, you can figure out the reward function (plus regularization term) that that LLM is best at maximizing. This collapses the two transformer networks into one. Thus, you now need to train only the LLM and no longer have to deal with a separately trained reward function. The DPO algorithm trains the LLM directly, so as to make the reward function (which is implicitly defined by the LLM) consistent with the human preferences. Further, the authors show that DPO is better at achieving RLHF's optimization objective (that is, (i) and (ii) above) than most implementations of RLHF_
 
_RLHF is a key building block of the most advanced LLMs. Itâ€™s fantastic that these Stanford authors â€” through clever thinking and mathematical insight â€” seem to have replaced it with something simpler and more elegant. While it's easy to get excited about a piece of research before it has stood the test of time, I am cautiously optimistic that DPO will have a huge impact on LLMs and beyond in the next few years. Indeed, it is already making its way into some top-performing models, such as Mistralâ€™s_ _Mixtral__._Â Â 
 \> From \<[https://mail.google.com/mail/u/0/#inbox/FMfcgzGwJckFzvwxdxCDkVDBLTWlWTMT](https://mail.google.com/mail/u/0/#inbox/FMfcgzGwJckFzvwxdxCDkVDBLTWlWTMT)\>