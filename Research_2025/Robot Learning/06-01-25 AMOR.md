Annoyed by having to retrain your entire policy just because your reward weights did not quite work on the real robot? ðŸ¤–
 
We are excited to share our SIGGRAPH 2025 paper,  
â€œAMOR: Adaptive Character Control through Multi-Objective Reinforcement Learningâ€  
Lucas Alegre*, Agon Serifi*, Ruben Grandia, David MÃ¼ller, Espen Knoop, Moritz Baecher
 
AMOR trains a single policy conditioned on reward weights and motion context, letting you fine-tune the reward after training.  
Want smoother motions? Better accuracy? Just adjust the weights â€” no retraining needed!
 
A base policy with uniform weights might fail on challenging motions, but with a few weight tweaks, it nails them. Like this double spin. ðŸŒ€ðŸ˜µâ€ðŸ’«
 
Curious how tuning weights mid-motion can help improve the sim-to-real gap and unlock dynamic, expressive behaviors?
 
Check out AMOR now on arXiv:  
Paper: [https://lnkd.in/eutN4qzk](https://lnkd.in/eutN4qzk)  
Full Video: [https://lnkd.in/ez-piF6M](https://lnkd.in/ez-piF6M)
 
#SIGGRAPH2025 #RL #robotics
 \> From \<[https://www.linkedin.com/my-items/saved-posts/](https://www.linkedin.com/my-items/saved-posts/)\>