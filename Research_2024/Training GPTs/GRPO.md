What is Group Relative Policy Optimization (GRPO)? Deepseek Coder v2 is the best open Code LLM rivaling GPT-4 on coding tasks. As part of the technical report, GRPO is mentioned as RLHF method, but what is it? ðŸ¤”
 
GRPO was introduced in the DeepSeekMath Paper earlier this year and is method in designed to improve improve mathematical reasoning capabilities with less memory consumption.
 
Implementation  
1ï¸âƒ£Â Generate multiple outputs for each input question using the current Policy  
2ï¸âƒ£Â Score these outputs using a reward model  
3ï¸âƒ£Â Average the rewards and use it as a baseline to compute the advantages  
4ï¸âƒ£Â Update the Policy to maximize the GRPO objective, which includes the advantages and a KL term
 
Insights  
ðŸ’¡ GRPO doesn't need value function model, reducing memory and complexity  
ðŸ”— GPRO adds the KL term directly to the loss rather than in the reward  
ðŸ“ˆ GPRO improved GSM8K and MATH ~5%  
ðŸ‘‰ GPRO looks similar to RLOO method (available in TRL)  
ðŸ” Used Iterative Approach to train new Reward Models  
ðŸ“Š RL data consisted of 144k CoT prompts from SFT dataset  
ðŸ§  Reward Model was trained using â€œMath-Shepherdâ€ process
 
RL is â€œboosting the correct response from TopK rather than the enhancement of fundamental capabilities.â€
 
DeepSeekMath: [https://lnkd.in/eGAk_vbG](https://lnkd.in/eGAk_vbG)  
Math-Shepherd: [https://lnkd.in/etUVyBgm](https://lnkd.in/etUVyBgm)
 \> From \<[https://www.linkedin.com/feed/](https://www.linkedin.com/feed/)\>