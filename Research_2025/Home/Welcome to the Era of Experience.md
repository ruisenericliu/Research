[https://storage.googleapis.com/deepmind-media/Era-of-Experience%20/The%20Era%20of%20Experience%20Paper.pdf](https://storage.googleapis.com/deepmind-media/Era-of-Experience%20/The%20Era%20of%20Experience%20Paper.pdf)
   

|   |
|---|
|**What's New**|
|Pioneers of modern reinforcement learning, Richard Sutton and David Silver released a new paper, _“Welcome to the Era of Experience,”_ proposing that AI should stop relying on fixed human-labeled data.<br><br>  <br><br>They argue that supervised pre-training and RLHF have hit diminishing returns. The paper introduces “streams”, continuous interaction loops with real or simulated environments, as the foundation for future agents.<br><br>  <br><br>**Core Concept: Learning through experience**  <br>Agents create and learn from their own data in dynamic environments.<br><br>- Avoids dependence on finite human-generated datasets for training.<br>- Uses environmental feedback instead of supervised labels or predefined objectives.<br>- Supports continuous, long-term learning across multiple tasks and domains.<br>- Focuses on capability growth over time, not task-specific performance.<br><br>  <br><br>**Streams**  <br>The stream model replaces static datasets with continuous interaction. Streams operate without resets, retain state across time, and provide real-time feedback from the environment.<br><br>- Each stream is a sequence of actions, observations, and rewards without episodic breaks.<br>- Agents update policies continuously instead of training on frozen datasets.<br>- Feedback comes from the task environment, not human annotations or rankings.<br>- Agents use rewards tied to measurable outcomes like scores, accuracy, or task completion.<br><br>  <br><br>**Setup: Use Persistent Environments**  <br>Stream-compatible setups require stateful, interactive environments that support long time horizons.<br><br>- Minecraft (via MineDojo or Voyager) supports long-term exploration and crafting behaviors.<br>- NetHack provides procedural depth and long-horizon reasoning under constraints.<br>- Scientific simulators model drug discovery or materials optimization with direct task rewards.<br>- Real-world tasks include learning from medical records or educational assessments over time.<br><br>  <br><br>**Adapt Existing RL Tools**  <br>You can use RLlib, CleanRL, or JAXline with added memory and environment persistence.<br><br>- Agents must handle cross-session memory and long-term state tracking.<br>- Replay buffers or episodic memory modules enable experience accumulation.<br>- Planning modules adapt models across variable state sequences.<br>- Exploration bonuses support open-ended learning without task supervision.<br><br>  <br><br>**Replace labels with task signals**  <br>Rewards derive from objective metrics instead of subjective preference scores.<br><br>- Use exam scores, chemical novelty, or execution correctness as ground truth.<br>- No human judgment required for reward shaping or ranking.<br>- Intrinsic motivation guides exploration when task feedback is sparse.<br>- Evaluation uses outcome-based metrics rather than imitation accuracy.<br><br>  <br><br>**Tool use and custom integrations**  <br>Both models support dynamic tool use with improved argument selection and context awareness.<br><br>- Integrates with _function calling_ and _custom tools_ through the OpenAI API<br>- Determines tool use based on system and user messages<br>- Refines arguments more accurately during multi-turn tasks<br>- Supports mixed-modality workflows including vision, text, and code<br><br>  <br><br>**Implementation: Where and how to apply it**  <br>You can prototype these systems in formal logic and structured environments.<br><br>- Use Lean, Coq, or similar theorem provers to simulate controlled learning settings.<br>- Train with RL libraries that support self-play and reward-based feedback loops.<br>- Focus on environments where exploration creates high-value learning data.<br>- Avoid reliance on human annotations or static benchmarks.<br><br>  <br><br>**Community Feedback**<br><br>  <br><br>**LostAndFounding**  <br>_"This seems completely sensible to me. It reminds me of the (much earlier) days of Machine Learning where there was a desire to move quickly towards "online" models that could ingest and adapt in realtime after some of the most popular algos (e.g. SVM) has been proven in static context."_<br><br>  <br><br>**Peter**  <br>_"Wow. This has significant implications - current foundation model companies with large marketshare will accumulate an insurmountable lead with their access to proprietary experience data. And as they verticalize, they’ll turn into the AI version of AWS/GCP/Azure"_<br><br>  <br><br>**visarga**  <br>_"Many people like to say we are just one or two discoveries away from AGI, I think they are wrong, it's a matter of data-loop, not algorithmic innovation. What Sutton says is that progress will come from interactivity not clever algorithms."_|
 \> From \<[https://mail.google.com/mail/u/0/#inbox/FMfcgzQbdrLnHQbgNhjkxjGNtcHXjLSL](https://mail.google.com/mail/u/0/#inbox/FMfcgzQbdrLnHQbgNhjkxjGNtcHXjLSL)\>