Excited to share our recent work at Physical Intelligence! Our latest VLA model π₀.₅ is a step towards meaningful generalization of robot manipulation to unstructured, everyday situations. We deployed and evaluated this model in entirely new environments — cleaning bedrooms and kitchens in real homes, never-before-seen in the training data.
 
This level of generalization is made possible by training on a diverse mix of robot and non-robot data, including language, web and demonstration data collected across a wide range of tasks and embodiments. Incredibly proud to be a part of this amazing team!
 
See the paper and blog post for more details: [https://lnkd.in/gCvtPScc](https://lnkd.in/gCvtPScc)
 \> From \<[https://www.linkedin.com/my-items/saved-posts/](https://www.linkedin.com/my-items/saved-posts/)\>        

Humans can easily adapt to new environments, but robots can only operate in environments they’ve been trained in.
 
Introducing π₀.₅ : our first step at changing that…
 
We wheeled a π₀.₅ powered mobile manipulator into homes—trained predominantly (97%) on data from other robots and web-based multimodal tasks—into completely unseen homes. We gave it natural-language goals like “load the dishes” or “make the bed,” then stepped back and watched it handle these chores autonomously, without scene-specific tuning.
 
Why this matters:  
- Cross-robot & Web Training: Only ~2% data from this mobile platform.  
- Unified Hierarchical VLA Policy: The same model outputs both high-level semantic plans and low-level motor commands.  
- Effective Generalization at Scale: Matches in-domain training after just ~100 diverse training homes.
 
This isn’t just a research milestone; it’s a real step toward general-purpose robots in our daily lives.
 
Check out the full paper and raw videos from [Physical Intelligence](https://www.linkedin.com/company/physical-intelligence/):
 \> From \<[https://www.linkedin.com/feed/](https://www.linkedin.com/feed/)\>