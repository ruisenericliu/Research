Apple just dropped a new paper on LRMs and itâ€™s not pretty. ðŸ™ˆ
 
Paper title: The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity.
 
Some highlights:
 
ðŸ§© Using controlled puzzle environments (e.g. Tower of Hanoi), the authors show that frontier LRMs like Claude 3.7 and DeepSeek-R1 ð—°ð—¼ð—¹ð—¹ð—®ð—½ð˜€ð—² ð˜ð—¼ ðŸ¬% ð—®ð—°ð—°ð˜‚ð—¿ð—®ð—°ð˜† beyond a certain complexity threshold.
 
âš ï¸ Surprisingly, giving these models the ð—²ð˜…ð—®ð—°ð˜ ð˜€ð—¼ð—¹ð˜‚ð˜ð—¶ð—¼ð—» ð—®ð—¹ð—´ð—¼ð—¿ð—¶ð˜ð—µð—º doesn't help. They still fail to execute logical steps correctly - exposing severe limits in symbolic reasoning and step-by-step verification.
 
ðŸ“‰ Even worse, as problems get harder, ð˜ð—µð—²ð˜€ð—² ð—ºð—¼ð—±ð—²ð—¹ð˜€ ð˜€ð˜ð—®ð—¿ð˜ ð˜ð—µð—¶ð—»ð—¸ð—¶ð—»ð—´ ð™¡ð™šð™¨ð™¨ - using fewer tokens despite having available budget. This suggests a compute scaling limit ð˜¯ð˜°ð˜µ caused by external constraints, but internal brittleness.
 
ðŸ¤– They exhibit â€œoverthinkingâ€: generating correct answers early, then undermining them by exploring incorrect paths. And as complexity increases, the models stop finding correct answers altogether.
 
ð—œð—ºð—½ð—¹ð—¶ð—°ð—®ð˜ð—¶ð—¼ð—»ð˜€?
 
To me all this is consistent with the fact that current LRMs donâ€™t â€œreasonâ€ in any robust or generalisable sense. They simulate reasoning through pattern-matching - and once the patterns break down so does their ability to reason.
 
Mistaking examples of reasoning for actual reasoning is a logical fallacy. Don't fall for it.
 \> From \<[https://www.linkedin.com/my-items/saved-posts/](https://www.linkedin.com/my-items/saved-posts/)\>