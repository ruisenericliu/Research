Is **VideoGen** starting to become good enough for robotic manipulation?
 
ðŸ¤– Check out our recent work, RIGVid â€” Robots Imitating Generated Videos â€” where we use AI-generated videos as intermediate representations and 6-DoF motion retargeting to guide robots in diverse manipulation tasks: pouring, wiping, mixing, and more.  
â›“ï¸â€ðŸ’¥ https://lnkd.in/ey8XwH57
 
Key takeaways:  
- VideoGen starts to become good enough for robotics  
- As the field progresses, we are expecting much better results in the coming years  
- Depending on whether video prediction models take actions or not (VideoGen vs Action-Conditioned Video Prediction), there are different ways to use them.  
- Controllability & steerability are still issues
 
In the paper, we explore:  
â€“ How do different video generation models compare for robotic imitation?  
â€“ Can generated videos replace real videos for imitation?  
â€“ What causes failure of imitation given high-quality videos?  
â€“ How does imitating from video compare with other representations (e.g., keypoint constraints like ReKep)?
 
ðŸŽ¥ Watch the video for (1) AI-generated inputs, (2) robot executions, and (3) the 3D intermediate representation bridging the embodiment gap.
 \> From \<[https://www.linkedin.com/my-items/saved-posts/](https://www.linkedin.com/my-items/saved-posts/)\>