A new model has landed in the Transformers library: EoMT! ðŸ”¥
 
EoMT is short for Encoder-only Mask Transformer, and it greatly simplifies the design of Vision Transformers for the task of image segmentation.
 
The authors asked themselves "Why build a rocket engine full of bolted-on subsystems when one elegant unit does the job?" cause that's basically the gist of this work. Previous models for universal image segmentation like Mask2Former and OneFormer are heavy and complex. They leverage a Vision Transformer (ViT) as backbone and add modules like an adapter, pixel decoder and Transformer decoder on top.
 
âŒ EoMT removes all of that. It only keeps the ViT and adds a few query tokens that guide it to predict masks, no decoder needed. Despite its simplicity, it achieves an optimal trade-off between accuracy (measured via panoptic quality or PQ) and speed (FPS) on the COCO dataset. It can handle instance, semantic and panoptic segmentation with great accuracy.
 
This is also the main reason this work was integrated into Transformers: simple models that do tasks really well are preferred over heavy complex ones.
 
The work was developed at Eindhoven University of Technology and was presented as a highlight at #CVPR 2025, one of the most prestigious conferences for computer vision.
 
Huge kudos to Yaswanth Gali for this amazing contribution.
 
Docs: [https://lnkd.in/eQqUY5vT](https://lnkd.in/eQqUY5vT)  
Models: [https://lnkd.in/eqrg8XkK](https://lnkd.in/eqrg8XkK)  
Inference notebook: [https://lnkd.in/eYe4Q7Xs](https://lnkd.in/eYe4Q7Xs)  
Credits for this image: [https://lnkd.in/eTGcF7eN](https://lnkd.in/eTGcF7eN)
 \> From \<[https://www.linkedin.com/my-items/saved-posts/](https://www.linkedin.com/my-items/saved-posts/)\>