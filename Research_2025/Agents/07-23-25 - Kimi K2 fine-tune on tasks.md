**Born to Be Agentic**  
   
An agent’s performance depends not only on an effective workflow but also on a large language model that excels at agentic activities. A new open-weights model focuses on those capabilities.  
   
**What’s new:** Beijing-based Moonshot AI released the [Kimi K2](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWG0zr6Jh43BW5kvlP23xC40YW6mk3Cf5zlJMgN8pR4Cg3qgz0W7lCdLW6lZ3l0Vcsqst35zCt5W7W4J6z5WLW2MW8bGkcP6V_KqpW6F7Qrr74wkYWW3j075k6HFMk-VDfCwd7x6LklW5zD1858kRvTMW3wrPWR77DHXbW4nXRQX3GjWPZW4fP7Yz3MFTLcW6jL03b9lrCFdW6zVLrp7ml_D9W8nwQfM7BKn-qW52Cx-f1gwfPQN5-Cq18HLpr2W7qN5071Z5Pz4W4KZSS73x1c26W8PZ9hx3-3cZRTNCG01XKNt_W3SnZXV5lFq04N3Y0BSr-TH5wW7FBQMm1R-MNDN3rL_6Qj3KpQW3lhzct3tKxwNf4vkzjW04) family of 1 trillion-parameter large language models (LLMs). The family includes the pretrained Kimi-K2-Base and Kimi-K2-Instruct, which is fine-tuned for core agentic tasks, notably tool use. Bucking the recent trend in LLMs, Kimi K2 models are not trained for chain-of-thought reasoning.

- **Input/output:** Text in (up to around 128,000 tokens), text out (up to around 16,000 tokens)
- **Architecture:** Mixture-of-experts transformer, 1 trillion parameters total, 32 billion parameters active
- **Performance:** Outperforms other open-weights, non-reasoning models in tool use, coding, math, and general-knowledge benchmarks
- **Availability:** [Web interface](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWG0zr6Jh43BW5kvlP23xC40YW6mk3Cf5zlJMgN8pR4B-3qgz0W6N1vHY6lZ3pjW427JGw8dBcp9W1sHBnK8pMF23W4nHdj77GqjNvW7MVKD38KBT9GW54tYVG42Z5SvW6mBzTJ4Nbl7lW6-fHYK1YFrCmW8wj1ll9gSmqXN4lZt2-ZHrf3W9lZRtF8cP_TSVGgpFF10Kj0BW7h5w0N2f8pVBW9hBCZF39TZYHW93F3lQ8ClGbWW5qtPfN5h6XKYW90CcPT3wyYhLW3CR3q02Wyy1KN3QD3kHpxhT2W5mD7fN8gVtVdW5VTJ9P8jmnr1W4vlfwD1zwYX8W63pvkM2ZnpV6f4MxqHW04) (free), [API](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWG0zr6Jh43BW5kvlP23xC40YW6mk3Cf5zlJMgN8pR4CT3qgz0W8wLKSR6lZ3n1W65z92v5WdKt0W7_X0Q-3Z_D4vW86gkbm46sPjLW4KllVx4z2LBkW17HCt25G2lyYW6vQXQR5ln2cRW88y4LD5wqPXXVN4Gk-6z0q2sN8_69z44yXDYW7tGYYb3yBpNyW1HLByp6my1pkW2Rr6Qv5rRDrZV1Q4QD1xDWhXW4FrRdm65crTTW2xXTbZ86-wfHW3DVcGS2mQh3LW111MPP2Bc_9DW32Q0qM81Yw78W2SN29m7mwGylW4xQGL061R786W7cp_QW3CmX5vW7XlnTv1l1vYwW8vk67G3G1tLfW3LDq_01TSp7qW53FsQP5dKVfyW2Pc7C840YrG0W4MP_J98_VyyTW8clFCd2qQW2rf5Gm_cF04) ($0.60/$0.15/$2.50 per million input/cached/output tokens), [weights](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWG0zr6Jh43BW5kvlP23xC40YW6mk3Cf5zlJMgN8pR4Cz3qgz0W7Y8-PT6lZ3p5W5qndPR2nGG0vW5B62JB8slR-zW82pdk55tDLfhW7Zy2ct5kxRPHW7Kb-RV2mVh_zW6mJH6q7k1py1W7Rft188K11g7N86pqH7BYVQ0W2KYq7Y3pss1rW30XLVn21Kk7gW8qCSDn7xGVcyW67v9wt3L-C-HW8qXLDr5z1pBjN1zK1Bzw1BFbW6FR-Qh6HqTvHW7Vpv2w3PYl10W6jYFgz1MKrpyW2knthG8rnzhVW4WNGVn1dg_kQVHp0W56j2rR8Vx59R03XDnWWW6QK0lP2GP9whW7JNL7h3k-V5yW8FJLG88BPrw7VhxCN64tWXyZW4MMRwD3Y28K4f6PGvHs04) available for non-commercial and commercial uses up to 100 million monthly active users or monthly revenue of $20,000,000 under “[modified MIT license](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWG0zr6Jh43BW5kvlP23xC40YW6mk3Cf5zlJMgN8pR4CT3qgz0W8wLKSR6lZ3pTW3WzcW96gqBJPVFgLpk5WTQ8QW6FFR2p94_BzWW7L3NjC1bQfyRW6FMWM49l82NtW4pF_jb3cxQBRW663wL72qH8yCW1FdFsQ6MQCh-N4zMmRh8Tyx8W44hncT5fzys5W4K-yBy4bgc2WW7L4Rr87LlNLHW7Tzvb74hlJjpW4j7P2B3jC4krV9vs5h68FzjWW8bw6fS3zcVp1VzPrnS76MVQfW86DV491pT66RN7-tXmt6Dv9GW2KfnX-1t7s2hW2tjzYd7dJrWxW6p-BTC4q9HQvVbxTlC5r4yBgW3bPD1R3vDbBbW1V-_YY8SlSKFW5Lvx0B12S_KTV8NDDz8RqyhTW8Fpbt61rM08qf6qk8N204)”
- **Features:** Tool use including web search and arbitrary tools
- **Undisclosed:** Specific training methods, training datasets  
**How it works:** Moonshot pretrained the models on 15.5 trillion tokens from undisclosed sources. It fine-tuned Kimi-K2-Instruct via reinforcement learning using a proprietary dataset.

- To enable Kimi-K2-Instruct to use tools, the team generated a large dataset of examples in which models used tools, both real-world and synthetic, that implement model context protocol ([MCP](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWG0zr6Jh43BW5kvlP23xC40YW6mk3Cf5zlJMgN8pR4D83qgz0W95jsWP6lZ3pDN4GfXYxVtL5pW4lpZRM5jZpBvW8WX6VP7NfhknW6d3YX_7k9MKgW7MG4d75vsFz1W6p99BM43D-ZbW1B4tYS75CYFQV8gFnr68x4h-W72zNT42Y_Dx2VlGDbp12KBBSW3H3j-x2RCt2YW7gH3bn30vGMpN4DWRt-7-bGbW4gXRgp3vKd53VX82Db6FyZqMW2X44pX8QdR7hW4xNQXn53b2SVW1YCm8H7Dq0-qW2Kn9W28P_TjkW8QYGxD80HJlSW6w9pTx3K_j2cW92244Z3C-XsbW5CLnXx3GxchRW5_SS-X7wLK-GW7vZy3c7lQm8bW8zgns35BlWsDW3JQgSL6vDxzPW1w6nX64LxCxPN2TJSMGk0L5-W5jBQvC8Qkbx6f5h2LqC04)). Unidentified models acted as users, and other unidentified models acted as agents that solved tasks assigned by the users.  A further model acted as a judge to filter out unsuccessful examples.
- The team fine-tuned Kimi-K2-Instruct via reinforcement learning. The model evaluated its own performance, used its evaluation as a reward, and iteratively improved its performance.
- The team also fine-tuned Kimi-K2-Instruct to solve coding and math problems via reinforcement learning. The model did not evaluate its own performance on these problems; it determined rewards according to pre-existing solutions or unit tests.  
**Results:** Moonshot compared Kimi-K2-Instruct to two open-weights, non-reasoning models (DeepSeek-V3 and Qwen3-235B-A22B with reasoning switched off) and four closed, non-reasoning models.

- Kimi-K2-Instruct outperformed the open-weights models across a range of benchmarks for tool use, coding, math, reasoning, and general knowledge.
- It achieved middling performance relative to the closed models, though it did relatively well in math and science tasks.
- Compared to all models tested, on [LiveCodeBench](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWG0zr6Jh43BW5kvlP23xC40YW6mk3Cf5zlJMgN8pR4Cg3qgz0W7lCdLW6lZ3ntW5CYHMS1Gy-smW7j1Ymm8qb0g2W2yLlr_5QmHfvW7_Y0R992jL2zW5jXgkX2Sqm6QW9b7_F140NWZxW12xQmJ6b8jXGW4DFcB13ZMh3qN931Fy-dsFM_W150Lx44qdlfJW3dRv4n14_0V0W68C0sd7jBk37W75_pGK5_BWtYW4R2pBn1FqnZMN7vlk9TF9ZtBW14VwBd4VSvRnW93ZY181LT5nVW4K29pZ3b23ZgVWwz575g0Z1fW2lzRlp4V8Gs1W4Y1Q3P3JX98cM4__22BRdSBVg_kZq3j-4tmW8Qty_01jrns2f7Lf58n04) (coding tasks), Kimi K2 (53 percent) achieved the best performance, ahead of Claude Sonnet 4 with extended thinking mode switched off (48.5 percent).
- Among all models tested, on [AceBench](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWG0zr6Jh43BW5kvlP23xC40YW6mk3Cf5zlJMgN8pR4Cg3qgz0W7lCdLW6lZ3kKW61lHN36Dl2pXW1f-Kc61tCLnCW16kWVb2pslmNW57PmQm9lj5gCVg2dPb5SLq2cW3N-dyZ1-_j9nW7QP72G61gFqZW2wKSQP7QcZ3TW1M9yQF4TY57CW8sWRLm2Q2sXSW4Q5pbv3hkBP5W2Gq5tJ7Ffr8yW4cT3mw38Lp5ZN15CbYBynw7zW1D-M6Y8NJLhbW7B_B4p6jKcnfW32r0T08Pnc7vW7RPRcb8xFpCFW4Cszz54z1G6pW6rVNsD97rKJ8W8PpNMK8ptDQZW5X757p5frWlhW97wMkp8xZ10jN7Q9q1QxWGqHf2nt_lM04) (tool use), Kimi K2 (76.5 percent accuracy) placed second behind GPT 4.1 (80.1 percent accuracy).
- On 8 out of 11 math and science benchmarks, Kimi K2 achieved the best performance of all models tested.
 
**Behind the news:** Third-party vendors have been quick to implement Kimi-K2-Instruct.

- The [Groq](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWG0zr6Jh43BW5kvlP23xC40YW6mk3Cf5zlJMgN8pR4Cz3qgz0W7Y8-PT6lZ3pwW3FH7Vq3mbgHmW17wyKP1DJw1SVd0MWC20b6yKVjdTxR2QyvvtVs23h78wQf3LN8Sy8P32B92BV8G_2v1SSMgkW284D6X6f5DmCW68Dw8x2YG1kVN7TmYYLqLB35W45xWDk4l_rkhW4m7j1P62WK84MBhRZ0V23F6W5ylghd3-K6dtN6RgWKsd0F9hW3hx73-1_89pmW6qqlW46H3y0xW9c2WyK46vHYsW61VzPZ8WXB8xW26ZdSf8-b-9xW72vjSG3h4cVFVVxm3J68N_CqV2sg-J8GPpCfW85vR542vT6cZW6TXpqC9cwwVFW25XYxd6MW64wf7r7mwP04)’s platform accelerates Kimi-K2-Instruct’s output to about 200 tokens per second ($1/$3 per million input/output tokens) compared to 45 tokens per second [reported](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWG0zr6Jh43BW5kvlP23xC40YW6mk3Cf5zlJMgN8pR4Cg3qgz0W7lCdLW6lZ3k-N97csZ9JpW6pW5TtZsz3yd-LxW41vTRd98LHSHW60lm-x10V51nW62NQG08gQfZNW66TpP46NstRBN7SLjPV8-97cTpMjr7nps0mW2C3sj23531-rW8hVHyg5xr28NV7mQzp28_K1WW7vfCLR7jyX73V5L4SZ7GtbnBW8K5jMv6JfB5QMQRn6cWyTFtVNk3m171fPhtW83vSCZ35f4nbVbpbJ_9lHkKkW8KPQB82Ss5ydVpk6kW20hWjGW7dSRYp7wqFy1W778Hqm8s8-19V9ppkX7DJRLNV_dZdK6Ms-NBf1fCNfg04) by Artificial Analysis.
- The fine-tuning platform [Unsloth](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWG0zr6Jh43BW5kvlP23xC40YW6mk3Cf5zlJMgN8pR4Cz3qgz0W7Y8-PT6lZ3lNN735GbF1H-HnW62LQ6Q3tNqybW8p_PVl6DF-tCMX-nhzFFMgHW17xNWL5s3DBCW3QWbbZ5gvv6hW3-1YRw45jKrSW77TX4g5DdMyWVhSV8f50MJW2W8XQjks99RLX5W7YN0Tv20G_4zW2jjjhW11QtpnW4Yn8yh8V6W9BW3LkTwc787BvLW6KcZyc18gDvzW72rnnt2wH8hYW4bgSNn6HSN3cN8X8l0YfyTlkN6s1v1NrLtTXW10lTdf41vZG8W5l8Kbb2xfMWFW4y4LbD6gG5ZdW18kw0h5TfvXTW8xCM5t3GzrSyW1wzLtG2zY0rxW97MYC1588mtsf9fQLSg04) released quantized versions that run on local devices that have 250 gigabytes of combined hard-disk capacity, RAM, and VRAM.
 
**Why it matters:** Demand is growing for LLMs that carry out agentic workflows accurately, as these workflows lead to better performance. Kimi-K2-Instruct gives developers a strong option for fine-tuning models for their own agentic tasks.  
   
**We’re thinking:** Early LLMs were built to generate output for human consumption. But the rise of agentic workflows means that more and more LLM output is consumed by computers, so it makes good sense to put more research and training effort into building LLMs that generate output for computers. A leading LLM optimized for agentic workflows is a boon to developers!
 \> From \<[https://mail.google.com/mail/u/0/#inbox/FMfcgzQbgRnJpmCfnSxSdJhCvkvKjVKp](https://mail.google.com/mail/u/0/#inbox/FMfcgzQbgRnJpmCfnSxSdJhCvkvKjVKp)\>