ðŸ”§ Precise manipulation meets generalization -- with just 32 demos and 45 minutes of interaction.
 
Robots are getting better at learning from large-scale data -- just like weâ€™ve seen in vision and language. But when it comes to precise tasks like inserting plugs, swiping cards, putting keys in locks or plugging USBs, scale alone isnâ€™t enough. These contact-rich tasks demand millimeter-level accuracy, and collecting diverse, high-quality data is difficult. This leads to an unwanted tradeoff: generalization vs precision.
 
We introduce VisuoTactile Local (ViTaL) policies -- a framework that leverages the complementary strengths of vision and touch to achieve generalizable, precise control for contact-rich manipulation. Our framework has two components:  
ðŸ§  Global policy (e.g., a pretrained VLM) handles coarse semantic localization.  
âœ‹ Local ViTaL policy takes over for the last-mile of precise, contact-rich execution.
 
ðŸ’¥ With just 32 demos per task and 45 min of real-world RL, ViTaL achieves \>90% success on 4 contact-rich tasks -- inserting plugs, swiping cards, putting keys in locks and plugging USBs -- in cluttered, unseen environments. ViTaL policies can be trained in the lab and deployed in kitchens, homes and offices without any retraining!
 
So how do you train a ViTaL policy? Two simple steps:  
1ï¸âƒ£ Behavior Cloning with semantic augmentations for robust visual generalization. This policy excels at reaching, but fails about ~50% of the time at the contact-rich portion of the task.  
2ï¸âƒ£ Visuotactile Residual RL effectively leverages tactile feedback for offset-based refinement, while maintaining the generalizability of the behavior cloning phase.
 
ðŸ”‘ Key insights:  
1ï¸âƒ£ Tactile sensing is critical -- removing it drops performance by ~40%.  
2ï¸âƒ£ Egocentric vision offers consistent spatial context tied to the robotâ€™s frame, enabling deployment on new robots.  
3ï¸âƒ£ Semantic augmentations improve generalization under scene and spatial variations.  
4ï¸âƒ£ Residual RL with strong visual encoders can boost task performance while preserving robustness.
 
This work would not be possible without Zifan Zhao's relentless pursuit of precise policies that actually generalize, Siddhant Haldar's invaluable insights on policy learning and residual RL, and consistent feedback from Jinda Cui and Lerrel Pinto.
 
For more details and videos: [https://lnkd.in/eXP5xTht](https://lnkd.in/eXP5xTht)  
Check out our paper for a comprehensive ablation study: [https://lnkd.in/e-wARCsB](https://lnkd.in/e-wARCsB)  
Open-source code: [https://lnkd.in/ezTYsD4Q](https://lnkd.in/ezTYsD4Q)
 \> From \<[https://www.linkedin.com/my-items/saved-posts/](https://www.linkedin.com/my-items/saved-posts/)\>