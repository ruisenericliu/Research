[https://deepmind.google/discover/blog/facts-grounding-a-new-benchmark-for-evaluating-the-factuality-of-large-language-models/](https://deepmind.google/discover/blog/facts-grounding-a-new-benchmark-for-evaluating-the-factuality-of-large-language-models/)
 
[https://arxiv.org/pdf/2407.01219](https://arxiv.org/pdf/2407.01219)
 
Don't Let Your Robot be Harmful: Responsible Robotic Manipulation: [https://arxiv.org/abs/2411.18289](https://arxiv.org/abs/2411.18289)
 
GSM Mathematics: [https://arxiv.org/abs/2410.05229](https://arxiv.org/abs/2410.05229)
 
Forcing structure: [https://arxiv.org/pdf/2408.02442](https://arxiv.org/pdf/2408.02442)
 
It is fresh news that OpenAI just dropped a new update to its most powerful model to force it better structure output (JSON/Pydantic). But at what cost?
 
A recent study, "Let Me Speak Freely? A Study on the Impact of Format Restrictions on Performance of Large Language Models," offers valuable insights on the effects of forcing LLM to output structured data.
 
- Performance Decline: LLMs show reduced reasoning and domain knowledge when confined to structured formats like JSON or XML.  
- Stricter Constraints: Greater performance drops occur with more stringent format constraints, impacting complex tasks.  
- Flexibility Matters: Allowing LLMs flexibility enhances their capabilities, suggesting the need for adaptable applications.  
- Classification Performances: On the other hand, classification tasks seem to be not degraded at all by JSON mode, in some cases even over performing the vanilla mode.
 
Link to the paper in first comment.
 ![Exported image](Exported%20image%2020260222122052-0.png)  

HYDRA: [https://arxiv.org/abs/2306.06272](https://arxiv.org/abs/2306.06272)
 
Learn to Decompose: [https://www.arxiv.org/abs/2408.06843](https://www.arxiv.org/abs/2408.06843)
 
RALM - BAIDU / Perplexity -  
[https://arxiv.org/pdf/2407.19813](https://arxiv.org/pdf/2407.19813)  
Venture Beat XD: [https://venturebeat.com/ai/baidu-self-reasoning-ai-the-end-of-hallucinating-language-models/](https://venturebeat.com/ai/baidu-self-reasoning-ai-the-end-of-hallucinating-language-models/)
 
Pastel - STL generation: [https://www.linkedin.com/posts/ashish-kapoor-a2971b6_robotics-machinelearning-safeautonomy-activity-7220241699518734337-iPY1?utm_source=screenshot_social_share&utm_medium=member_ios&utm_campaign=copy_link](https://www.linkedin.com/posts/ashish-kapoor-a2971b6_robotics-machinelearning-safeautonomy-activity-7220241699518734337-iPY1?utm_source=screenshot_social_share&utm_medium=member_ios&utm_campaign=copy_link)
 
LLM to PDDL:  
[https://towardsdatascience.com/thinking-fast-and-slow-with-llms-and-pddl-111699f9907e](https://towardsdatascience.com/thinking-fast-and-slow-with-llms-and-pddl-111699f9907e)
 
**2)** **Language Models as Zero-Shot Planners** **(Huang et al) â€” studies the possibility of grounding high-level tasks to actionable steps for embodied agents. Pre-trained LLMs are used to extract knowledge to perform common-sense grounding by planning actions.**