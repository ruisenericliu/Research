With OpenAI, Figure 01 can now have full conversations with people
 
-OpenAI models provide high-level visual and language intelligence  
-Figure neural networks deliver fast, low-level, dexterous robot actions
 
Everything in this video is a neural network:
 \> From \<[https://www.linkedin.com/feed/update/urn:li:activity:7173681028664901634/](https://www.linkedin.com/feed/update/urn:li:activity:7173681028664901634/)\>     

I am curious how much of this is truly scripted/pre programmed and how much it dynamically determined. I am certain the responses were dynamically generated by a LLM (which is already crazy that we are taking this for granted now), and that the robot automatically determined what actuators to move how to get to all the positions it needed to. However the taking the output of that large language model's interpretation of the users input and translating it to what the robot SHOULD do is what's blowing my mind, because saying I am going to pick up a dish and actually command arm 1 to go to xyz and fingers to do abc is what's so impressive.
 \> From \<[https://www.linkedin.com/feed/update/urn:li:activity:7173681028664901634/](https://www.linkedin.com/feed/update/urn:li:activity:7173681028664901634/)\>      
[Shawn Fumo](https://www.linkedin.com/in/shawnfumo/) You're probably talking about this: [https://x.com/coreylynch/status/1744106386211152057?s=20](https://x.com/coreylynch/status/1744106386211152057?s=20)
 
That explanation makes sense. However, having seen the demo, I strongly believe that the visual reasoning (describe visual experience, plan future action, reason its action) demonstrated on their demo is generated by the multi-modal LLM being used. Then based on the task instruction ("Get me something to eat" or "Sort the trash"), the most relevant behaviour(end-to-end visuomotor policy) is selected which they've already learnt
 \> From \<[https://www.linkedin.com/feed/update/urn:li:activity:7173681028664901634/](https://www.linkedin.com/feed/update/urn:li:activity:7173681028664901634/)\>