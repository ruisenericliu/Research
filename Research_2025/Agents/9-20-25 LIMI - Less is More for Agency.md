GAIR has just dropped a groundbreaking paper on Hugging Face: "LIMI: Less is More for Agency," fundamentally reshaping how we approach autonomous AI development.
 
Their research challenges the traditional "more data is better" paradigm. Instead, LIMI demonstrates the "Agency Efficiency Principle": sophisticated agentic intelligence emerges from strategically curated, *minimal* demonstrations.
 
The results are stunning: LIMI achieves an impressive 73.5% on comprehensive agency benchmarks, dramatically outperforming state-of-the-art models. Most strikingly, it shows a 53.7% improvement over models trained on 10,000 samples, all while using 128 times fewer training examples (just 78!).
 
This means a future where developing highly capable AI agents could be significantly more resource-efficient and sustainable.
 
Researchers, this is a must-read if you're building AI that doesn't just think, but *works*. The paper, models (LIMI & LIMI-Air), and the carefully curated 78-sample dataset are all available on the Hugging Face Hub, making it incredibly easy to reproduce and build upon this work.
 
Explore the paper, models, and dataset now:
 
Paper: [https://lnkd.in/en4p7sUn](https://lnkd.in/en4p7sUn)
 
Models:  
LIMI: [https://lnkd.in/eAjqFM4g](https://lnkd.in/eAjqFM4g)  
LIMI-Air: [https://lnkd.in/eEgDZPc7](https://lnkd.in/eEgDZPc7)
 
Dataset: [https://lnkd.in/e3uXqNiH](https://lnkd.in/e3uXqNiH)
 
Code: [https://lnkd.in/e3cjZ-B2](https://lnkd.in/e3cjZ-B2)
 \> From \<[https://www.linkedin.com/preload/](https://www.linkedin.com/preload/)\>