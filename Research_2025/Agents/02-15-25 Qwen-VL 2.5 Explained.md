How are Vision Language Models trained? ðŸ–¼ï¸ Qwen 2.5-VL demonstrates state-of-the-art performance through dynamic resolution processing, absolute time encoding for videos, and a redesigned Vision Transformer in 3B/7B/72B variants for edge and cloud deployment. ðŸ‘€
 
Pre-training (Three-Phase):  
\> Phase 1 (ViT Only): Train only the ViT on image captions, visual knowledge, and OCR data (1.5T tokens, sequence length 8192).
 
\> Phase 2 (Multimodal): Unfreeze all parameters. Train on interleaved image-text data, VQA, video grounding, agent-based interaction data, and pure text data (2T tokens, sequence length 8192).
 
\> Phase 3 (Long-Context): Train on long video, long agent, and long document data (0.6T tokens, sequence length 32,768).
   

Post-training (SFT & DPO):  
\> Supervised Fine-Tuning (SFT): Fine-tune the model (with ViT frozen) on a 2 million instruction pairs on VQA, Document processing and OCR, Grounding, Video analysis, and Agent interactions, and more.
 
\> Direct Preference Optimization (DPO): Align the model with human preferences using image-text and pure text data (with ViT frozen)
 
Insights  
ðŸŒ Native dynamic resolution processing (handles 4Kâ†’224px natively)  
â±ï¸ Implements MRoPE (Multimodal Rotary Position Embedding) aligned to absolute time for temporal understanding  
ðŸ–¼ï¸ Window Attention reduces ViT computation by 40%  
ðŸ¤– 93.7% success rate on Android Control LowEM agent tasks without auxiliary marks  
ðŸ“š Trained on 4.1T tokens including synthetic HTML-formatted docs with layout annotations  
ðŸ“± Smaller models (3B/7B) can be run locally  
ðŸŒ Multilingual support across 10+ languages for OCR and document understanding
 
Paper: [https://lnkd.in/eBhSKusb](https://lnkd.in/eBhSKusb)  
Models: [https://lnkd.in/e9HH-umX](https://lnkd.in/e9HH-umX)
 \> From \<[https://www.linkedin.com/my-items/saved-posts/](https://www.linkedin.com/my-items/saved-posts/)\>