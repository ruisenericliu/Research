![No alt text provided for this image](Exported%20image%2020260222122209-0.jpeg)   
ð—£ð—¼ð˜ð—²ð—»ð˜ð—¶ð—®ð—¹ ð—½ð—®ð—¿ð—®ð—±ð—¶ð—´ð—º ð˜€ð—µð—¶ð—³ð˜ ð—¶ð—» ð—Ÿð—Ÿð— ð˜€: ð—»ð—²ð˜„ ð—½ð—®ð—½ð—²ð—¿ ð—¯ð˜† ð— ð—²ð˜ð—® ð—°ð—¹ð—®ð—¶ð—ºð˜€ ð˜ð—µð—®ð˜ ð˜„ð—² ð—°ð—®ð—» ð—´ð—²ð˜ ð—¿ð—¶ð—± ð—¼ð—³ ð˜ð—¼ð—¸ð—²ð—»ð—¶ð˜‡ð—²ð—¿ð˜€! ðŸ¥³
 
Current LLMs process text by first splitting it into tokens. They use a module named "tokenizer", that -spl-it-s- th-e- te-xt- in-to- arbitrary tokens depending on a fixed dictionnary.  
On the Hub you can find this dictionary in a model's files under tokenizer.json.
 
âž¡ï¸ This process is called BPE tokenization. It is suboptimal, everyone says it. It breaks text into predefined chunks that often fail to capture the nuance of language, struggles with non-english languages (grouping "the" as 1 token makes sense for english, but is useless in other languages). But it has been a necessary evil in language models since their inception.
 
ðŸ’¥ In Byte Latent Transformer (BLT), researchers at [Meta](https://www.linkedin.com/company/meta/) propose an elegant solution by eliminating tokenization entirely, working directly with raw bytes (on a lower level, text is encoded as bytes: 011011110101) while maintaining efficiency through dynamic "patches."
 
This had been tried before with different byte-level tokenizations, but it's the first time that an architecture of this type scales as well as BPE tokenization. And it could mean a real paradigm shift! ðŸ‘ðŸ‘
 
ð—§ð—Ÿ;ð——ð—¥:  
ðŸ—ï¸ ð—”ð—¿ð—°ð—µð—¶ð˜ð—²ð—°ð˜ð˜‚ð—¿ð—²:  
Instead of a lightweight tokenizer, BLT has a lightweight encoder that process raw bytes into patches. Then the patches are processed by the main heavy-duty transformers as we do normally (but for patches of bytes instead of tokens), before converting back to bytes.
 
ðŸ§© ð——ð˜†ð—»ð—®ð—ºð—¶ð—° ð—£ð—®ð˜ð—°ð—µð—¶ð—»ð—´:  
Instead of fixed tokens, BLT groups bytes based on their predictability (measured by entropy) - using more compute for complex sequences and efficiently handling simple ones. This allows efficient processing while maintaining byte-level understanding.
 
ðŸš€ ð—£ð—²ð—¿ð—³ð—¼ð—¿ð—ºð—®ð—»ð—°ð—²:  
BLT matches or exceeds token-based models like Llama 3 while using up to 50% less compute at inference! It shows particular strength in handling noisy inputs, different scripts, and character-level tasks.
 
This shows that removing tokenization altogether can not only maintain, but improve performance and efficiency, challenging the long-held assumption that tokenization is necessary for practical language models.
 
I hope this breakthrough is confirmed and we can get rid of all the tokenizer stuff, it will make model handling easier!
 
Read their paper here ðŸ‘‰ [https://lnkd.in/eavCY4_2](https://lnkd.in/eavCY4_2)
 \> From \<[https://www.linkedin.com/feed/update/urn:li:activity:7273382398891810816/](https://www.linkedin.com/feed/update/urn:li:activity:7273382398891810816/)\>