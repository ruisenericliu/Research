Datasets that were scraped from the web tend to be unbalanced, meaning examples of some classes (say, cats) are plentiful while examples of others (say, caterpillars) are scarce. A model that’s trained on an unbalanced dataset will perform unevenly across classes, but the labor required to balance the data manually can be prohibitive. An automated method addresses such imbalances.  
**What’s new:** Huy V. Vo and colleagues at Meta, France’s National Institute for Research in Digital Science and Technology, Université Paris Saclay, and Google proposed a [method](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVrlKm2wY_jpN8zW33_wz3tJW5NMkyh5kSvpCM1--jz3qgyTW6N1vHY6lZ3pVW5Wr7V73M88smW2fZzM43wGjbFW67jB773Vxx4qW3LZMlQ7Gq3-zVL2Lwl7-CnjLW7v5wmF6K2nClW2fRSkV2Z6K2bW99xD6z5YQqHLVcycGB50WypCVnqsfg3sPKHSW26zK845DvyLbW2mnCj26QqFXfW6Q-QN98mngvqW5z03Bw70sDf4W1dTY9b7PJmVcW6pPLhK8jNfBYW2D6VpD63qBNSMRSVYq50N4rW7T8H2W3nj8XcW6klMFV5v6jJHW7_fftm7rlCT9w7XlQNcbNYf5CTc2s04) that automatically selects a balanced subset of text or image datasets.  
**Key insight:** A naive way to balance a dataset automatically is to cluster it using [k-means](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVrlKm2wY_jpN8zW33_wz3tJW5NMkyh5kSvpCM1--jT3qgyTW7lCdLW6lZ3l4W28tZCY3fd1B3W86b5FV5rW8w1W3j0m5F6sc1D-W7-Sbj_4GbS3KN40SPvN2vzblW5-_vHh2SxyhJW2BLZ3x9fYM95N1Ysw2GvzM6wW1Nb77t25TzqKW975VmP1jqZCgN80CvNHFDQY3W6Z5qRl8y3_nqVKPJ3p5bXPGrW87Bh3q8Gpj9sN5Vl8yNK0CbSW3jKYvn6n9y6HW8ndpFZ28CrjwW5PZp1r2LDw4nW2453GP5L7PpnW3mXybz4_PRy5N55W4f0KjwNrW6SCMDH4h95nXW4m1-yc7D300GW9bM2wd1nMgqLf2z9xYd04) to define implicit categories and then draw an equal number of points randomly from the resulting clusters. But this approach tends to form many clusters in areas of the distribution that have more examples, leading to over-representation of certain categories. For instance, when the authors applied k-means to web images and associated the clusters with their nearest neighbors in ImageNet, around 300 clusters (out of 10,000) corresponded to the ImageNet class “website.” However, after clustering, the distribution of the centroids is a bit more uniform than that of the entire dataset. Applying k-means repeatedly distributes the centroids (and thus the clusters) more uniformly. After a number of iterations, each cluster is more likely to represent a distinct category, and selecting equal numbers of examples from each cluster makes a balanced dataset.   
**How it works:** The authors balanced image and text datasets using several iterations of k-means clustering. Their image dataset started with 743 million examples from a “publicly available repository of crawled web data.” For text, they started with [CCNet](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVrlKm2wY_jpN8zW33_wz3tJW5NMkyh5kSvpCM1--jT3qgyTW7lCdLW6lZ3mDW6LkH_m992Dn_N7_VVN4HTgKbW8nKPhz5KxBxVVrZ4sX60vgxgW3ClVXJ5BZvRWW4Z2XVJ2BqPMkW16HPgV802TcZN3bgnMcWJYCMW3F38Fr5mWKllW6znLfC14-m5WV7Ppsl2Xn1YPN2K9P9018k3MW37G51X7H98S-W3s-Pjs84lTbFW7CfLQT64Xl6dW90ySLm67-G88W6xzKrL3Q_-HHVs33jh40xsMsN3LjdCG_3KCRW1GfDdl4Cxj-MN6m_DtMkcKYSV2B5fm4LFKzRW6W4DMr7YLvX1W4tP9lq5CyqT5djjKvR04), a version of [Common Crawl](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVrlKm2wY_jpN8zW33_wz3tJW5NMkyh5kSvpCM1--jg3qgyTW69sMD-6lZ3nPN753vNrpvwLPW16XhB58yjXJ_N3ZBDntp0qpwW3PF6GF3xPgvhW4N_Pm22mwtbxW7hpS3S3_1k2jW5n83px7mNkfCW9j8q2l8synDxW1g0DTD1D1xvxV1Y2qZ1FspTmW3J7fkn5YLqjhW5vrQ8B2cV0JRVfHSns8gPWX4W45hnZw2NH85QW5C-s2J7QGyZkW9jdG6k8wG1l3W2fqHMm5VDpTxW8-JP1-5l0771W6M3ly3384PHdW53khfm7Nsq9Rf2_dJ_T04) that was filtered to match the distribution of language and topics found in Wikipedia. The following approach ensured balanced sampling from all levels, maintaining a balance among high-level classes (such as animal, vehicle, and sport) and lower-level subclasses (such as dog, airplane, and football):

- The authors embedded the data. They built an image-embedding model by training a [ViT-L](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVrlKm2wY_jpN8zW33_wz3tJW5NMkyh5kSvpCM1--jz3qgyTW6N1vHY6lZ3q6W2K74Vl1CQtkgW76L6vQ8WNQJ8W9bJ3c_80v8crW1_lGBt5TJcjpW877HhP59xCGDW4wxhS124pTHqW2zqwn9798kpFW5dsG8r1R3B1mN6Pgn_nDDKVFW4WMSD43lb2jmW5r3-gZ4m10BQW2SPnHr4lw178W8qvt4R5PYGNcW44lQCG7D5zknW19b44w5DYycWMwTFTjBH99DW1x2Tms6VJBcXW2JLF8r46DLRfW2vmhqL14N2V3W2fr7Gq4H_1DPW2L3JTz61y1VyW5BdB005yx3Pwf4ZvWsj04) (307 million parameters) on [ImageNet1k](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVrlKm2wY_jpN8zW33_wz3tJW5NMkyh5kSvpCM1--jg3qgyTW69sMD-6lZ3mZW4_QvDF1Tst3zW31rFCv1T16sZW5XlHWL3q9DwcW3LhJPF1cF2zFW1ZcDc78v3l20N8Cfb7PW6MjkW4LYWKl2Gg4RJN67rmsJyssq6V7F48q7p9fBrW5J0hXL1Qh6TmN2Dd87wTVXn9W69dSQP82vj5PW92DJS53jLv-fW5yQz-w3VPYYyW95Wy-t5ZCrN6W5k_jxs4t7m-6VkKdwG3VrlHXW6DJt0D5vvfPnW5CNCnb18gxnTV14NY344LW2Lf1T9Vpz04) according to the [DINOv2](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVrlKm2wY_jpN8zW33_wz3tJW5NMkyh5kSvpCM1--jz3qgyTW6N1vHY6lZ3kSW8Xf96q4pyXF5W8lkvhY7c165vW1_NYpL4lvR51W5J3mhN8yFS36W70ZFtJ3GbDNpW47RbKW5sfD0MVvrP881bnbg4W72XRx73zrny-W4cYdBk6-LBlpW66k9G76yWb02W1cp4fD75YNX1W2YWh7F8gmHsxW8SQ9Xr486w6gW1JBbvH8lxRLQW1rgJzH5Z14KpW6sSHWV6MClzJW4g762F99PlQNW1LjKRg2tCnFvW12VwxP2hbV41W1YvvH08Sj-PnW1FMpV-7FM6jRW2ZlQVL1kJ0GGf1L-M8004) self-supervised training method. To embed text, they used a pretrained [SBERT](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVrlKm2wY_jpN8zW33_wz3tJW5NMkyh5kSvpCM1--jz3qgyTW6N1vHY6lZ3q2W43X_fr2WLTh3W6V11XZ2yjsljW4NKLtr8ZfbfvW5P2kLc5wdL4jVbH21q4njVNXW6qQWwH4-bF-QW5tFPMy5ZbVl1W8mD7Gg1z6htwW3r6L8f52WGlgW5xY3Q81fF2JrN68Y1bgM_q45W7bvL-p3LMgLwW4PSZB255vLTcW5xds-c5_DBJhW1jkTD25FXl2HW7d6CGw4XDr2fW1PnKc35pznXRW8l2DKB70DRXcW533dyP1sl1PFN8mM6N3KtkZ2W9hXYYd7Gwq3qN8hZZG5VQyQhf8xDGXK04).
- They clustered the data via k-means to produce 10 million clusters.
- They selected a small number of points closest to the centroid of each cluster. Then they applied k-means to the selected points to find new centroids. They repeated this process four times, each time decreasing the number of clusters, so the new clusters represented higher-level categories. With each iteration, the distribution of centroids became more uniform.
- Using the resulting hierarchy of clusters, the authors randomly selected balanced datasets of 100 million images and 210 billion text tokens. Specifically, starting with the highest-level clusters, they computed the number of samples to be drawn from each cluster. Then they looked up which clusters in the previous level were contained within each of the clusters in the current level and determined the number of samples to be drawn from each of these subclusters. They repeated this process at each level. In this way, when they reached the lowest level, they knew how many points to draw randomly from each of the lowest-level clusters. The points they drew made up a balanced dataset. 

**Results:** Both vision and language models that were pretrained on the balanced data outperformed models that were pretrained on the corresponding unbalanced datasets. 

- To test their balancing method on image classifiers, the authors pretrained [ViT-g](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVrlKm2wY_jpN8zW33_wz3tJW5NMkyh5kSvpCM1--jz3qgyTW6N1vHY6lZ3lzW8WKxR65jZPMYVrfXLp18cyFmW43S2yK5gKYJBW6vR82L1ys6NyN4W9Sj83RBPRW7n0Gw_5mF2MBW6TvRH13-FKMBW4xnV1v7N57D6VLvC1f8c749NW67Hgkt6VxLNJW8JY84X3Rsd4ZW8-1m671kslZGW5xZ9v86mGFC1W2V_ZWL8bb7vfW3h_Xxt79qk_rW48t5J91lthFMW2KD2Pg2slgyPW9f4h9L3Y152bV8RwjH7pwqyWN4vHkgCmQkfKW59NZC955dlSQW5vD9tb1WnR1bf3tGNm204) models on their balanced dataset and the unbalanced raw data. They froze the trained models and fine-tuned a linear layer on top of them to classify ImageNet. Pretrained on their balanced dataset, ViT-g achieved 85.7 percent accuracy on the ImageNet 1k validation set. Pretrained on the unbalanced dataset, it achieved 85.0 percent accuracy.
- To test their method on language models, they compared performance on various tasks of [LLaMA-7B](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVrlKm2wY_jpN8zW33_wz3tJW5NMkyh5kSvpCM1--jz3qgyTW6N1vHY6lZ3kzVQwGWN6j2xh0W1tSSqV6gR6YjMp2XM26RRLHVkPp0g8FBFBXW8WZnXT7-8TL9V-3kQh6DdK1XW8W-mjl1NyLHlW4CNm3M79GB3qW7HMcCr4ch0KWW8gzgv_3csZgNW9lTcKS1YpxwtW1dWmHC1SQQ4CW86x2KR1l6XHjW7B-pMK2G75lLW8W1bpn749ZFZW4GrM3F77m20hW1r53v58jZnxwW43hHKL6nVqgJW1yMfbF5CtyRNW6fDCDW179y1QVwW5QQ8lzK-VW90PmSP92ld7vf2L6q3604) models that were pretrained on their balanced version of 210 billion tokens in CCNet and the unbalanced CCNet. For instance, on the [HellaSwag](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVrlKm2wY_jpN8zW33_wz3tJW5NMkyh5kSvpCM1--jz3qgyTW6N1vHY6lZ3nbW3G9Y8J6ss5YfW1V1vTt1l_G6GW4b3sQ37FdwfkW2Ks6R57q-p6gW4zJyQf3qKYCQW5QT98c5sXPRZW5Gn2-V5p4cpMW4MPFz51C3wd4W2clpkM8qgKkCW4FqkZt83RBxMW7DQm3k7SKkVsW3lw7QG6KHlSQW2B1LF68h7l87W58hzCJ3cndQlN4CtNZ0rSt0VW4f5MC03QFTbYW5gTqBB865MWwN3xnqWBG6CntW5NhXhn8bjv2-W9lngWG3y_m7yVsJ2KT4tjCp6N6pFr7y4zKzvdy3X8n04) question-answering dataset (zero-shot), the model pretrained on balanced data achieved 52.7 percent accuracy, while the model pretrained on unbalanced data achieved 51.9 percent accuracy. Similarly, on [Arc-C](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVrlKm2wY_jpN8zW33_wz3tJW5NMkyh5kSvpCM1--jz3qgyTW6N1vHY6lZ3lkW6xpr8017HV3mW2VYFtm2nld8tW8RwSHT6tTxsQW5LGq3x7-r_c3Vy9gK-2FjzhVW6ywnRP2qbT_qW8Q4PhM1kdTWLW3q48hL3CgZpBN7x0pL_n9Bt7W4s26Lt6jfZ4jW1QkWnc4_SmxFW86-91V1Rj3fMW2qCbXj1Z-rgYW2KdJF81dZX8TW1JLp7b62dLj8W2XL0Hd5QSM_FW5R-fx09bLpQWW8yDnrC5mxy0wW2zjWJp6PjvKTW7nDGkm74C2T0W6jhSFs99LLBVW3tXBks1WCp2Kf7Dmk4d04) (questions about common-sense physics such as the buoyancy of wood, zero-shot), the model pretrained on balanced data achieved 40.1 percent accuracy, while the model pretrained on unbalanced data achieved 35.5 percent accuracy.

**Why it matters:** The old-school machine learning algorithm k-means can organize quantities of pretraining data that are too large for manual inspection yet crucial to data-hungry models. Breaking down data into clusters also makes it possible to manually inspect cluster elements, which might help identify unwanted data.  
**We’re thinking:** Even in the era of foundation models, data-centric AI — that is, systematically engineering the data used to train such models — remains a critical, often under-appreciated step. This paper offers a promising way to create more balanced datasets. The encouraging results suggest fruitful avenues for further study.   
 
 \> From \<[https://mail.google.com/mail/u/0/#inbox/FMfcgzQVzXgcHZXDjNwkvWBWQhVVclrP](https://mail.google.com/mail/u/0/#inbox/FMfcgzQVzXgcHZXDjNwkvWBWQhVVclrP)\>