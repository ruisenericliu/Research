Datasets/Evaluation:  
LIBERO:  
ðŸ“„ LIBERO paper: [https://lnkd.in/erj963Zw](https://lnkd.in/erj963Zw)  
ðŸ“Š LIBERO dataset on Hugging Face: [https://lnkd.in/eYrTEcK3](https://lnkd.in/eYrTEcK3)  
ðŸ“š LeRobot libero docs: [https://lnkd.in/ePJzS4FE](https://lnkd.in/ePJzS4FE)  
InternScenes:  
Learn more and access the paper: [https://lnkd.in/eqqccFXM](https://lnkd.in/eqqccFXM)  
Explore the dataset: [https://lnkd.in/e7y9zTxw](https://lnkd.in/e7y9zTxw)  
Project page: [https://lnkd.in/e_F_uYA5](https://lnkd.in/e_F_uYA5)  
Code on GitHub: [https://lnkd.in/efyxa6z6](https://lnkd.in/efyxa6z6)  
BEHAVIOR: [https://behavior.stanford.edu/](https://behavior.stanford.edu/)  
FineVision: [https://huggingface.co/spaces/HuggingFaceM4/FineVision](https://huggingface.co/spaces/HuggingFaceM4/FineVision)  
EgoDex: Preprint: arxiv.org/abs/2505.11709  
Dex1Bâ€”a framework that produces 1 BILLION diverse dexterous hand demonstrations for both grasping ðŸ–ï¸ and articulation ðŸ’» tasks. We train generative models on this data for diverse and robust dexterous manipulation.  
Website: [https://lnkd.in/gt6_F6-f](https://lnkd.in/gt6_F6-f)  
Paper: [https://lnkd.in/gVUVqxPA](https://lnkd.in/gVUVqxPA)  
RoboEval:Website: [https://lnkd.in/gpb5GRQX](https://lnkd.in/gpb5GRQX) | Paper: [https://lnkd.in/gjsvzsdC](https://lnkd.in/gjsvzsdC)  
Robospatial: [https://huggingface.co/datasets/chanhee-luke/RoboSpatial-Home](https://huggingface.co/datasets/chanhee-luke/RoboSpatial-Home)  
OpenX Embodiment: [https://robotics-transformer-x.github.io/](https://robotics-transformer-x.github.io/)  
BridgeData V2: [https://rail-berkeley.github.io/bridgedata/](https://rail-berkeley.github.io/bridgedata/)  
HomeRobot: Open Vocabulary mobile manipulation: [https://arxiv.org/abs/2306.11565](https://arxiv.org/abs/2306.11565)  
[https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-paligemma2-on-detection-dataset.ipynb](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-paligemma2-on-detection-dataset.ipynb)  
AgiBot World: [https://x.com/OpenDriveLab/status/1873621488315801772](https://x.com/OpenDriveLab/status/1873621488315801772)  
ROS standardization of Sim: [https://lnkd.in/dr6SZnaB](https://lnkd.in/dr6SZnaB).  
Meta PARTNR is a large-scale human and robot collaboration benchmark for planning and reasoning in embodied multi-agent tasks. Work like this informs our work as scientists and engineers pushing this important field of study forward.  
Research paper âž¡ï¸ [https://go.fb.me/8cwppu](https://go.fb.me/8cwppu)  
Dataset and code âž¡ï¸ [https://go.fb.me/rld05r](https://go.fb.me/rld05r)  
HumanoidBench
   

Environments:  
[https://www.reddit.com/r/robotics/comments/z905og/help_choosing_simulators/?rdt=63268](https://www.reddit.com/r/robotics/comments/z905og/help_choosing_simulators/?rdt=63268)  
Drake: [https://drake.mit.edu/](https://drake.mit.edu/) | Pybullet| ProcThor | UniSim  
Meta: Habitat 3.0 [https://aihabitat.org/habitat3/](https://aihabitat.org/habitat3/)  
Isaac Lab: [https://isaac-sim.github.io/IsaacLab/main/index.html](https://isaac-sim.github.io/IsaacLab/main/index.html)  
[https://developer.nvidia.com/blog/fast-track-robot-learning-in-simulation-using-nvidia-isaac-lab/](https://developer.nvidia.com/blog/fast-track-robot-learning-in-simulation-using-nvidia-isaac-lab/)  
[https://www.nvidia.com/en-us/events/corl/https://openreview.net/group?id=robot-learning.org/CoRL/2024/Workshop/XE#tab-accept-poster](https://www.nvidia.com/en-us/events/corl/https://openreview.net/group?id=robot-learning.org/CoRL/2024/Workshop/XE#tab-accept-poster)  
Â 

![Exported image](Exported%20image%2020260222122632-0.png)
 
**Foundational Model Surveys:**  
Real-World Robot Applications of Foundation Models: A Review: [https://arxiv.org/abs/2402.05741](https://arxiv.org/abs/2402.05741)  
Toward General-Purpose Robots via Foundation Models: [https://arxiv.org/abs/2312.08782](https://arxiv.org/abs/2312.08782)  
Foundation Models in Robotics: Applications, Challenges, and the Future : [https://arxiv.org/pdf/2312.07843.pdf](https://arxiv.org/pdf/2312.07843.pdf)  
Deep generative models for robotics survey: [https://arxiv.org/abs/2408.04380](https://arxiv.org/abs/2408.04380)
 
**VLM/VLA Building:**  
**VLA-0:** [https://arxiv.org/abs/2510.13054](https://arxiv.org/abs/2510.13054)  
âœ… Outperforms Ï€â‚€, GR00T-N1, MolmoAct, SmolVLA  
âœ… No new components or action vocabulary  
âœ… #1 on LIBERO among non-pretrained methods  
âœ… Trained from scratch on 100 demos per task, beats pretrained baselines in real-world tests
 
NovaFlow: RAI  
**Gemma Robotics 1.5:** [https://deepmind.google/discover/blog/gemini-robotics-15-brings-ai-agents-into-the-physical-world/](https://deepmind.google/discover/blog/gemini-robotics-15-brings-ai-agents-into-the-physical-world/)  
**RDT-2:** [https://rdt-robotics.github.io/rdt2/](https://rdt-robotics.github.io/rdt2/)  
MolmoAct: [https://arxiv.org/abs/2508.07917](https://arxiv.org/abs/2508.07917)  
07/09: LBMS: [https://toyotaresearchinstitute.github.io/lbm1/](https://toyotaresearchinstitute.github.io/lbm1/)  
Hume, SHJT - VLA  
Villa-X: [https://microsoft.github.io/villa-x/](https://microsoft.github.io/villa-x/)  
Gemma 2.5: [https://developers.googleblog.com/en/gemini-25-for-robotics-and-embodied-intelligence/](https://developers.googleblog.com/en/gemini-25-for-robotics-and-embodied-intelligence/)?  
IJRR paper "Multimodal Spatial Language Maps for Robot Navigation and Manipulation" Project: [https://mslmaps.github.io](https://mslmaps.github.io)  
Paper: [https://lnkd.in/gU48dBXQ](https://lnkd.in/gU48dBXQ)  
SpatialVLA  
SAM2Act: [https://sam2act.github.io/](https://sam2act.github.io/) (successor to RVT2)  
Gr00t- N1  
- Whitepaper: [https://lnkd.in/gfqKN6im](https://lnkd.in/gfqKN6im)  
- Open-source code repo: [https://lnkd.in/gG9ShUwj](https://lnkd.in/gG9ShUwj)  
DexVLA: [https://arxiv.org/abs/2502.05855](https://arxiv.org/abs/2502.05855)  
Pi0 and Pi0fast: [https://huggingface.co/lerobot/pi0](https://huggingface.co/lerobot/pi0) | [https://huggingface.co/blog/pi0](https://huggingface.co/blog/pi0)  
ðŸš€ ðˆð§ð­ð«ð¨ðð®ðœð¢ð§ð  Ï€ðŸŽ ðšð§ð Ï€ðŸŽ-ð…ð€ð’ð“: ð•ð¢ð¬ð¢ð¨ð§-ð‹ðšð§ð ð®ðšð ðž-ð€ðœð­ð¢ð¨ð§ ðŒð¨ððžð¥ð¬ ðŸð¨ð« ð†ðžð§ðžð«ðšð¥ ð‘ð¨ð›ð¨ð­ ð‚ð¨ð§ð­ð«ð¨ð¥ ðŸ¤–  
Fine-tuning: [https://github.com/Ke-Wang1017/lerobot/tree/pi_zero_finetune](https://github.com/Ke-Wang1017/lerobot/tree/pi_zero_finetune)  
HAMSTER: Paper: [https://arxiv.org/abs/2502.05485](https://arxiv.org/abs/2502.05485)  
MAGMA -8B: [https://github.com/microsoft/Magma](https://github.com/microsoft/Magma)  
Robot Utility Models (RUMs): [https://robotutilitymodels.com/](https://robotutilitymodels.com/)  
CogAct, VLA with a special action module: [https://cogact.github.io/](https://cogact.github.io/)  
Body Transformer (sensors, actuators as graph): [https://arxiv.org/abs/2408.06316](https://arxiv.org/abs/2408.06316)  
VLMap (map building) [https://arxiv.org/pdf/2210.05714](https://arxiv.org/pdf/2210.05714)  
Theia, model distillation: [https://arxiv.org/abs/2407.20179](https://arxiv.org/abs/2407.20179)  
A3VLM (Action Affordance VLM): [https://github.com/changhaonan/A3VLM](https://github.com/changhaonan/A3VLM)  
Mobility VLA (navigation from demonstration videos): [https://arxiv.org/abs/2407.07775v1](https://arxiv.org/abs/2407.07775v1)  
**SARA-RT:** [https://arxiv.org/abs/2312.01990](https://arxiv.org/abs/2312.01990)  
Octo: [https://octo-models.github.io/](https://octo-models.github.io/)  
Learning Humanoid Locomotion Over Challenging Terrain: [https://humanoid-challenging-terrain.github.io/](https://humanoid-challenging-terrain.github.io/)  
Humanoid Locomotion as Next Token Prediction: [https://humanoid-next-token-prediction.github.io/](https://humanoid-next-token-prediction.github.io/)
 
**VLM Application Papers + Agent Systems:**  
An Empirical Study of Autoregressive Pre-training from Videos (didnt read but looks amazing)  
[https://arxiv.org/pdf/2501.05453](https://arxiv.org/pdf/2501.05453)  
Our results demonstrate that, despite minimal inductive biases, autoregressive pre-training  
leads to competitive performance across all benchmarks. Finally, we find that scaling our video models  
results in similar scaling curves to those seen in language models, albeit with a different rate
 
Robots that Ask for Help: [https://robot-help.github.io/](https://robot-help.github.io/)  
AutoRT (scaling up systems): [https://auto-rt.github.io/](https://auto-rt.github.io/)  
ROSA - Robot Operating System Agent: [https://arxiv.org/abs/2410.06472](https://arxiv.org/abs/2410.06472)  
Robots that use tools from LLMs: [https://creative-robotool.github.io/](https://creative-robotool.github.io/)  
MOSAIC (Cooking): [https://portal-cornell.github.io/MOSAIC/](https://portal-cornell.github.io/MOSAIC/)  
DynaMem (Open World Manipulation): [https://arxiv.org/pdf/2411.04999](https://arxiv.org/pdf/2411.04999)  
Prime 1: [https://www.businesswire.com/news/home/20250107559141/en/Ambi-Robotics-Deploys-PRIME-1-The-First-Vertically-Integrated-AI-Foundation-Model-for-Warehouse-Robots](https://www.businesswire.com/news/home/20250107559141/en/Ambi-Robotics-Deploys-PRIME-1-The-First-Vertically-Integrated-AI-Foundation-Model-for-Warehouse-Robots)
   

**Image Goals for Planning:**  
**SEE: Calvin Benchmark**  
SUSIE -- \> SOAR (Language Conditioned Policy): [https://auto-improvement.github.io/](https://auto-improvement.github.io/)  
Embodied Chain of Thought (outperforms OpenVLA) : [https://embodied-cot.github.io/](https://embodied-cot.github.io/)  
Steering Your Generalists (value guided policy steering): [https://nakamotoo.github.io/V-GPS/index.html](https://nakamotoo.github.io/V-GPS/index.html)  
GHIL-GLUE (glue together language prediction): [https://ghil-glue.github.io/](https://ghil-glue.github.io/)  
ManipGen (Sim2Real, outperforms OpenVLA) : [https://mihdalal.github.io/manipgen/](https://mihdalal.github.io/manipgen/) (code coming soon)  
3D VLA (U-Mass Paper, goal conditioning) [https://arxiv.org/pdf/2403.09631.pdf](https://arxiv.org/pdf/2403.09631.pdf)  
Vid2Action (Generate video states as goals for exploration): [https://video-to-action.github.io/](https://video-to-action.github.io/) (learn from youtube)
 
**Generate your own interaction Sim:**  
IRA-Sim (POC show how to simulate trajectory rollouts): [https://gen-irasim.github.io/](https://gen-irasim.github.io/)  
**UniSim Learning Interactive Real-World Simulators:** **https://universal-simulator.github.io/unisim/**
 
**World Modelers:**  
DINO-WB: [https://dino-wm.github.io/](https://dino-wm.github.io/)  
GENIE2: [https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/](https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/)  
SIMA - Scalable Instruction Multiworld Agent: [https://deepmind.google/discover/blog/sima-generalist-ai-agent-for-3d-virtual-environments/](https://deepmind.google/discover/blog/sima-generalist-ai-agent-for-3d-virtual-environments/)  
Oasis - (Decart & Etched): [https://www.etched.com/blog-posts/oasis](https://www.etched.com/blog-posts/oasis)  
PWM (RL downstream from World Models): [https://www.imgeorgiev.com/pwm/](https://www.imgeorgiev.com/pwm/)  
Dreamer papers (1-3, DayDreamer)  
Diffusion World Model: [https://arxiv.org/pdf/2402.03570.pdf](https://arxiv.org/pdf/2402.03570.pdf)
 
Video generators:

![Exported image](Exported%20image%2020260222122634-1.png)  

**Goals from human demo:**  
**Act1:** [https://www.sunday.ai/journal/no-robot-data](https://www.sunday.ai/journal/no-robot-data) **(use a glove)**  
Omni-retarget: [https://arxiv.org/abs/2509.26633](https://arxiv.org/abs/2509.26633)  
Anytracker: [https://arxiv.org/html/2509.13833v1](https://arxiv.org/html/2509.13833v1)  
âœ… AnyTracker: a unified motion tracker trained to follow a wide range of motions  
âœ… AnyAdapter: history-informed adaptation to handle terrain, forces, and property changes  
âœ… Zero-shot sim2real: deployed directly on Unitree G1 hardware without fine-tuning  
ðŸ“Paper: [https://lnkd.in/dgtdtwtt](https://lnkd.in/dgtdtwtt)  
Project: [https://lnkd.in/dkUSK-XC](https://lnkd.in/dkUSK-XC)  
EMMA: ðŸ“Paper: [https://lnkd.in/eBvn-3P6](https://lnkd.in/eBvn-3P6)  
Project: [https://ego-moma.github.io](https://ego-moma.github.io)  
(ImMimic: Cross-Domain Imitation from Human Videos via Mapping and Interpolation  
[https://arxiv.org/pdf/2509.10952](https://arxiv.org/pdf/2509.10952)  
Masquerade:  
ðŸ“Paper: arxiv.org/pdf/2508.09976  
Project: masquerade-robot.github.io  
Intermimic: [https://github.com/Sirui-Xu/InterMimic](https://github.com/Sirui-Xu/InterMimic)  
**EgoVLA:**  
Website: [https://lnkd.in/g-CPWWsu](https://lnkd.in/g-CPWWsu)  
Papers: [https://lnkd.in/gJ_aF7tn](https://lnkd.in/gJ_aF7tn)
 
ðŸ“£ Excited to share our latest work: Phantom ðŸ‘» : Training Robots Without Robots Using Only Human Videos!  
(Project page: [https://lnkd.in/gdbysfG7](https://lnkd.in/gdbysfG7))  
Introducing our latest work, Humanoid Policy ~ Human Policy ([https://lnkd.in/gYmHTAp6](https://lnkd.in/gYmHTAp6)). We advocate human data as a scalable data source for co-training egocentric manipulation policy.  
VideoMimic | BodyTransformer | MuJoCo Playground (pip install playground)  
WHIRL (2022)  
ASAP: [https://agile.human2humanoid.com/](https://agile.human2humanoid.com/)  
ASAP (real-sim-real): [https://lnkd.in/gDASEDEr](https://lnkd.in/gDASEDEr)  
We built DexGen foundation controller, it takes coarse motion prompts to refine them to fine, safe actions. DexGen+teleop now enables tasks like screwdriver use & many more  
Paper: [https://lnkd.in/dU-Uk_QM](https://lnkd.in/dU-Uk_QM)  
Webpage: [https://lnkd.in/dViTJdfs](https://lnkd.in/dViTJdfs)
 
R+X (RGB + Sawyer, HaMeR hand decomp): [https://www.robot-learning.uk/r-plus-x](https://www.robot-learning.uk/r-plus-x)  
Reward Learning from Video (Youtube to Ant, Humanoid, ANYmal) : [https://arxiv.org/pdf/2410.09286](https://arxiv.org/pdf/2410.09286)  
VideoAsAgent: (self-improving generated video plans, self-conditioning consistency): [https://video-as-agent.github.io/](https://video-as-agent.github.io/)  
Vid2Robot (human video transfer): [https://vid2robot.github.io/](https://vid2robot.github.io/)  
HUDOR (reward construction from human video --\> allegro hand): [https://object-rewards.github.io/](https://object-rewards.github.io/)  
SlowMo: synthesize a physically plausible reconstructed key-point trajectory from monocular videos): [https://slomo-www.github.io/website/](https://slomo-www.github.io/website/)  
LAPA Latent Action Pre-Training (direct to VLA): [https://latentactionpretraining.github.io/](https://latentactionpretraining.github.io/)
 
Human2Humanoid (sim2data + simulator trainer, AMASS dataset) [https://human2humanoid.com/](https://human2humanoid.com/)  
Dex Pilot(human hand, vision based teleoperation) [https://sites.google.com/view/dex-pilot](https://sites.google.com/view/dex-pilot)
 
KALIE (select annotations from humans, shape, texture, pose) [https://kalie-vlm.github.io/](https://kalie-vlm.github.io/)  
Hand-Object Interaction Pretraining from Videos (RL fine tune trajectories after): [https://hgaurav2k.github.io/hop/](https://hgaurav2k.github.io/hop/)  
ManiWav (+ Audio to approximate contact information): [https://mani-wav.github.io/](https://mani-wav.github.io/)
 
**Transfer Robot to Robot:**  
VINN [https://jyopari.github.io/VINN/](https://jyopari.github.io/VINN/) (Decouple representation and behavior learning)  
We still do the contrastive/self-supervised learning as the first step of our learning algorithms, and then we use those representations for the downstream architecture.  
CrossFormer (900k trajectories): [https://crossformer-model.github.io/](https://crossformer-model.github.io/)  
Extreme Cross Embodiment (Manip and Nav scaling): [https://extreme-cross-embodiment.github.io/](https://extreme-cross-embodiment.github.io/)  
Unsupervised Zero Shot RL via Reward Encodings (Offline to zero-shot): [https://arxiv.org/abs/2402.17135](https://arxiv.org/abs/2402.17135)  
PIDformer (state spaced transformer): [https://arxiv.org/abs/2402.15989](https://arxiv.org/abs/2402.15989)
   

**Sim2Real:**  
Newton: [https://lnkd.in/gpfHSuWM](https://lnkd.in/gpfHSuWM)  
Mujoco Warp  
Nvidia:  
3DGRT Project Page: [https://lnkd.in/e9ghKe9h](https://lnkd.in/e9ghKe9h)  
3DGUT Project Page: [https://lnkd.in/egfS-Ac5](https://lnkd.in/egfS-Ac5)  
GENESIS  
Blender MCP is here, allowing Claude to talk directly to Blender. For example, provide a 2D reference image and ask Claude to create it in 3D for you. Check it out on GitHub: [https://lnkd.in/gjhreHsf](https://lnkd.in/gjhreHsf)  
[https://github.com/Genesis-Embodied-AI/Genesis](https://github.com/Genesis-Embodied-AI/Genesis) | [https://genesis-embodied-ai.github.io/](https://genesis-embodied-ai.github.io/)  
Insertion Sim 2 Real: [https://residual-assembly.github.io/](https://residual-assembly.github.io/)  
Nvidia blog: [https://blogs.nvidia.com/blog/robot-learning-humanoid-development/](https://blogs.nvidia.com/blog/robot-learning-humanoid-development/)  
SIMPLER (manipulation, train on Real, test in Sim): [https://simpler-env.github.io/](https://simpler-env.github.io/)  
LucidSim (Parkour goal generation, MuJoCo): [https://lucidsim.github.io/](https://lucidsim.github.io/)  
Get-A-Grip (sim2real transfer, Isaac Gym) [https://sites.google.com/view/get-a-grip-dataset](https://sites.google.com/view/get-a-grip-dataset)  
Uni-graspformer (Train individual RL grasp into a central one): [https://dexhand.github.io/UniGraspTransformer/](https://dexhand.github.io/UniGraspTransformer/)  
(Isaac Gym, base is [https://pku-epic.github.io/UniDexGrasp++/](https://pku-epic.github.io/UniDexGrasp++/))
 
RialTo Real to Sim to Real ( RL w/ digital twin, Isaac Sim): [https://real-to-sim-to-real.github.io/RialTo/](https://real-to-sim-to-real.github.io/RialTo/)  
MimicGen (data generation for LfD - scaling up demos, Isaac Sim): [https://mimicgen.github.io/](https://mimicgen.github.io/) (Isaac GenFactory)  
DexMimicGen (data generation for dexerous manipulation, Nvidia): [https://dexmimicgen.github.io/](https://dexmimicgen.github.io/)  
RVT-2 (3D manipulation, with tasking, Nvidia) : [https://robotic-view-transformer-2.github.io/](https://robotic-view-transformer-2.github.io/)  
Lessons learned on spinning pens (Nvidia, Eureka, pre-train policy in sim): [https://arxiv.org/abs/2407.18902](https://arxiv.org/abs/2407.18902)  
Table tennis - Hierarchical (Sim2Real, library of low/high controllers, MuJoCo): [https://sites.google.com/view/competitive-robot-table-tennis/home](https://sites.google.com/view/competitive-robot-table-tennis/home)  
Scaling up, Distilling down (scale up data, Diffusion policy on language condition, MuJoCo) : [https://www.cs.columbia.edu/~huy/scalingup/](https://www.cs.columbia.edu/~huy/scalingup/)  
MuJoCo is amazing for rigid body simulation, but in my experience from [FlingBot](https://flingbot.cs.columbia.edu/), NVidia's simulators for cloth simulation is still the way to go.

\> From \<[https://github.com/real-stanford/scalingup/blob/master/docs/extend.md#environments--tasks-](https://github.com/real-stanford/scalingup/blob/master/docs/extend.md#environments--tasks-)\>  

Gaussian Splatting for Robots (Position based dynamics): [https://embodied-gaussians.github.io/](https://embodied-gaussians.github.io/)  
VISTA: View Synthesis Augmentation (MuJoCo, robosuite/mimicgen frameworks) : [https://arxiv.org/abs/2409.03685](https://arxiv.org/abs/2409.03685)  
Autodesk - dynamic compliance Insertion: [http://arxiv.org/pdf/2311.07499](http://arxiv.org/pdf/2311.07499)
   

**Planners:**  
[https://wildlma.github.io/](https://wildlma.github.io/) (+ VR Teleoperation)  
Embodied AI with Two Arms: Zero-shot Learning, Safety and Modularity: [https://sites.google.com/view/safe-robots](https://sites.google.com/view/safe-robots)  
BUMBLE: Unifying Reasoning and Acting with Vision-Language Models for Building-Wide Mobile Manipulation [https://robin-lab.cs.utexas.edu/BUMBLE/](https://robin-lab.cs.utexas.edu/BUMBLE/)  
VLM-Tamp: [https://zt-yang.github.io/vlm-tamp-robot/](https://zt-yang.github.io/vlm-tamp-robot/) (Dieter Fox)  
Text2Interaction: [https://sites.google.com/view/text2interaction](https://sites.google.com/view/text2interaction)  
Using gpt4 to create a behavior tree [https://arxiv.org/pdf/2408.08282](https://arxiv.org/pdf/2408.08282)  
Video Language Planning: [https://arxiv.org/pdf/2310.10625](https://arxiv.org/pdf/2310.10625)  
LLMs to Actions (Language to Latent Codes before downstream): [https://arxiv.org/abs/2405.04798](https://arxiv.org/abs/2405.04798)  
[https://fredshentu.github.io/LCB_site/](https://fredshentu.github.io/LCB_site/)  
NVEagle (Points2Plans) : [https://sites.google.com/stanford.edu/points2plans](https://sites.google.com/stanford.edu/points2plans)  
RobotVQA (Long Horizon reasoning): [https://arxiv.org/pdf/2311.00899.pdf](https://arxiv.org/pdf/2311.00899.pdf)
 
**Shadowing:**  
Dream Control: [https://genrobo.github.io/DreamControl/](https://genrobo.github.io/DreamControl/)  
Ego mimic (treats human demo as embodied via ARIA glasses): [https://egomimic.github.io/](https://egomimic.github.io/)  
Shadow (shadows human motion): [https://shadow-cross-embodiment.github.io/](https://shadow-cross-embodiment.github.io/)  
**Humanoid shadowing:** [https://humanoid-ai.github.io/](https://humanoid-ai.github.io/)  
Hover: [https://arxiv.org/pdf/2410.21229](https://arxiv.org/pdf/2410.21229)  
HOVER: Versatile Neural Whole-Body Controller for Humanoid Robots
 
**Navigation:**  
EMMA: Embodied Multi-Modal Agent - Visual Nav: [https://arxiv.org/abs/2311.16714](https://arxiv.org/abs/2311.16714)  
[https://github.com/stevenyangyj/Emma-Alfworld](https://github.com/stevenyangyj/Emma-Alfworld)  
Poliformer: [https://poliformer.allen.ai/](https://poliformer.allen.ai/) ; [https://procthor.allenai.org/](https://procthor.allenai.org/)  
LeLaN: Learning A language Conditioned Navigation Policy from in-the-wild videos: [https://learning-language-navigation.github.io/](https://learning-language-navigation.github.io/) (learn from youtube)