[https://medium.com/@bp_64302/this-business-of-robotics-foundation-models-cb4bdede1444](https://medium.com/@bp_64302/this-business-of-robotics-foundation-models-cb4bdede1444)
 
Investor interest in robotics has never been higher and for good reason. AI is not only going to change the nature of cognitive work, but it will change the nature of physical work as well. We’ve seen well over $1B invested in this space just this year.  
At the same time, we’re seeing increasingly concentrated bets in particular companies. Our company, Cobot, has clearly benefitted from this, having raised $105M Series B earlier this year led by General Catalyst, with Sequoia, Khosla, Mayo Clinic, Lux Capital, and Jeff Bezos all participating.  
Earlier this year at the peak of the excitement around humanoid robots, I wrote a series of articles to try to explain the challenges I saw and how we approach them going forward. If you’re curious, check them out here:  
[The Problems With Humanoid Robots](https://medium.com/@bp_64302/the-problems-with-humanoid-robots-9d8684d62008)  
[The Path to Human-Capable Robots](https://medium.com/@bp_64302/the-path-to-human-capable-robots-dec4965f41f0)  
[The Path to Great AI for Human-Capable Robots](https://medium.com/@bp_64302/the-path-to-great-ai-for-human-capable-robots-a7e12033289c)  
Similar to the humanoid space earlier this year, we’re also seeing an investment surge in companies building foundation models for robotics. [SKILD raised $300M earlier this year](https://finance.yahoo.com/news/skild-ai-raises-300m-series-184400332.html) with 16 people. [Fei Fei Li’s World Labs raised $230M](https://techcrunch.com/2024/09/13/fei-fei-lis-world-labs-comes-out-of-stealth-with-230m-in-funding/). [Physical Intelligence raised $70M seed earlier this year](https://www.maginative.com/article/physical-intelligence-raises-70m-to-build-ai-powered-robots-for-any-application/) and is rumored to be raising another large round.  
When it comes to AI for robotics, however, the challenge though for all of us — founders, investors, candidates, students, the public — is trying to understand the space and where we’re really at.  
When you first start training a new model on a novel task, the early phases of performance gains come really really fast.  
Is this all just the classic [phase 1 of Gartner’s Hype Cycle](https://www.gartner.com/en/documents/3887767) where a new innovation sparks investor excitement, but leads eventually to a valley of disillusionment? Or are we seeing rapid breakthroughs that are going to quickly transform the physical world?  
A few weeks ago, Michael Vogelsong, our Head of Foundation Models AI for Cobot, penned a thoughtful survey piece titled “[Unlocking the Future of Robotic Intelligence](https://medium.com/@mjvogelsong/unlocking-the-future-of-robotic-intelligence-991e151bffe9)” outlining the state of the research in this space. It both broad and deep in the techniques different researchers across academia, startups and big tech are attempting and well worth a read.  
In this article, I want to go a little deeper though in trying to explain why we’re seeing the excitement we’re seeing, where I think we’re at in the maturity curve of this technology, what the bottlenecks are likely to be, the reasons to be optimistic, and ultimately what I think the smart bet in this space is.  
**Why the Excitement?**  
I posit there are two reasons. The first is the we’ve finally started to see artificial intelligence that appears to generalize in the form of large-language models. With OpenAI rumored to be raising money at $150B valuation, if you can build a new foundation model that generalizes in a new domain, maybe you’ll be worth $150B too. The idea that a $1.5B company might be able to raise a fast follow-on at $15B if they have anything that resembles a working foundation model for robotics is more than enough return to get VCs excited, even if it doesn’t hit $150B.  
The second reason is we’re starting to see videos of robotics AI that are much more impressive than they were in the past. We’re seeing novel tasks like folding clothes or washing dishes. We’re seeing 20 different tasks being conducted simultaneously from companies like 1X.  
It’s now 2024 and we have poured $160B into self-driving cars. We now have self-driving cars available for public rides from one company working in 2 cities. DARPA put $2M in and managed to convince investors to follow with another $160B.  
If you interpolate from very limited progress over the past 15 years to what appears to be rapid progress and then overlay that with the LLM progress curve and corresponding growth in valuation, the idea of investing $300M in at $1.5B doesn’t seem so crazy.  
Here’s the problem. In robotics, I don’t believe we’re anywhere near the ChatGPT 3.0 to 3.5 to 4.0 to 4o to 4o1 part of the curve. Not even close.  
**What’s Really Going On?**  
**Fast Progress in the Early Part of the Learning Curve**  
When you first start training a new model on a novel task, the early phases of performance gains come really really fast. Hitting 10% to 40% to 60% to 80% can come in just months or even weeks. This progress feels really fast.  
We saw this in autonomous vehicles. The first DARPA Grand Challenge didn’t go that well. The second one went much much better and a few cars completed the circuit. The first DARPA Grand Challenge was 2004. Stanford won in 2005!  
That progress felt amazing. In just 2 years teams went from nothing to completing a full course. It’s now 2024 and we have poured $160B into self-driving cars. We now have self-driving cars available for public rides from one company working in 2 cities. DARPA put $2M in and managed to convince investors to follow with another $160B.  
The transformer architecture is finally being applied to robotics… with the transformer architecture and smart ways to tokenize images and motion, we can show the same results in 20–40 demonstrations.  
Amazon did the same thing with the [Amazon Picking Challenge](https://www.herox.com/blog/348-history-of-challenges-amazon-picking-robot), motivating teams of researchers to compete in top-down bin picking with a 6DOF arm in 2015. The “picking rates” in the early competitions quickly got to the 60–80% benchmark and somewhere on the order of $1B went into startups coming out of that challenge. It took a number of years to get into the low-90s with the end result being Amazon just acqui-hired Covariant, one of the best funded of the bunch having raised $222M. The details aren’t public, but the speculation is investors maybe barely got their money back, or maybe got pennies on the dollar?  
The challenge is the promising early results can inspire a lot of investment dollars to flow, without a true comprehension of just how long it will take to achieve functional real-world performance. This is why Gartner calls the first phase of a hype cycle the “Innovation Trigger”.  
**Attention Is All You Need**  
The other thing that’s happening is the transformer architecture is finally being applied to robotics. This is really critical because transformers really do work better and they work better with less initial data.  
Imitation Learning is a technique roboticists have been using for 10+ years, perhaps longer. But with convolutional neural nets and other techniques, the amount of data you needed was massive. At Amazon, we ran an experiment to use human judgment to recreate OpenAI’s still incredibly impressive “[Learning Dexterity](https://openai.com/index/learning-dexterity/)” paper. We got it to work, but it took on the order of 40,000 human judgments. Simpler tasks still took as many as 400 demonstrations to learn. No summer intern or grad student was really that motivated to do 400 demonstrations for one result.  
Now, with the transformer architecture and smart ways to tokenize images and motion, we can show the same results in 20–40 demonstrations. This is impressive and has fueled lots of excitement about “end to end policy” work, allowing us to show lots of novel tasks as engineers, interns and grad students puppet-master their robots to do new things.  
**When You’re Starving In the Desert, Any Water Will Do**  
Data is the life-blood of a great model. The debate about data quality vs. dataset size was a hot topic in AI in 2022 when I was CTO for Scale AI. [Andrew Ng coherently argued for a focus on data quality](https://spectrum.ieee.org/andrew-ng-data-centric-ai) over data quantity to drive the best results.  
In practice, what we’ve learned since is that quantity and quality matter. Bad data messes up your long-tail performance. Not enough data means you can’t interpolate in the long-tail. But almost no amount of quantity of data swamps those bad samples.  
But you really do have to put into context… is the performance jump we’re seeing because of some fundamentally new phenomenon such as a derived world model, or is the performance jump we’re seeing because we just lacked enough data to get any performance out of these models?  
But the situation is very different early on in model development for a new domain. Early on, the challenge is you’re data starved to build any model at all. But the nice thing is at this stage model performance is low and almost any data will help.  
Just this past week, I saw Sergey Levine speak and walk through some of the results from their continued work on cross-embodiment. Sergey Levine is a world-renown robotics professor and researcher at UC Berkeley and his work has consistently pushed forward the frontier of robotics AI. When he speaks, we all listen.  
Cross-embodiment is taking data from different robots and different morphologies, adding it to a base model, typically in a pre-training phase, and then fine-tuning to a task. This technique has [shown impressive jumps in model performance](https://arxiv.org/abs/2402.19432).  
The results were presented as surprising. But are they really? Again, in the early phase of model performance, big jumps are to be expected. Its when you get to higher and higher accuracy that it starts to bog down. And if you’re data starved, almost any data will help.  
He shared that they weren’t seeing the same performance improvement in navigation and planning, again suggesting some surprise. But I’m not sure that’s surprising either. We’re much farther down the path to high-performance models in navigation and planning.  
In self-driving, everyone tried to break the data bottleneck without having to put huge fleets of cars on the road. They explored domain randomization to take what data they had and make the models more robust by manipulating that data. They explored synthetic data from simulation.  
Cross-embodiment is an important idea though. Clearly when we scale up LLMs, something surprising did happen as the compression of information seems to have led to an implicit textual world model that can then be used to generate novel explanations.  
Andrej Karpathy, who previously led Tesla’s work in Full Self Driving, [spoke recently on the No Priors podcast](https://www.youtube.com/watch?v=hM_h0UA7upI) about how the Tesla FSD models were able to transfer to Tesla Optimus with surprisingly little refinement. In the Tesla FSD-\>Optimus case, I think there’s a good argument there’s enough data to have a reasonable an ego-centric world model for navigation and planning.  
But for other tasks where we’re in early model development and heavily data starved, I suspect we’re simply in the early stages of model building where almost any data helps and not yet seeing a world model or generalization occurring.  
This isn’t to say anyone is misrepresenting results or that cross-embodiment isn’t important. The papers in this space are credible and transparent. But you really do have to put into context… is the performance jump we’re seeing because of some fundamentally new phenomenon such as a derived world model, or is the performance jump we’re seeing because we just lacked enough data to get any performance out of these models? And ultimately, is it better to invest in diverse in-domain data, or to try to gather lots of cross-embodiment data? The latter is more expensive, but the former may deliver results much faster.  
This is the $100M question I hope investors are asking.  
**Can We Predict Where this is Going?**  
Well, yes and no.  
**Reasons to Expect this to Take a Long Time**  
I think we can predict that there’s going to be a massive ramp up in exploring ways to collect data. More data is required to make better models.  
The learning from self-driving though has been that data quality REALLY REALLY matters. In self-driving, everyone tried to break the data bottleneck without having to put huge fleets of cars on the road. They explored domain randomization to take what data they had and make the models more robust by manipulating that data. They explored synthetic data from simulation.  
We’re getting smarter at all of these every day, so there’s good reason in general to be optimistic that we’ll find ways to speed up the data collection, align different data sources, and improve our models more quickly than in the past.  
I know some members of the early Full Self-Driving team at Telsa who worked under Andrej Karpathy. Andrej was famous for reviewing data labels himself by hand and correcting or rejecting bad labels. This manually intensive effort paid off in improved model performance.  
This article does a really good job of describing [the pipeline Telsa had to build to get highly currated data.](https://saneryee-studio.medium.com/deep-understanding-tesla-fsd-part-4-auto-labeling-simulation-60c9bfd3bcb5)  
I had a front-row seat to all of this as CTO for Scale AI. Scale did the data labeling for most of the self-driving car companies. It was laborious, tedious work. But quality really really mattered to model performance.  
So to summarize this data cycle:

1. Initially there’s very little data and models do badly.
2. Almost any data you throw into the model, simulated, domain-randomized, cross-embodiment helps it do better.
3. You start to tediously eliminate all of that junk data and replace it with high-quality real-world data.

If this follows anything like the self-driving efforts and timelines, we’re years from models that really perform.  
**Reasons To Be More Optimistic**  
The first reason to be optimistic is that attention really is all you need to get a lot more from your data more quickly. The transformer architecture does fundamentally work better. Diffusion models also help us to climb the learning curve more gracefully. Our tools are better now than they were a few years ago.  
The next reason is that people are actively trying to explore self-learning loops. The fear is that self-learning loops lead to model collapse as they resolve to local minima. This is a real concern. OpenAI claims to be learning off its own generated data without model collapse. That’s great, but maybe it requires first having a really really stellar model trained from 13 trillion tokens with which to refine? But maybe it doesn’t. Maybe robots can self-play and self-learn new tasks.  
If the customers of robotics foundation models are robotics companies, but those robotics companies don’t have many robots out there, then who buys these models?  
Another compelling idea is to try to take other sources of cross-domain data, say data from videos, and use AI models to align that data with task-specific data, thus automatically improving data quality to better match the model.  
We’re getting smarter at all of these every day, so there’s good reason in general to be optimistic that we’ll find ways to speed up the data collection, align different data sources, and improve our models more quickly than in the past.  
**Ok, But Is there a Business Here?**  
This is obviously the $150B question. Is there another OpenAI to be built in this space?  
With Chat-based interfaces, we’re still learning how the business models will play out. But the interface is compelling and easy to use and generating revenue.  
The challenge with robotics foundation models are that you need robots that can take advantage of these foundation models. Most of the RFM companies are not building hardware. Robots like humanoids are going to require the AI to work before they can scale up. Quadrupeds can probably show improvement with AI most quickly given the capablity of their actuators and the availability of quality hardware, but what are we going to have them do? How much business value is there in scrambling up rocks or patrolling a parking lot?  
We think the right strategy is to build vertically integrated hardware that can start earning a paycheck while simultaneously starting to collect high-quality real-world data.  
If the customers of robotics foundation models are robotics companies, but those robotics companies don’t have many robots out there, then who buys these models?  
Also, we know DeepMind, OpenAI, Amazon and others are also starting down the process of building RFMs. So it may be that there’s no money to be made at all in pure foundation models work if one of those companies sees it as valuable to make it available at low or no cost.  
**Where’s is the Opportunity?**  
At Cobot, we’re building a robot that doesn’t require a robotics foundation model, but will be able to take on more tasks in the world once we have one. We think we can deploy a million robots or more well before we need a robotic foundation model. There’s meaningful work in the world where we can collaborate with humans to move boxes, totes and carts around without requiring robotics AI that generalizes.  
We think the right strategy is to build vertically integrated hardware that can start earning a paycheck while simultaneously starting to collect high-quality real-world data. At the same time, build a very strong team of RFM scientists and engineers and just start to make progress, while carefully evaluating which techniques are working and assessing the best path to deploying a robotics foundation model in real customer environments.  
Our mission is to win the race to deployed robotics intelligence at scale. We’re well on our way
 \> From \<[https://medium.com/@bp_64302/this-business-of-robotics-foundation-models-cb4bdede1444](https://medium.com/@bp_64302/this-business-of-robotics-foundation-models-cb4bdede1444)\>