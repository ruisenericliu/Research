Dear friends,  
   
Planning is a key [agentic AI design pattern](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VX1WzB6PBm-GN8MDHWwSHfLRW3cF4jk5cLnpYM5Y3kR3qgyTW8wLKSR6lZ3lvW64QPnZ3kWx4fW6hv5ws48dQLRW2DD_Wd7jj_yYN4kKl-0_PPQrW250KXB6k1mtbN71RsHvHxlCBW49WMjp3hBz_fW62Bg6r6By8JYW7qKZm57B1HlLW5W7V6z6brDKNN2DYGTyKmM9QW7HZscP4H50b3W1nZSDX4GxjH0W39QKLh8s2r3yW27q7jT5bv3jpW65dPY74sZWSyW8xCjRR6py_dpW1mm18r3vvx4tW2gHLJP3D5yhxW67vTdZ5d6m8jN7jmhGq4Mlz7W2MXctX7PFsPVW2L875_8jcfRRW5fm3Xg6CZXXFW8MRn6r2L8Nf2W4P7Xnp6Xgy9kW1xkCFq19Q1TvVyFl0P6YhTzcf3xDXTW04) in which we use a large language model (LLM) to autonomously decide on what sequence of steps to execute to accomplish a larger task. For example, if we ask an agent to do online research on a given topic, we might use an LLM to break down the objective into smaller subtasks, such as researching specific subtopics, synthesizing findings, and compiling a report. 
 
Many people had a “ChatGPT moment” shortly after ChatGPT was released, when they played with it and were surprised that it significantly exceeded their expectation of what AI can do. If you have not yet had a similar “AI Agentic moment,” I hope you will soon. I had one several months ago, when I presented a live demo of a research agent I had implemented that had access to various online search tools. 
 
I had tested this agent multiple times privately, during which it consistently used a web search tool to gather information and wrote up a summary. During the live demo, though, the web search API unexpectedly returned with a rate limiting error. I thought my demo was about to fail publicly, and I dreaded what was to come next. To my surprise, the agent pivoted deftly to a Wikipedia search tool — which I had forgotten I’d given it — and completed the task using Wikipedia instead of web search. 
 
This was an AI Agentic moment of surprise for me. I think many people who haven’t experienced such a moment yet will do so in the coming months. It’s a beautiful thing when you see an agent autonomously decide to do things in ways that you had not anticipated, and succeed as a result!  
   
Many tasks can’t be done in a single step or with a single tool invocation, but an agent can decide what steps to take. For example, to simplify an example from the HuggingGPT paper (cited below), if you want an agent to consider a picture of a boy and draw a picture of a girl in the same pose, the task might be decomposed into two distinct steps: (i) detect the pose in the picture of the boy and (ii) render a picture of a girl in the detected pose. An LLM might be fine-tuned or prompted (with few-shot prompting) to specify a plan by outputting a string like _"{tool: pose-detection, input: image.jpg, output: temp1 } {tool: pose-to-image, input: temp1, output: final.jpg}"_. 

![AGENTS PLANNING 2](Exported%20image%2020260222203032-0.png)

This structured output, which specifies two steps to take, then triggers software to invoke a pose detection tool followed by a pose-to-image tool to complete the task. (This example is for illustrative purposes only; HuggingGPT uses a different format.) 
 
Admittedly, many agentic workflows do not need planning. For example, you might have an agent reflect on, and improve, its output a fixed number of times. In this case, the sequence of steps the agent takes is fixed and deterministic. But for complex tasks in which you aren’t able to specify a decomposition of the task into a set of steps ahead of time, Planning allows the agent to decide dynamically what steps to take.   
   
On one hand, Planning is a very powerful capability; on the other, it leads to less predictable results. In my experience, while I can get the agentic design patterns of [Reflection](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VX1WzB6PBm-GN8MDHWwSHfLRW3cF4jk5cLnpYM5Y3kR3qgyTW8wLKSR6lZ3nYN70JkCVS0RFKW6NvsDt8p507gVlRqpx55Jx-WW5mGxdQ5fhRdMW45vwHQ2GmQ0bW6c_xVz4YskKmVS_srw3PJHRhW1fsYqG2J4D1rV8L9z02hmFJZV9dT7n5ldzd1W1Pgg2B2fqHyfW8DHXPc8SfNWwN5tT3KBMSLQpVs7N_14lj8B2W8pf3gw1K5g4bW2krB5j3Z0V8nW7H6VZb6DNbgbW5kMkM_1BBytZW8c9C7W7C0f1VW1gV3dT5GCZV4W8YBRg78bTz_VW4ZLXSl22q5myW2LrBXY61v9X9W9lkZvG7C5HHnW1v-yD27NbNc6W4kzXDJ7sQB9RW67h-LX4NPHvNW4HRgPR2vnkZydlDRBg04) and [Tool use](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VX1WzB6PBm-GN8MDHWwSHfLRW3cF4jk5cLnpYM5Y3kR3qgyTW8wLKSR6lZ3pdW7jYQs-4MDb64W1GC3LG8Pl249N22Z1xgJdHJqW37mhz-6T9G4LW16JHxX4WMXWqN11VhnpHwPjkW6_Nc6S3f4V_nW1blWSR1Zn84cW30DRPf3fqTsGW1Rn4MJ4BjHF5TMQf41xl67BN81Rj6JVYYQPN5d1FYv-Sm4qW1RdxC3250zMfW8pkt5s5fRkPPN40JKq_VYWfmV88_GF73YFKBMXfF_sRFyBqN5P-nM5dh86jW45FHH475tFRLN3XPbG1q0LwSW3kH27z7M1VCGVN-vHp4D48VTW1xb4SN2mm1J9N6XC2pffCRhpW2kX0Ks93gRl5W7cYglg3vmP21W5vmvXT3-fnTYf5CQ2hK04) to work reliably and improve my applications’ performance, Planning is a less mature technology, and I find it hard to predict in advance what it will do. But the field continues to evolve rapidly, and I'm confident that Planning abilities will improve quickly. 
 
If you’re interested in learning more about Planning with LLMs, I recommend:

- “[Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VX1WzB6PBm-GN8MDHWwSHfLRW3cF4jk5cLnpYM5Y3jY3qgyTW6N1vHY6lZ3prW8_HyFf3LPrSLW8RZhyd5BJkCFW8Wx2DY2lcqsyM6dF8yRwDBCVyG7NN3Q6FwMW80YqqK2FvSz1W8rcxM84zcTyWW6kM4f72-kZvYVV9TtN2qj8cqW3yCnZp1lDQcHW1Q6Jjy7nbchZW3jK5fp8zlLMhW5G1_ZK997tTxW3k_7mH6WXQrYW6Kcxmy3wBKPVW9gs6Zp4KW9W_W1Jvfdr5vVCNxW8m5xfT7KPsqCVj08h63bFVNmW5PPBN12DD7H4N6C-5nQdHrwJW7jZqK55jy8BLdSpcCR04),” Wei et al. (2022)
- “[HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VX1WzB6PBm-GN8MDHWwSHfLRW3cF4jk5cLnpYM5Y3jY3qgyTW6N1vHY6lZ3nSW6_Mf8S4qQr8PW3BQFnN3hlJdPW4LrCG57KqRhNW54bDM_5BCjNYW63qCvd1tyv25W2MW2P99bP73wW4fPxq09d7BSFW4BWMpp33MqmfW6921fK493FhRW8hl2RF1hSM7RW1dkMM15wNYY5W7jZvcw36fWmWVP9cY12vzlSyW6W7Gq_2sd2xMVPV_1h7nxmT_W6PXYnB5nmJbDW5s-pSl1DcSMyW4s_tvk2zYsQBVbnhgx1qC5mmVjHbBp2zS4DgW650v2S4Vy1X_W5gN-fX6bry8Sf4vWD7204),” Shen et al. (2023)
- “[Understanding the planning of LLM agents: A survey](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VX1WzB6PBm-GN8MDHWwSHfLRW3cF4jk5cLnpYM5Y3jY3qgyTW6N1vHY6lZ3nQW5X1jcS76mHHYW172z-m2l76HfW1L87wz74k2YKW3dkCvP3lcJ7JW30fhLj91qLrhW7tLWd590xM6QW61jmJL6wZWCYW6-XQN57dqydSW7h6tBy6_wQ59W2Lflms5MHjMqW6QhKNk6qBzJhN676J9Gh84NsV5qtMy5nFL40W1dCCCV2_kfVFW9h0M2j964S4CVgpvTt1dPYhvW938NSv6z69jnW7kbVwD4DymPnW2l5dvg8xXzzbW6jqb0M69FbQxN69FfqRQWMZNW6Vv55X3QRz6Pf8NghbM04),” by Huang et al. (2024)

Keep learning!  
Andrew  
   
P.S. Making sure your RAG system has access to the data it needs to answer questions is an important, but often laborious, step for good performance. Our new short course “Preprocessing Unstructured Data for LLM Applications,” taught by Matt Robinson of Unstructured, teaches you how to build systems that can easily ingest data from a wide range of formats (like text, images, and tables) and from many different sources (like PDF, PowerPoint, and HTML). You’ll learn practical ways to extract and normalize content from diverse formats, enrich your content with metadata to enable more powerful retrieval and reasoning, and use document layout analysis and vision transformers to process embedded images and tables. Putting these components together, you’ll build a RAG bot that draws from multiple document types, demonstrating how high-quality data ingestion and preprocessing affect the quality of RAG output.
 \> From \<[https://mail.google.com/mail/u/0/#inbox/FMfcgzGxSbvNnhQQfNgHKrxlmxxpdqrQ](https://mail.google.com/mail/u/0/#inbox/FMfcgzGxSbvNnhQQfNgHKrxlmxxpdqrQ)\>