ð•ð¢ð¬ð¢ð¨ð§ ð¥ðšð§ð ð®ðšð ðž ð¦ð¨ððžð¥ð¬: ð­ð¡ðž ð ð¢ðšð§ð­ ð¥ðžðšð©ð¬ ð¨ðŸ ð¨ð©ðžð§-ð¬ð¨ð®ð«ðœðž ð¦ð¨ððžð¥ð¬ ðŸ¦˜
 
[Andrew Reed](https://www.linkedin.com/in/ACoAABHNSP4BCf6sKFrMia_cGyhzXSnSBFicC8U) built a cool space that shows that OS LLMs are catching up with closed source LLMs in ELO ranking in the Arena (link below).  
For vision, the same dynamic is happening: the field is still evolving fast, but soon OS models will be able to match GPT-4oâ€™s vision skills.
 
I witnessed the Idefics teamâ€™s work and their many late nights before their publishing of Idefics-2-8b. Now they just published a paper that summarizes their insights!
 
ð™ƒð™šð™§ð™šâ€™ð™¨ ð™– ð™¨ð™ªð™¢ð™¢ð™–ð™§ð™® ð™¤ð™› ð™¬ð™ð™–ð™© ð™©ð™ð™šð™® ð™›ð™¤ð™ªð™£ð™™:
 
âž¤ ð—£ð—²ð—¿ð—³ð—¼ð—¿ð—ºð—®ð—»ð—°ð—² ð—¼ð—³ ð—©ð—Ÿð— ð˜€ ð—¶ð˜€ ð—¹ð—®ð—¿ð—´ð—²ð—¹ð˜† ð—±ð—¿ð—¶ð˜ƒð—²ð—» ð—¯ð˜† ð—½ð—²ð—¿ð—³ ð—¼ð—³ ð˜ð—µð—²ð—¶ð—¿ ð˜ð—²ð˜…ð˜-ð—¼ð—»ð—¹ð˜† ð—¯ð—®ð—°ð—¸ð—¯ð—¼ð—»ð—²ð˜€. In ablation studies, replacing the llama-1-7b with Mistral-7b directly brings +7% performance ðŸ¤¯
 
âž¤ ð—§ð—µð—²ð˜† ð—°ð—¼ð—ºð—½ð—®ð—¿ð—²ð—± ð˜ð˜„ð—¼ ð—°ð—¼ð—ºð—½ð—²ð˜ð—¶ð—»ð—´ ð—®ð—¿ð—°ð—µð—¶ð˜ð—²ð—°ð˜ð˜‚ð—¿ð—²ð˜€:  
- ðŸ”€ ð—–ð—¿ð—¼ð˜€ð˜€ ð—®ð˜ð˜ð—²ð—»ð˜ð—¶ð—¼ð—» ð—®ð—¿ð—°ð—µð—¶ð˜ð—²ð—°ð˜ð˜‚ð—¿ð—²: images are encoded through the vision backbone, and their information is inserted within the text processing at various places  
- ðŸ”¢ ð—™ð˜‚ð—¹ð—¹ð˜† ð—®ð˜‚ð˜ð—¼ð—¿ð—²ð—´ð—¿ð—²ð˜€ð˜€ð—¶ð˜ƒð—² ð—®ð—¿ð—°ð—µð—¶ð˜ð—²ð—°ð˜ð˜‚ð—¿ð—²: the output is directly concatenated to the sequence of text embeddings, and entire sequence passed as input to the LM (cf image)  
The comparison's outcome is the following â‡’ ð—™ð˜‚ð—¹ð—¹ð˜† ð—®ð˜‚ð˜ð—¼ð—¿ð—²ð—´ð—¿ð—²ð˜€ð˜€ð—¶ð˜ƒð—² ð—®ð—¿ð—°ð—µð—¶ð˜ð—²ð—°ð˜ð˜‚ð—¿ð—² ð—¼ð˜‚ð˜ð—½ð—²ð—¿ð—³ð—¼ð—¿ð—ºð˜€ ð—°ð—¿ð—¼ð˜€ð˜€-ð—®ð˜ð˜ð—²ð—»ð˜ð—¶ð—¼ð—» ð—®ð—¿ð—°ð—µð—¶ð˜ð—²ð—°ð˜ð˜‚ð—¿ð—² when you fine-tune the whole system using LoRA
 
âž¡ï¸ ð—§ð—µð—²ð˜€ð—² ð—³ð—¶ð—»ð—±ð—¶ð—»ð—´ð˜€ ð—¹ð—²ð—± ð˜ð—¼ ð˜€ð—²ð˜ƒð—²ð—¿ð—®ð—¹ ð—®ð—¿ð—°ð—µð—¶ð˜ð—²ð—°ð˜ð˜‚ð—¿ð—®ð—¹ ð—¶ð—ºð—½ð—¿ð—¼ð˜ƒð—²ð—ºð—²ð—»ð˜ ð—¶ð—» ð—œð—±ð—²ð—³ð—¶ð—°ð˜€-ðŸ®:  
âž¤ Replaced cross-attention architecture with fully autoregressive architecture  
âž¤ Enable treating images with varying aspect ratio  
âž¤ Allow to split an image in 4, to be encoded on 320 vision tokens instead of 64, if you want to increase perf at the cost of more compute
 
âœ¨ As a result, Idefics-2 reaches state-of-the-art performance for this model size! Now just a few more steps to catch up to GPT-4o!
 
Congrats for this great release [LÃ©o Tronchon](https://www.linkedin.com/in/ACoAACAdueYBmpnFOU8_oo7CAOBp-zGvll6mmV8) [Hugo LaurenÃ§on](https://www.linkedin.com/in/ACoAACMxEXQBc5mFHZjXBGUGf8lXLH54NjgGr_A) [Victor Sanh](https://www.linkedin.com/in/ACoAABuSz-MBn6jxcRvc72Y7BoEWaz4cUTI8D0A)! ðŸ‘
 
ðŸ‘‰Â ð—¥ð—²ð—®ð—± ð˜ð—µð—² ð—œð—±ð—²ð—³ð—¶ð—°ð˜€-ðŸ® ð—½ð—®ð—½ð—²ð—¿: [https://lnkd.in/esKMq6Eu](https://lnkd.in/esKMq6Eu)  
ðŸš€Â ð—”ð—»ð—±ð—¿ð—²ð˜„â€™ð˜€ ð˜€ð—½ð—®ð—°ð—² ð˜ð—µð—®ð˜ ð˜€ð—µð—¼ð˜„ð˜€ ð—¢ð—¦ ð—ºð—¼ð—±ð—²ð—¹ð˜€ ð—°ð—®ð˜ð—°ð—µð—¶ð—»ð—´ ð˜‚ð—½ (ð—³ð—¼ð—¿ ð˜ð—²ð˜…ð˜ ð—ºð—¼ð—±ð—²ð—¹ð˜€): [https://lnkd.in/e5vX_Ukb](https://lnkd.in/e5vX_Ukb)  
âš”ï¸Â ð—–ð—¼ð—ºð—½ð—®ð—¿ð—² ð˜ƒð—¶ð˜€ð—¶ð—¼ð—» ð—ºð—¼ð—±ð—²ð—¹ð˜€ ð—¶ð—» ð˜ð—µð—² ð—©ð—¶ð˜€ð—¶ð—¼ð—» ð—®ð—¿ð—²ð—»ð—®: [https://lnkd.in/eAuDnSiV](https://lnkd.in/eAuDnSiV)
 \> From \<[https://www.linkedin.com/feed/](https://www.linkedin.com/feed/)\>