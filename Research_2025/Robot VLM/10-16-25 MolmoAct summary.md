Robot control systems that accept only text input struggle to translate words into motions in space. Researchers developed a system that enables robots to plan spatial paths before they execute text instructions.
 
**What’s new:** Jason Lee and colleagues at Allen Institute for AI and University of Washington introduced [MolmoAct](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/MX1q08nTR79VTr8fk3qz-t_W93kvSZ5DHMk-N7WbryH3qgz0W7lCdLW6lZ3m3W6Fskss85HqKTW5BZYSq5JKNtWW2J7CWZ92DB5tW71p8QZ7gdgrTW5sDb-g5sMp4xN3Dwc_nB3CGpW930n611kPZp1W8-4gDZ5L_R0rW6trGDn6myMfdW9f8M8P8TGTdCW7gSdM_3nLMjqW8q-9XQ6mrsXGW3LtTzn4wjzrTW48CM0z4mHZF6W7GH-Qz1MB6pxW224skM3LlfRBW5SdPb31x-QZLW7yTKq32s-wYLV6GsXZ4Wtsz2W8dQW858jRJlkVDRgdV694hs1W67wC_B8wf8thW10VPpp5kTL8NW6VSxRD5pqhryf4tmxWs04), a robotics action system that improved a 3-jointed robot arm’s ability to manipulate objects and perform multi-step tasks by first estimating spatial depth and planning motion paths. The [weights](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/MX1q08nTR79VTr8fk3qz-t_W93kvSZ5DHMk-N7Wbrzg3qgz0W8wLKSR6lZ3n2N6p2CfTJgfHMW5L6Fph8yn7N5W8pnPrh9g-8mFW4210f26fPgjKN1w2lspPc3lMW5d7yP33qcRyLW82djf_6s3dtNW3C5yXL2q67f8W48WGmG6VSLgfN2-Ck6Tg7bFpVG70JR494qHhW22h5hh5VljmjW7qvz0c4B3BqqV_17Yx8Wd2WmW9jXsc67Rqp4VW7g9cSY63cDR_N74h0xSdlZ-RW96MqmP3rG450W602v6V76wxVMW6qWK207s7nJVW5SdYgj1ShD78W8b5JPt8wDxRwW7Gqlky2fGHF6Vg8S6h1_YWrqW4WXc-18ps2-5W4mp8808qGvhcV7Cbn76kQpsgW1b45x33pgqCYf5sG4kY04) and [code](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/MX1q08nTR79VTr8fk3qz-t_W93kvSZ5DHMk-N7WbryH3qgz0W7lCdLW6lZ3p8W4vg43m5pXGjfW1WW8832WK5DNW2Cf2rM8h84JXW6N-KgD7vgvpJN6Wh8vdGYCHXN9d_3ZK1yrJSW8ScHv72f-74bW3pj5fq6JG3cmW2Wmcdt23g6X6Vhl0M47ls1pBVnQsSv32W-4BTZFgG1MYggxW2w86Fl4BY95nW6q0Qqm9lzK78W3nJTLB8SGgmdW1hX2zd4CMhLXW1_1kF544mTgDW4lQKZf6gL_tXW28gmyW6v0bCgW6wB_Tj9fWdJHW7LX14W8cmRJ7W63fNnZ5yQbCDN97zySqlcXhTW5XJHwp41LyWvdwvQxR04) are available for noncommercial and commercial uses under the Apache 2 license, while the authors’ fine-tuning [dataset](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/MX1q08nTR79VTr8fk3qz-t_W93kvSZ5DHMk-N7Wbrzz3qgz0W95jsWP6lZ3kXW14_jnv3MQVfYW7Pj5G4746fkWN94JRtB-sVlGW7PRD-X3pkLWCW8jWnwR7SM1wlW3vLNBC1Y0KVqW96s4fm7vmjW5MGjjH99X-FkW6Nbdx24BC51nW5rzyhZ87BRXcW97wyFk6g_rKhW3r7JVX5Z2KNhW2mWNPd1NM--KV9GLdk582m4FW2YyJQ338s4SnW63ND7C8X5C8QMJ_ff6-rLL5W2kwcvv4dXK-vW1Lkcx849v9wcVplKzS6wwnD4N8Lg6JXzWMfyW5wWfCr8shcSVW21Vb1Y93RrvwW8RW2rk9cFxVXW8czkMk7ttt8sV4PsMt19G0N4W4dK-t97N9zsTW96gJK61Wrk51W3wprSr97zcyRW5Hdd-n58DZB9f5FSF7604) is available under CC BY 4.0.
 
**Key insight:** Natural-language instructions don’t translate precisely into spatial directions. Just as humans can navigate more effectively with a map, robots perform more accurately given a sense of 3D space (a depth map) and the desired trajectory (a motion path drawn over a camera’s view). Along with a command like “take the cup off the table and put it in the trash,” the additional information enables a robot to avoid collisions with objects and move more precisely.
 
**How it works:** MolmoAct uses a [SigLIP2](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/MX1q08nTR79VTr8fk3qz-t_W93kvSZ5DHMk-N7WbryH3qgz0W7lCdLW6lZ3pbM-RFnSr70jTW2TGqkw2GjjznVzYd_156pZ7kW2S1gST8_RdJxW1fdCNX5GFWb9W7YKZcs5hgpmNW52ffD269YBkDW7n8VcR78KFlFW5vr7C03DG73XW9bKRcc6SdsWnW8thM_B1-nQ7fW8FWrwj2P7J7hW2y4t438QJL6qW1twMk73s0jNwW4n60tW7Hh0v_W8Dlqj56vWwZ_N2mz4mQcfWlBN6b6VKqnPlC4VhBPrV99M4byVVFPmQ27M9FcW8vZ6yQ950qFyW1CcHdY4FzpNsW6FMR-_2Stn1dW6QfXsR64W5hrf8znBW404) pretrained vision transformer to encode camera images into tokens. Given the image tokens and text instructions, a pretrained [Qwen2.5-7B](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/MX1q08nTR79VTr8fk3qz-t_W93kvSZ5DHMk-N7WbryH3qgz0W7lCdLW6lZ3kWW7VJZ1X7YXlwDN2Jpwz8dQbVgW9jd17P7fPzvgW6JD6W14QjwdcW5WZCTP623trqW2WK-PQ7gsM0cW7gvXch8P7BG1W3yjnDH78VTQQW2_TY-F6WdTj2W6vGjCR1GXYwtW8cC-kz6vZY_XW2VWfD42Z11fDW79F-wy2MFQsbW6gYQJx3TfZs-W4Mb2vt5LcTf1W9gZf783F-Yg8W6pNdzd6YmsdVW14G1nX8B-xt0W7-gPqC3QJPkBW7D4H0Y65TxssW3S3ynN7W_hclN5pr7syVfm-dW3wwqwQ5PcMChW4-4zCW1kmtL2f5r_MgH04) large language model learned to generate tokens that represented (i) a depth map, (ii) a motion path, and  (ii) changes in joint positions.

- The authors started with 24.3 million [robot demonstrations](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/MX1q08nTR79VTr8fk3qz-t_W93kvSZ5DHMk-N7WbryH3qgz0W7lCdLW6lZ3kFW4WF-C33xnTCsW2nX66M42hBNzW61TMmY7_MLrJW1TtCRj6M8GmsV2qJNY5XSZxWW6yc80V4zxCrtW5dqxSW9fjlZkN72SXRTGNFV8W8341-53B8-yBN3c7wblRGM8mW7jtfBc19xqSVW64bTMD2Bhz13W8mp-m71ZJDfkN2tgs8vKdnQJV63FrM51dg0QV9kjXS5cjWrXW89SwXh25jcHnW5fcVDt6pzsBqN6b_c7fFwSF_W7fm7c156lJVdW36B63G7CLJMhW4GNvrN464Tf9W26vzTG7CZWYSN3sP52LgzML8dt-mmF04) of tasks such as “pick up the water bottle from the drawer and put it on the desk.” Each example included a text instruction, camera views, and changes in joint positions. The authors augmented the examples with depth maps and motion paths.They generated the depth maps using a pretrained [Depth Anything 2](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/MX1q08nTR79VTr8fk3qz-t_W93kvSZ5DHMk-N7WbryH3qgz0W7lCdLW6lZ3pyW8_PHSL6ssRXRV73Bcf4zZvVhW8F_K1T8jvVfkW6ZtJ1R99s5ppW6D16VZ9g5hsWW7DK2Bf2yKpsRW2KtG1V6wNy77VyVFGk5XBlxVW3QN7cn6NNWlJW6ZXWjZ878dlPV2F5zX9gsFvhW1Bz7pw5Dw4z1W2pP7wn17BKpfW8DwPxZ2Z-MmcW3r4c6q4pq1KTW7mG6LN9kvl-hW2_D0r47TGsHhW87SzfP18NF-HW44FY7M1c6lhmW8wGfSp1QnvC4VzVJbD3lttXKVz3BgJ14Qms1W6gdN0n2M7bHMW7BGq9S68_hdVf7RSHz404), and they produced visual paths by tracking the robot arm’s gripper in the camera images using [Molmo](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/MX1q08nTR79VTr8fk3qz-t_W93kvSZ5DHMk-N7WbryH3qgz0W7lCdLW6lZ3pTW18Ml6X978mGDW3MMTZC5N_h35W5cz3zM5DP0YdW7K4Wkr349z9nW3hLHzf3LWTgZW4T-rZ351MzzYW1GJBMW1qg3qyW3frY7F6vBVNxW8GZ_Q32x-36GW2Rnl4-11syBgW7KjF8D5Pg9rCW2Y4y-M66cVP3N4kwkVXCHH-pN91_QhmlVT9YW1zlN7w3FqNZ0N7CZGbM48xLjW6XhJBF1bBhZMW4Jt-bN8C01C0W522XMn18fdGXW5lV0Qr65GY9gW65RW8m5PCqzTW70YZ675-KnctW4rXQMG41xgmjW89WSfc1xjy4_f2Sy5Z204), a pretrained vision-language model.
- They trained Qwen2.5-7B on the augmented dataset. Given a text instruction and camera image, the model learned to generate tokens that represented, in this order, (i) a depth map, (ii) a visual path, and (iii) changes in joint positions.
- To improve the system’s vision-language understanding, they further pretrained both models on 2 million examples of images and text scraped from the web.
- The authors fine-tuned the models to generate the next token in more than 2 million examples, which they collected themselves, of robots performing various tasks from start to finish. The examples included various combinations of text instructions, camera views, changes in joint positions, depth maps, and motion paths.
- At inference, users can see the next motion path before the robot moves and revise it by redrawing it via a tablet. This capability makes the robot’s actions interpretable and enables users to address potential errors before they happen.

**Results:** The authors tested MolmoAct’s performance using one or two Franka robotic arms in a simulation as well as 15 real-world tasks, including opening a container, putting trash in a bin, and folding a towel. On average, the system outperformed all other competitors.

- MolmoAct achieved 86.6 percent average success on diverse simulated challenges in [LIBERO](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/MX1q08nTR79VTr8fk3qz-t_W93kvSZ5DHMk-N7Wbry-3qgz0W7Y8-PT6lZ3mZW6pf2-S2ksgj-W1jsf_p6sn2NwW8hYTd09cM-XdT6v9W4w4hjdW54fbCw7N3fRVW1xGdQx69hW4XVrMSBJ7ntNyxW6bsMdr3H9Z4kW4htlqy7MqQfwV-t3c29kqhTPN3D4SXQXXp7NV1Wkf27fRK3kW8dsFM41KRZ3BW8V0mMH6fyGhTVQ8wlD5XYhlQW2qqmWF8ZSlK5W6qdQNR3qK4v-W7LHvFk7W7X0zN4fNsbxn9gvpW7g_1vx5J85zzW1KQ48G5lHhGbVXyJPQ4H8GDFW877M3x6v5DF6W1zyWg63JPNwQW4VZg019jvmDTW3f77kX5NRjFzf1MRNg804). The closest competitor, [π0-FAST](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/MX1q08nTR79VTr8fk3qz-t_W93kvSZ5DHMk-N7WbryH3qgz0W7lCdLW6lZ3n_W1LQ7lb82P5sTW3qMns581kmnpVwN2YD4yT1LTW8FnVHT8M58jkW2NVs774sTfs4N24-nbrv2d8BW5N1CgN87sCY9W6TLQZx5BS36GW8rqzSV4jPmV0W5x528y4BR67JW2j-40p1N6xsPV6WnH779tsZQN3qxm18MPfyBW8lNvZ141BB7JW3W3-WH28v5vxW6-XgRk6h9-KsW1qgn9b2G9hnVW7rPZ_05np4NVW38bl3L4xQLfvW2cM-qQ1pbrJ8N19RmQSQMZn2N3qzJZd8v1p9W7Q3NWG48b-SrN4JhGbKQWmT6f3Dlh1g04), achieved 85.5 percent average success.
- In real-world tasks, MolmoAct achieved 0.679 average task progress (a 0-to-1 score that represents how much of each task the robot completed, higher is better), while π0-FAST achieved 0.446 average task progress.

**Why it matters:** Earlier robotic control systems that use LLMs to interpret text instructions map visual input and text instructions directly to low-level actions without explicitly representing 3D space or visual motion paths. MolmoAct's approach makes such systems more precise, adaptable, and explainable.1