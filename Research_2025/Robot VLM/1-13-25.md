FuSe: [https://arxiv.org/pdf/2501.04693](https://arxiv.org/pdf/2501.04693)
 
Can generalist robot ðŸ¤– policies understand heterogeneous multimodal sensors?  
Introducing FuSe, a novel finetuning recipe allowing VLAs to reason across ðŸ‘ï¸ vision, ðŸ¤ touch, ðŸ”Š sound & ðŸ—£ï¸ language, enabling queries "pick the object that feels likeðŸ but is brown" (hint: pinecone)
 
Interacting in the ðŸŒŽ is a multisensory experience, but pretrained generalist policies only use vision+proprioception. How can we connect the semantic knowledge ðŸ§  of generalist policies with new heterogeneous modalities? We find that natural language acts as an excellent bridge ðŸŒ‰
 
How does it work?  
Our multimodal VLA finetuning recipe combines two auxiliary losses:  
1ï¸âƒ£ A contrastive loss to maximize mutual information between views & semantics of the same scene  
2ï¸âƒ£ A language generation loss to predict high-level semantics across modality combinations
 
The best part? VLAs can now handle multimodal and cross-modal prompting for new sensing modalities and generate object descriptions solely from touch! ðŸ¤ Perfect for partially observable tasks like grasping items from a shopping bag ðŸ›ï¸ðŸ¤–
 
How does it perform?  
Over 1000 real-world evals. 100+ unseen objects. Challenging tasks, some with partial observability, requiring joint reasoning across vision ðŸ‘ï¸, language ðŸ—£ï¸, touch ðŸ¤ & audio ðŸ”Š  
FuSe models outperforms strong baselines by a large margin ðŸ“ˆ + results in paper
 
How to use it? It's all on HuggingFace! ðŸ¤—  
âœ… 27K robot trajectories across diverse envs with touch ðŸ¤, audio ðŸ”Š, language ðŸ—£ï¸ etc.  
âœ… Open-source code & model weights  
âœ… A 3B PaliGemma-FuSe VLA ready to go ðŸš€
 
Really excited about this work! While single-image-based transformer models have dominated due to easier data scaling and data scarcity for touch sensing ðŸ¤ & co, a truly generalist robot policy ðŸ¤– will need to understand multiple sensing modalities for robust decision-making ðŸ”
 
Big shoutout to my co-leads [Carmelo (Carlo) Sferrazza](https://www.linkedin.com/in/csferrazza/) and Josh, and thanks to fantastic collaborators [Kyle Stachowicz](https://www.linkedin.com/in/kylestach/), [Pieter Abbeel](https://www.linkedin.com/in/pieterabbeel/) and [Sergey Levine](https://www.linkedin.com/in/sergey-levine-5a31a24/) Had a great time with this [Berkeley Artificial Intelligence Research](https://www.linkedin.com/company/bair-lab/) collaboration!
   

Paper: [arxiv.org/pdf/2501.04693](http://arxiv.org/pdf/2501.04693)  
Website & Code: [fuse-model.github.io](http://fuse-model.github.io)  
Models & dataset: [https://lnkd.in/gfPgXBUX](https://lnkd.in/gfPgXBUX)
 \> From \<[https://www.linkedin.com/feed/](https://www.linkedin.com/feed/)\>