Can we robustly track an objectâ€™s 6D pose in contact-rich, occluded scenarios? Yes! Our solution, V-HOP, fuses vision and touch through a visuo-haptic transformer for precise, real-time tracking.
 
Unlike vision-only methods ðŸ‘€ that struggle under occlusion and rapid dynamics, V-HOP leverages a unified haptic representation to enhance robustness and accuracy.
 
Inspired by human multisensory integration, we combine visual cues with tactile and kinesthetic data - represented as a coherent point cloud - for more stable perception.
 
With exceptional generalization across diverse gripper embodiments and sensor types, we show that we can reliably achieve precise manipulation tasks by incorporating our real-time object tracking results into motion plans, even with human perturbations.
 
This project is a collaboration with Mingxi Jia, Mete Tuluhan Akbulut, Yu Xiang, George Konidaris, and Srinath Sridhar
 
arXiv: [https://lnkd.in/eXiD2pZz](https://lnkd.in/eXiD2pZz)  
Project: [https://lnkd.in/ebTZQaxP](https://lnkd.in/ebTZQaxP)
 \> From \<[https://www.linkedin.com/my-items/saved-posts/](https://www.linkedin.com/my-items/saved-posts/)\>