We've just open-sourced nanoVLM, a pure PyTorch library for training Vision-Language Models (VLMs) from scratch â€” in just 750 lines of code! ðŸš€
 
With just 6 hours of training on a single NVIDIA H100 GPU, nanoVLM-222M achieves a 35.3% score on MMStar, matching SmolVLM-256M performance â€” but using 100x fewer GPU hours. ðŸ‘€
 
You can even comfortably train nanoVLM in a free Google Colab notebook! ðŸ¤¯
 
nanoVLM, inspired by Andrej Karpathy's nanoGPT, is designed to be simple, readable, and easily modifiable. The whole model is implemented in ~750 lines of code, and the training in ~200 lines of code.
 
It consists of a:  
ðŸ§  SigLiP-ViT vision encoder  
ðŸ—£ï¸ LLaMA-style language decoder  
ðŸ” Modality projection layer to connect vision and text
 
We'd love to see what you build with nanoVLM â€” share your creations with us! Links in the comments!
 \> From \<[https://www.linkedin.com/my-items/saved-posts/](https://www.linkedin.com/my-items/saved-posts/)\>