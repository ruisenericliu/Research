[https://www.jetson-ai-lab.com/openvla.html](https://www.jetson-ai-lab.com/openvla.html)
 
Ready to enter the world of embodied agents and physical AI? Come along with us at Jetson AI Lab as we dive into Vision/Language Models and OpenVLA in sim, fine-tuning, and deployment [https://lnkd.in/gR_CAQAz](https://lnkd.in/gR_CAQAz)
 
First we extended our lead streaming VLM stack with optimized quantization & inference for edge VLA's. Then integrated MimicGen from Yuke Zhu @Yuke Zhu and Jim Fan for an endless stream of training episodes for OpenVLA to learn new tasks from Karl Pertsch. We dialed in the workflow with test LoRA's onboard Jetson AGX Orin, and when the initial results were looking good hopped on some quick A100/H100 instances from Brev.dev
 
The result: can stack blocks like a toddler ðŸ¤£ðŸ¤–
 
But we cleared the workflow, learned a lot along the way training our own vision/language models, and the whole space is progressing rapidly - the latest last week was Crossformer. Kudos to RAIL Lab at Berkeley for all the amazing science they are sharing. Over time the workflow for robot learning will become streamlined to the point where it can adapt in-situ by end users, and perhaps robots will undergo nightly fine-tunings the same way that humans are thought to relive/simulate the day's experience during REM sleep.
 
In the end we'll all become model tamers and robot mechanics âœŒ
 
Get the VLA training + deployment guide here! - [https://lnkd.in/gR_CAQAz](https://lnkd.in/gR_CAQAz)
 \> From \<[https://www.linkedin.com/my-items/saved-posts/](https://www.linkedin.com/my-items/saved-posts/)\>