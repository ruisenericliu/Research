[https://huggingface.co/papers/2506.17218](https://huggingface.co/papers/2506.17218)
 
ðŸ§ ðŸ‘ï¸ Can AI visualize solutions?
 
Humans often solve visual problems by sketching ideas in our minds. What if Vision-Language Models (VLMs) could do something similar, not by generating full images, but by using internal â€œmental sketchesâ€?
 
Thatâ€™s the idea behind Mirage, a new framework that empowers VLMs to reason using latent visual tokens. Instead of just thinking in words, Mirage mixes in abstract visual representations that help the model solve complex tasks.
 
These aren't photorealistic images. They're compact, internal representations optimized purely to support reasoning.
 
ðŸ”§ Mirage is trained in two phases:
 
1) Grounding: It learns to produce latent tokens anchored in real images.  
2) Refinement: The model drops the images and learns to generate visual tokens on its own.
 
ðŸ“ˆ And yes, it works!  
On challenging benchmarks like Visual Spatial Planning, Jigsaw puzzles, and Spatial Attention Tasks, Mirage clearly outperforms GPT-4o and other strong baselines.  
Smart sketches \> empty words.
 
By mimicking the way humans visualize solutions, Mirage gives AI a new kind of imagination, one thatâ€™s faster, more efficient, and more human-like.  
Kudos to the teams at UMass Amherst and MIT behind this exciting work. Check the comments for a link to the paper!
 \> From \<[https://www.linkedin.com/feed/update/urn:li:activity:7346509967887130624/](https://www.linkedin.com/feed/update/urn:li:activity:7346509967887130624/)\>