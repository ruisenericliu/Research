[https://www.reuters.com/technology/artificial-intelligence/openai-rivals-seek-new-path-smarter-ai-current-methods-hit-limitations-2024-11-11/](https://www.reuters.com/technology/artificial-intelligence/openai-rivals-seek-new-path-smarter-ai-current-methods-hit-limitations-2024-11-11/)
 
Ilya Sutskever, perhaps the most influential proponent of the AI "scaling hypothesis," just told Reuters that scaling has plateaued.
 
Here's a couple of direct quotes from the Reuters article:
 
ğŸ’¬ "Ilya Sutskever, co-founder of AI labs Safe Superintelligence (SSI) and OpenAI, told Reuters recently that results from scaling up pre-training - the phase of training an AI model that use s a vast amount of unlabeled data to understand language patterns and structures - have plateaued... Sutskever is widely credited as an early advocate of achieving massive leaps in generative AI advancement through the use of more data and computing power in pre-training, which eventually created ChatGPT. Sutskever left OpenAI earlier this year to found SSI."
 
ğŸ’¬ â€œThe 2010s were the age of scaling, now we're back in the age of wonder and discovery once again. Everyone is looking for the next thing...Scaling the right thing matters more now than ever.â€
 
This comes on the heels of a big report that OpenAI's in-development Orion model had "disappointing" results. It's becoming evident that the jump from this gen of LLMs to the next-gen of LLMs won't be as significant as e.g. the jump from GPT-3 and 3.5 to GPT-4.
 
Sutskever raised earlier this year $1B in the largest seed round in venture capital history for his startup SSI at a $5B valuation (pre-product).
 
But itâ€™s still worth noting that SSI has way less total funding than its competitors (OpenAI, Anthropic, xAI), so itâ€™s in Sutskeverâ€™s interest to advance this narrative.
 
However, the harsh truth is that we haven't seen anything more than marginal improvements since GPT-4 launched 1.5 years ago. But the price compression per million tokens has been unbelievable. The cost of an LLM of constant quality is currently decreasing by 10x per year.
 
As highlighted by [Saanya Ojha](https://www.linkedin.com/in/saanyaojha/), a slowdown in model improvement could be a blessing in disguise. It means the tech isnâ€™t evolving under everyoneâ€™s feet every few weeks, allowing founders to build with more certainty instead of scrambling to stay ahead of ever-shifting baselines. If youâ€™re not constantly trying to keep up with the latest model release, you have the breathing room to think strategically.
 
Even if AI model improvement would hit an asymptote today, there would be at least a decade worth of implementation opportunities e.g. in the enterprise with the current model capabilities.
 
I still firmly believe that we've entered the golden age of building.
 
To quote [Gustaf AlstrÃ¶mer](https://www.linkedin.com/in/gustafalstromer/) ğŸ‡ºğŸ‡¦ :
 
"The last 18 months have been the most exciting since I arrived in the Bay Area 17 years ago. It might sound like yet another hype cycle, but it's not. AI is more real than anything I've seen in technology."