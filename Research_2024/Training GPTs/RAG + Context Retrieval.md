Answering by approximate retrieval or by understanding+reasoning are two ends of a spectrum.  
Humans occupy various locations on this spectrum, depending on the task, experience, and depth of understanding.  
Students of physics and math exemplify this dichotomy; some study diligently, complete numerous problems, learn solution templates, and possibly achieve passing grades, while others barely study yet attain top grades due to their mental models that enable reasoning and intuition.
 
The same is true for AI. Current (Auto-Regressive) LLMs are pretty close to the "retrieval" end of the spectrum.  
They don't possess good mental models of the situations they face.  
To get to the next level in AI, we need machines to learn mental models that can be used for reasoning and planning.
 
In this thread, François points out that current LLMs have pretty much failed every single reasoning and planning test thrown at them, as long as it wasn't in their training set and they could not rely on mere retrieval.
 \> From \<[https://www.linkedin.com/feed/](https://www.linkedin.com/feed/)\>        

_P.S. Vector databases are a backbone of large language model (LLM) search and data-retrieval systems, for example in retrieval augmented generation (RAG). In our new short course, created with Weaviate and taught by Sebastian Witalec, you’ll learn the technical foundations of how vector databases work and how to incorporate them in your LLM applications. You’ll also learn to build RAG and search applications. I invite you to sign up for “__Vector Databases: from Embeddings to Applications__”!_
 
RAG++: [https://arxiv.org/abs/2310.01352](https://arxiv.org/abs/2310.01352)
 
**Tuning LLMs for Better RAG**  
Retrieval-augmented generation (RAG) enables large language models to generate better output by retrieving documents that are relevant to a user’s prompt. Fine-tuning further improves RAG performance.  
**What’s new:** Xi Victoria Lin, Xilun Chen, Mingda Chen, and colleagues at Meta proposed [RA-DIT](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWrsK69hFYL4N7QLpjSVmXT4W1FPBGm5d0pkgMDyKmx3qgyTW6N1vHY6lZ3k_W6wd76G7YC2sjW75RDll5JtjrpW7LxNn686DV9qW8Gg6C16NhcVgV9j4355htwmlVykf6W78rb0WW3JRvvP65WBNVW5t4Nxl1QyDdSN8vnrY_X2gWkW5BzJQH8dTq66W5j-FDy1HKhSlW7YQ8N-2Dh7HgW7GgSdH1Br1dmW91tLGc8nrq4NW2JX3X89brKw1W5khY0-8BJ2nFVnlRR-1fZHfgW3H9Xth3kr2dYW3KKS2g5nqfBdV8bkqH7M875XW7_7x8367ffctW96M6Gn1JyPtlf2ZnWtP04), a fine-tuning procedure that trains an LLM and retrieval model together to improve the LLM’s ability to capitalize on retrieved content.  
**Retrieval augmented generation (RAG) basics:** When a user prompts an LLM, RAG supplies documents that are relevant to the prompt. A separate retrieval model computes the probability that each chunk of text in a separate dataset is relevant to the prompt. Then it grabs the chunks with the highest probability and provides them to the LLM to append to the prompt. The LLM generates each token based on the chunks plus the prompt and tokens generated so far.  
**Key insight:** Typically LLMs are not exposed to retrieval-augmented inputs during pretraining, which limits how well they can use retrieved text to improve their output. [Such](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWrsK69hFYL4N7QLpjSVmXT4W1FPBGm5d0pkgMDyKmR3qgyTW7lCdLW6lZ3nPW4jrJ3w3hzf3TW93RN0F3ZyXjjW3GWNdv1JGDBYW6754dz49GMpxW1lzRyH4LrwzLW8j8FYp8MW-lTN8QNLDmDqKggMdfQbfYY94vW1p-Bxd77gv5wW4rLY3f370ZFbW1CKn0V6MDSt1W2N3K7121mcJrW7D10Yx8NDZj_W6k_D8m8WWWT8N7fk_81k7DjRW58z3MV8KFRdxW5j8Mmk4nMC5MW3HN16z40h5qcV47nl_6J9fJFW34G3ND3dlSt3W7h9tpB2HQ6fqVlvnc58RvxWRW3yQ6nL14F0pTW4fHWKQ5KyRB0f8Pf7Gb04)[methods](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWrsK69hFYL4N7QLpjSVmXT4W1FPBGm5d0pkgMDyKmR3qgyTW7lCdLW6lZ3kwVdwvBb3vyjqtN81Qrdwz-cMWW2VXCHB8Vc5cCW2CShcG4XQBVlW7ghcmv7GMt0rW2x9MTP8X7YpqN1_6GWqNwTFGW8--MNM7pkKZfW45Hmw03FlKGkW76TTzX3HQmDsW6x16gw3JyCYsW4cwxbq6nRFXVW2lXgMz1kN7pzW6j6Cj07XHSKpW2Q8sFq8cp6_zVSNtPP4xWK9CW1rwCP-79ph60W7cJw4d4CryxxW2RVNPx1p043XW4j2TGF1wV-xvW4hj1GV8LpcVbW3SZHDG90yBkwW3hl0j01BkYfHW23lX3n3Lffjwf3kgG3Y04) have been proposed, but they’re costly because they require processing a lot of data. A more data-efficient, and therefore compute-efficient, approach is to (i) fine-tune the LLM to better use retrieved knowledge and then (ii) fine-tune the retrieval model to select more relevant text.  
**How it works:** The authors fine-tuned [Llama 2](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWrsK69hFYL4N7QLpjSVmXT4W1FPBGm5d0pkgMDyKmx3qgyTW6N1vHY6lZ3njW1Rv11b4KMLnbW4Kwz-z3GBR-XW1_RLrY8hKNz2N3Sc2N2xMPhqW6TvfbM7WxW1MW5v799t1m0ZK2W5zpM0c6dvDMcW3xjJwt4MhVf9W5FJSCQ6Dt1F3N2S2ZKscK_dnW93ySBq2cfyPTW7Y9fdK2db1G-N2mDv3KTT1DcW7hLHB53ny5h0W479Gp86CK2xnW1tQL7W3Kp3WlVWqKKW4mcRpzMn2msWlwP-FW3br5rq3nmzFLW1dx4G_8H7ZVlW80RlXX6SMDb_W3sC3vv5hkSCwf3HxJMx04) (65 billion parameters) and [DRAGON+](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWrsK69hFYL4N7QLpjSVmXT4W1FPBGm5d0pkgMDyKmx3qgyTW6N1vHY6lZ3pwW8wRvFZ4z5M-9W9jxmZ41NKWccW6VSxSL8FW98xW6jQCS12PF-lYW91C5yZ1Rml6mW1D8yJ55CFnPMW2tnSwC4rmbGdW4G8TJ24MWm_VW1SbyTX15js8PW8yLCY34hS7mMTf8bQ3bdCm4W3wmP6S7_qt-0W4BJGbT3qNVk0W7rKwt67VqzW2W7t3d6C2wn3MqW7sBRpl4r8-3fN8QlVx3B-1_RW317Kpj7LLGDYVky6D_68RZh9Vnh51p924x4cW3SvYhD80-8-fV7pzfX5wLHrBf6p-Zdx04), a retriever. They call the system RA-DIT 65B.

- The authors fine-tuned Llama 2 on prompts that consist of retrieved text and a question or instruction. They used 20 datasets including [dialogue](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWrsK69hFYL4N7QLpjSVmXT4W1FPBGm5d0pkgMDyKmx3qgyTW6N1vHY6lZ3mgW3j6WK53CDyfDN58mwLc66ybjW94r8nJ15kr6BW1DXJKd4J_B7TVwKtKm1SNSL-VN_j833KYqkbW6DB3Dj1LzzYhW4SBNDW6hrbF3W3cX4-F7hD_qHW5dqHvP8mrJrNW54RxY18dwHfDVwfCln2cSb15W7qHc-J4m50krW31-0xZ2kTbNGW6XY2LR1B9fStW48gxb617t9w3W2gJ-wD2yRWFyVtkRX29c7hKLW7wzJx55cNLjLW6lsRB47fNsrfW40FLXb8fp5PbVxf13m740CjXf5RdCf204), [question-answering](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWrsK69hFYL4N7QLpjSVmXT4W1FPBGm5d0pkgMDyKmR3qgyTW7lCdLW6lZ3p2W3DpNl28RVNGjW4pjjPM2l4_pFW2bnT5Z2XYM5CW2YPQgT5ftyZRVVCZ7M6dl7b5W5kwF3Y76HWrrVbNN_-5BPn23W9m0qJw1CGhMrW8MbpSC1ScwYhW83Wtyw70Nbn2W4YmsGZ7fw0l5N4dc4_8ZdNg2Vljrcl8sr_sZW4K4t0c8PqhY4W2lb-hv49srMbW8d10sM6-VbVKVBv4Nl4LpMxZN8QxR0yWwd3zW1QQsFv1D3rmrW7vv7w68Dnv0DW1HMq-v6QfhdgW7WbjN68zrZjQN4Wzc7dNBT6mW1Zq1LY5tG6nLf2_bXJg04), [answering questions about a given text passage](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWrsK69hFYL4N7QLpjSVmXT4W1FPBGm5d0pkgMDyKmx3qgyTW6N1vHY6lZ3nBW7yTfYb23ND3XVPV0Ll2WLy6nN6jwnS83Ch9FVS8NST8TXtWxW3235w-5kR4GRW33KZc99kLB0tW8S_1n37jxwx_W81qjpX172blzW8-yy523fBwjKN7x2Hd6jfWR5W4mDYxt3bG1SLW38Y2YT7LSmNRW4GYqQN4y_NgbW5nqv4r7z1K-HW1XzLs386KncgW39JMWR1x7FfpV8G78n7QNpmKW1Bc6Cz8fYzC-W4N067d4rs2W7W24xxd971klnmN2P5GZwz8TR5W97jxfp3x1BBDdLVW1Y04), [summarization](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWrsK69hFYL4N7QLpjSVmXT4W1FPBGm5d0pkgMDyKnK3qgyTW95jsWP6lZ3pBW4j6XXR8SZDRpW89l5WP75NjM4W8mlMKm2nMLVQW7jK9Yy89gXXZW4zq_Tg5RDv9-W2YD0nM46VQqSMDPd_3tbynSW5F9DF73_kZZVW63Mm4440mQVdW2HK1GH2lBJSZV4J6zW2htdb9W2ymW5x90K8JfW8kZ8NS6KSBcPW2B5nR18tcQ6DW64nyKx8nbsrVW7mwXnb4Sdq7LW5cnXCl8bmhP1W93vxYk997qSNW94bPRS7ccDtJW4TGV4h50zwRFW5sxDvl1n8gd-W5Zb5fY6-tYZ5VhH2B63wWw1SW3xtjCm313ssbW5mGV745D7C0kW2kFbLx7ghGkyW98kQl340FnD9W7v8znX4xGM0dW2gxqVf1wnNKWW8PzTCT4BS1mXf4-wxTn04), and datasets in which the model must answer questions and [explain its reasoning](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWrsK69hFYL4N7QLpjSVmXT4W1FPBGm5d0pkgMDyKmx3qgyTW6N1vHY6lZ3nGW1x0mtD8vS4cyW655q_J8B5w9_VdFrKM5CgLzFW8tff-D4NNjJKW4TjHlJ3b6bZ-W3zSc3t2ZpQfzW6hfWTs7hvCShW7-wK178JxVC6W3RGqg81l0yR1W4MF4sN393l6mW1-vh1f71mVXmW2wdDkH6Vtb30W71mMXs2YVP9XW7BGrGR6_rl2wW4kySfX3l_PN_W2wlm_t2cnJZgN2w8HcWHYBclW5x6w4Z11Kp7vN2QrfmfrMmdNW1mQkGT4-gSLPW2LVlKH6M9CW_W3kV7MV714_cwf2F8ymb04). 
- They fine-tuned DRAGON+’s encoder to increase the probability that it retrieved a given chunk if the chunk improved the LLM’s chance of generating the correct answer. Fine-tuning was supervised for the tasks listed above. Fine-tuning was self-supervised for completion of [37 million text chunks](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWrsK69hFYL4N7QLpjSVmXT4W1FPBGm5d0pkgMDyKmx3qgyTW6N1vHY6lZ3n6W4HCQQ18Zm1FZN4GLm-TdH_LjW1ZTD5L3JqjVTW2qMmGf3BfqwSW46lpST2lvP77W4tytJS98swWMW2XT7gq26GNH-W8Ty1MJ3KKBcKW5tN-118R3Qt0W6-RlzC7RTy3YW6Vvl2q4LMK1VW4fKTTm6TNJBkVwSfrK54hN3cW4NyMQp4M6mJvW3HqrhX25K7M9W375Y257tt7ZkW18J9bk3cnRT-W1wqrn33MHp06W3GnvBM7NWCzxW6WjZVd2kZKGRW2bYdcw50y4yxW5fQK3C2xNrqgf4_60kb04) from Wikipedia and 362 million text chunks from [CommonCrawl](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWrsK69hFYL4N7QLpjSVmXT4W1FPBGm5d0pkgMDyKmd3qgyTW69sMD-6lZ3ptW7ClM-J7HXNtsW5q_jT072YBPwW1QBRLP6q9GwtW9krzp66G4pyQVySCZG94zzMNW18kG3j9b54drW4JPslj5cGrzZVBGh9y7hNb8rW6CtPxx6ZXyp5W3hrK7Y1yrGvCW4qB8wz3frzjDW6QL1pq8hkqMPW51m1CZ2YrCDjN2yd-z-ZG3RhV8NkBK5L3dXhW2KClpH7_5-_sW7k3l1F3YgQQvW9f4yBy6sFM8BW4JcFlS1JF4JLW6vqybz5YHkP9f9ccFSF04). 

**Results:** On average, across four collections of questions from datasets such as [MMLU](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWrsK69hFYL4N7QLpjSVmXT4W1FPBGm5d0pkgMDyKmx3qgyTW6N1vHY6lZ3nDW91b0t36SVR3YW8bF2J-4D8sW6W4K8sqP8ZCW1BW55rkvw4Wh16gW86JKX89bq0lDVssV2m9cSqY5W4CNKHs8ncwLcW5Nr2GT1-HsHLW8LwgYC8gkyN7W3Xdymj679rZpW4b49yM3BDK1qW4RNkF61n1KCtW22HvdZ58zyRnW1qR3ZM41pDDxW7vy8Pf7968twN8PrC2wy96gMW9lxfC06XVRFjV6TsXL5lVcZmW1FCtQB8j95WNW3vJrXd7M9zPfW8sFkYP7F5chwN3FmfGPwxFLTf7GRdLK04)that cover topics like elementary mathematics, United States history, computer science, and law, RA-DIT 65B achieved 49.1 percent accuracy, while the combination of LLaMA 2 65B and DRAGON+ without fine-tuning achieved 45.1 percent accuracy, and LLaMA 2 65B without retrieval achieved 32.9 percent accuracy. When the input included five examples that showed the model how to perform the task, RA-DIT 65B achieved 51.8 percent accuracy, LLaMA 2 65B combined with DRAGON+ achieved 51.1 percent accuracy, and LLaMA 2 65B alone achieved 47.2 percent accuracy. On average, over eight common-sense reasoning tasks such as [ARC-C](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWrsK69hFYL4N7QLpjSVmXT4W1FPBGm5d0pkgMDyKmx3qgyTW6N1vHY6lZ3kJW6FCxwt39C67bN3YDtjWcLhPTW4qk1TS3MhX1BVDbV7H3ZhjsfN3pqLt7F8h4wW3z3dNt4tX2SsW7yxQYV7tGqv-W7_zT-_7mQM7cVPdQLr2CsjLSW6b1-F4402R8LW1FLrrb2n3fj1W5cZk5Y3ptvnWW1Cv9272Xk8nRW8Z3MyL54hVVkW7JjD8c72820BN7vq_VLSBCNqW8p0m4R4rNpXGTTz_v2ywdx2W1WPxLV6WgX2zW119R3L2WrKl7W5dZrWb36wLD5W854Pv_1KwK17f4-CTyP04), which involves common-sense physics such as the buoyancy of wood, RA-DIT 65B achieved 74.9 percent accuracy, LLaMA 2 65B with DRAGON+ achieved 74.5 percent accuracy, and LLaMA 2 achieved 72.1 percent accuracy.  
**Why it matters:** This method offers an inexpensive way to improve LLM performance with RAG.  
**We’re thinking:** Many developers have found that putting more effort into the retriever, to make sure it provides the most relevant text, improves RAG performance. Putting more effort into the LLM helps, too.
     \> From \<[https://www.linkedin.com/feed/](https://www.linkedin.com/feed/)\>     

**Retrieval augmented generation ELI5:** [https://www.lakera.ai/blog/retrieval-augmented-generation](https://www.lakera.ai/blog/retrieval-augmented-generation)  
[https://lnkd.in/gCYEYP49](https://lnkd.in/gCYEYP49)
       
[https://www.anthropic.com/news/contextual-retrieval](https://www.anthropic.com/news/contextual-retrieval)