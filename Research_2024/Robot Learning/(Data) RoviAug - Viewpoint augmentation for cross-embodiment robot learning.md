âž¡ï¸Paper Title: RoVi-Aug: Robot and Viewpoint Augmentation for Cross-Embodiment Robot Learning
 
ðŸŒŸFew pointers from the paper
 
ðŸŽ¯Scaling up robot learning requires large and diverse datasets, and how to efficiently reuse collected data and transfer policies to new embodiments remains an open question.
 
ðŸŽ¯Emerging research such as the Open-X Embodiment (OXE) project has shown promise in leveraging skills by combining datasets including different robots.
 
ðŸŽ¯However, imbalances in the distribution of robot types and camera angles in many datasets make policies prone to overfit. To mitigate this issue, Authors of this paper proposed â€œRoVi-Augâ€, which leverages state-of-the-art image-to-image generative models to augment robot data by synthesizing demonstrations with different robots and camera views.
 
ðŸŽ¯Through extensive physical experiments, they showed that, by training on robot- and viewpoint-augmented data, RoVi-Aug can zero-shot deploy on an unseen robot with significantly different camera angles.
 
ðŸŽ¯Compared to test-time adaptation algorithms such as Mirage, RoVi-Aug requires no extra processing at test time, does not assume known camera angles, and allows policy fine-tuning.
 
ðŸŽ¯Moreover, by co-training on both the original and augmented robot datasets, RoVi-Aug can learn multi-robot and multi-task policies, enabling more efficient transfer between robots and skills and improving success rates by up to 30%.
 
ðŸ¢Organization: [University of California, Berkeley](https://www.linkedin.com/company/uc-berkeley/), [Toyota Research Institute](https://www.linkedin.com/company/toyota-research-institute/), Physical Intelligence
 
ðŸ§™Paper Authors: [Lawrence Yunliang Chen](https://www.linkedin.com/in/lawrence-yunliang-chen/), [Chenfeng Xu](https://www.linkedin.com/in/chenfeng-xu-3ba921169/), [Karthik Dharmarajan](https://www.linkedin.com/in/karthik-dharmarajan/), [Zubair Irshad, PhD](https://www.linkedin.com/in/zubair-irshad/), Richard Cheng, [Kurt Keutzer](https://www.linkedin.com/in/kurtkeutzer/), [Masayoshi Tomizuka](https://www.linkedin.com/in/masayoshi-tomizuka-4b1a5021/), Quan Vuong, [Ken Goldberg](https://www.linkedin.com/in/goldbergken/)
 
1ï¸âƒ£Read the Full Paper here: [https://lnkd.in/gdnksU2C](https://lnkd.in/gdnksU2C)
 
2ï¸âƒ£Project Page: [https://lnkd.in/gMgYadvw](https://lnkd.in/gMgYadvw)
 
3ï¸âƒ£Code: Coming ðŸ”œ
 
ðŸŽ¥ Be sure to watch the attached Demo Video -Sound on ðŸ”ŠðŸ”Š
 
Find this Valuable ðŸ’Ž ?
 
â™»ï¸REPOST and teach your network something new
 
Follow me ðŸ‘£, [Naveen Manwani](https://www.linkedin.com/in/naveen-manwani-65491678/), for the latest updates on Tech and AI-related news, insightful research papers, and exciting announcements
 \> From \<[https://www.linkedin.com/feed/](https://www.linkedin.com/feed/)\>