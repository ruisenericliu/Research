Excited to share our latest work on Visual Object Tracking:  
ðŸŒŠ SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory
 
The Segment Anything Model 2 (SAM 2) has set a high bar for object segmentation, but visual object tracking in challenging scenarios remains a hurdle. Thatâ€™s where SAMURAI comes in. By integrating the motion cues with a motion-aware memory selection mechanism, SAMURAI tackles issues like crowded scenes, fast motion, and self-occlusion without computational overhead!
 
ðŸ’¡ Key Highlights:  
- Zero-shot Generalization: No fine-tuning or retraining required, we use the exact identical models provided by SAM 2. SAMURAI can directly adapts to VOT tasks!  
- SoTA Performance: SAMURAI achieves state-of-the-art performance on various VOT benchmarks including GOT-10k, LaSOT-ext, and NeedForSpeed!
 
From dynamic environments to real-world applications, SAMURAI demonstrates precision, robustness, and efficiency, setting a new standard in visual object tracking.
 
ðŸ”— Learn more:  
ðŸ“„ Paper: [arXiv 2411.11922]( [https://lnkd.in/gHVmi6hS](https://lnkd.in/gHVmi6hS))  
ðŸŒ Project: [SAMURAI Website]( [https://lnkd.in/gY3zjHke](https://lnkd.in/gY3zjHke))  
ðŸ’» Code: [GitHub Repository]( [https://lnkd.in/gXgQmZBp](https://lnkd.in/gXgQmZBp))
 \> From \<[https://www.linkedin.com/feed/](https://www.linkedin.com/feed/)\>