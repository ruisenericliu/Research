12/5:  
Gemini Deep Think Comes out
                

9/30, Tinker released  
[https://app.alphasignal.ai/tc?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJkZXN0aW5hdGlvbiI6InJ1aXNlbmVyaWNsaXVAZ21haWwuY29tIiwiYXNfY2FtcGFpZ25faWQiOiJmZGUyMzY5ZmVkN2M2Mjc1IiwibGlua19pZCI6InFMT1BveUgwYTFJQWVSMVUifQ.BB6vzn03X4gtmbtrRwfH9TjIxQMfliy2r0UsS4FO_mc](https://app.alphasignal.ai/tc?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJkZXN0aW5hdGlvbiI6InJ1aXNlbmVyaWNsaXVAZ21haWwuY29tIiwiYXNfY2FtcGFpZ25faWQiOiJmZGUyMzY5ZmVkN2M2Mjc1IiwibGlua19pZCI6InFMT1BveUgwYTFJQWVSMVUifQ.BB6vzn03X4gtmbtrRwfH9TjIxQMfliy2r0UsS4FO_mc)
 
Claude Sonnet 4.5
            

07/15: Kimi-K2 by Moonshot AI
                
05/12:  
[Google adds two models to Gemini](https://link.alphasignal.ai/mzzxix), enabling 6-hour video analysis and code generation from long video input
 
Tencent unveils HunyuanCustom, rivaling closed-source models with high-fidelity, multi-subject, and audio-synced video generation.
   

Hugging Face releases Open Computer Agent, challenging OpenAI Operator with open models and virtual desktop.
   

05/7 Qwen 3
 
05/06: [NVIDIA unveils an open-source speech to text model](https://link.alphasignal.ai/HbMepR) with song lyric transcription, timestamp accuracy, and 24-minute audio context.
    
04/21: Gemma 3 comes out, quantization aware (can run on 3090)
 
04/18:

- [Microsoft introduces real-time UI agents](https://link.alphasignal.ai/zZVz5f) in Copilot Studio to automate desktop and web workflows without APIs.
- [Google adds Veo 2](https://link.alphasignal.ai/FVSha9) to Gemini API, AI Studio, and mobile app for text-to-video generation.
- [Meta unveils Perception Language Model](https://link.alphasignal.ai/776tec): open 8B vision-language model trained without distillation for video understanding.
- [Ai2 launches DataDecide](https://link.alphasignal.ai/2EBVO7), a public suite of 30K models to predict best pretraining data efficiently.
- [ByteDance announces a video foundation model](https://link.alphasignal.ai/jKCRxL) enabling fine-grained storytelling, camera control, and CGI-tuned realism.   

04/11: [Google unveils open-source Firebase Studio](https://link.alphasignal.ai/0tlXWN): Build full-stack apps from prompts in seconds.
   

04/11:  
[Integrate real-time speech-to-text with sub-second latency](https://link.alphasignal.ai/UMkJ6q) and 90%+ accuracy to your voice stack today.
 
04/05 - Llama 4 release (3 versions, MOE)ï¿¼  
04/03 MoshiVis (Voice2Voice)
 
03/21: Gemini 2.5 Pro - does decent at coding
 
03/26: Gemini Pro
 
03/25: Sesame AI - CSM-1B Voice agent
 
03/17: Aya Vision (multilingual)
 
03/17: Ernie by Baidu [https://link.alphasignal.ai/j6rX6f](https://link.alphasignal.ai/j6rX6f)
 
03/12 Gemma 3
 
03/6 - Mistral OCR model
 
03/06 qwq 32B, manus: [https://huggingface.co/blog/LLMhacker/manus-ai-best-ai-agent](https://huggingface.co/blog/LLMhacker/manus-ai-best-ai-agent)
 
02/28 Phi 4
 
02/27 - Gemini Assistant  
02/25 - Claude 3.7
 
02/21 Helix, Siglip2
 
02/15 - zonos tts
 
02/10 EleGNT [https://machinelearning.apple.com/research/elegnt-expressive-functional-movement](https://machinelearning.apple.com/research/elegnt-expressive-functional-movement)  
01/31 - Tulu 405B, Mistra Small 3  
01/21 - Hunyan 3D 2.0  
01/21 - Deepseek R1 paper released
 
01/16 - Apache 2.0 8B SJT  
01/13:  
01/11 - 350MB is all you need to get near SoTA TTS! ðŸ¤¯  
Meet Kokoro 82M - Apache 2.0 licensed, Text to Speech model, trained on \< 100 hours of aud
    
01/08/2025 Phi 4 (LLM)
    
Homebrew Labs released Ichigo, an opensource Llama 3 based advanced voice mode

12/1:  
Qwen VL 3  
SDPA output gating  
DeepSeek 3.2
 
Bytedance Depth Anything 3
 
Nano Banana 2
 
11/25:  
Google Antigravity - coder
 
11/8:  
Gemini 3 comes out

![CDN media](Exported%20image%2020260222122305-0.png)   
11/8:  
Kimi-K2 Thinking beats GPT 5
 
11/6:  
Futurehouse just announced the launch of its commercial spinout **Edison Scientific**, debuting an autonomous AI research system called **Kosmos**, a tool built to handle full-scale scientific discovery.
 
11/5:  
**Results:** MiniMax-M2 achieved 61 on independent evaluator Artificial Analysisâ€™ Intelligence Index (a weighted average of benchmark performance in mathematics, science, reasoning, and coding), a new high for open weights models, ahead of DeepSeek-V3.2 (57 points) and Kimi K2 (50 points). It trails proprietary models GPT-5 with thinking enabled (69 points) and Claude Sonnet 4.5 (63 points). Beyond that, it excelled in coding and agentic tasks but proved notably verbose. It consumed 120 million tokens to complete Artificial Analysis evaluations,Â tied for highest with Grok 4.
 
10/30:  
Cursor major upgrade
 
FastVLM from Apple
 
10/23:  
Ling 1T:

- **Input/output:** Text in (up to 128,000 tokens), text out (up to 32,000 tokens)
- **Architecture:** Mixture-of-Experts (MoE) transformer, 1 trillion parameters, 50 billion parameters active per token
- **Performance:** Outperformed leading non-reasoning models in 22 of 31 benchmark tests of reasoning, math, coding, general knowledge, and writing.
- **Availability:** Weights free to download fromÂ [HuggingFace](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWQh9n19rhFMW2Q6CpG8VqS7_W97gBTk5DZJqFN8fXWBx3qgz0W7lCdLW6lZ3ltW6p4dRX6VRgT1W7NQl0l25xCgdW6xhn638npY6BW3ZNTBF8Fcpl7W3KsBSH6YMMQ4W2Rnyls1rCG1tW2G5cdc3m61rwW4klHgL5RJjcXN38wQwBjxHXLW62Vnpw4KL0-3W35wm8d3NxR1MW2HWy0t2w0d75W5ykJH66kLFYfW8zP-892SB6YCW36x-pN8-l1PLW8CmBPK3Mw9-kVjlXgf43qp8xVt_wps3P8hHBW67MsVT36LGWGW2XgLrh8djpkLW3sCdYD3sZsj2W12jhqy4B3y1yVgcXw02kbRWlW9jN-806T6h2Yf68Rx0W04) and [ModelScope](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWQh9n19rhFMW2Q6CpG8VqS7_W97gBTk5DZJqFN8fXWBx3qgz0W7lCdLW6lZ3pYW8d4JNJ15hkyVW5bJ5nL3CFKsNW42zq1q8lvZ5gW3QKmj77w7dNkV_3c431C4tDwW7ms4M88tQ5XcN6j22pWNkxlPW25038n6-ZJmWW8nlDjr4N97dcW6bxStg8kK10xW6_Tgw66Qn365W5xqmZ9413YTvW53kszF4jXkpkW49jH8C5CbPK4W99Lk5V5PKW9xW9cq_VZ1Xpy5DW4Bw1qM7R2SsbW7mMx2Z6cfVPMVfPj0w8pJ9ppW7Gbc3P5x1xWRW3n82N06cV2SDW2SkjsQ31JDSwW4gzxcS8fBn7HW56h3yC49l9hcf1lScg804) for commercial and noncommercial uses under the MIT license, API $0.56/$0.112/$2.24 per million input/cached/output tokens via [zenmux.ai](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWQh9n19rhFMW2Q6CpG8VqS7_W97gBTk5DZJqFN8fXWBx3qgz0W7lCdLW6lZ3nXVTZwS47XdFLTW3WdRH6774jD4W2X7th37V69Y5W7b1lCQ2JyvQfW97g5S373H2lLW1LnJLr71tLxfW7qPV2J5tRW_kW1GgJ5C4ldjv-N1gFKZJrDmqWW2Ns20051t9wJVs9Dnf4hDgwMN85gYtXQHQqSW6jcp1R7lhrKmVJGmqy7_vT7RW8zmg3v10f3YmN3MLK2NttRDhW2RRJ-r4LJT44W8nyThm1nrnlgW5bh-gT1QmydfW7jB_Qx4-CLr3W6vd-bW3zwCs9W26Kh3034lQmjW5jXzc28QK4YzN3qhVQ4wKD0Qf9bhm5z04)
- **Undisclosed:** Training data, specific training methods
 
10/16:  
Qwen3 VL 4B and 8B out
 
10/06:  
RF detr
    
09/15:  
[Google introduces VaultGemma, an open-weight model ensuring data protection](https://link.alphasignal.ai/PFGVYS)
 
Differential privacy is a mathematical framework that adds noise during training so that individual training examples cannot be extracted from the model. It ensures that model outputs reveal patterns about the dataset as a whole, while protecting sensitive details about any single data point.
   

09/02:  
â–ª Microsoft debuts open-source text-to-speech model, outperforming Gemini 2.5 and Eleven v3 - VibeVoice  
[Microsoft ships its first in-house AI models, covering speech and text](https://link.alphasignal.ai/bDSIi4)  
Apple introduces FastVLM
    
08/25:  
**How it works:** Magic Cue takes advantage of an updated version of [Gemini Nano](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/MWVCwjf6MFnMvQ3H7nxCdbW8SvcRS5BLH6gN2x86xg5nR3bW50kH_H6lZ3n9N6VzRHvLMfV5W70yNz0774TMDW70jvsb1pqdWnW9kCZgv2DY3qDW6Wzvq73d73k2W2kspjJ91Ygd9N4r0yHy1jlgqW4zT9nM6nYfLLW9fZ62c3l4ZbSW1thBZ91z0gVpW3N5-HT3FP14VW4zM_wg5n5LZPW7r5TLV7DmFSfW3GcSvm39hWTZN3W0TlWQkSFVW48NN1w8LZ49RVLy8th9b1-tpN7wdMYhGwy-8W7mxqPK4P9t0gW6B8Yq92nFrH_W6bJP_p3NlPMtW68SPYZ2W93CcV4-Hzg2gQw2-W8SzPpB2CN9xpW2BFfBx7WvY0bW35DJvN6tGqWqW1M7Nf02cCHTrW41MJ715tmfvLW4yRK6h5Jq8DDW7cqYJ14t51WNW4WsGV027GG63W4H7Dv445dL7Sf6B31mF04) and runs on the Pixel 10â€™s newly upgraded [Tensor G5](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/MWVCwjf6MFnMvQ3H7nxCdbW8SvcRS5BLH6gN2x86yM3qgz0W7Y8-PT6lZ3pJV7T5243-ZKGbW7zJgDy2GM--7VKZ5LS5hhFK0N5BtZxL1K1MYW3SFpr56VG-_gVThG-R8HVdjRVrFH-45Z6T2RW8qtD1J89V8PqW77QFfl57L3lxW5mcY687LsN8kV64_RM3nf84dW7dX-mZ8YRWqTN7pgwNSzSJdmN190Kv31RbmDW6p08w71dzbdPN23HbPv5cLFxW8tgm1K3cPc46Vmn2Zc3fxmXRW6XsPQ97_x_ncW18bKrq2PC-4jN4QG24ZYns88W3SRSlf8cDj2zW2BHl_g13jSSXW1HlfZW4J_nBdW8Tw5cr2m4TYdMbZ-1xHZv7Gf30ZDhb04)Â AI processor. The system tracks user behavior and provides relevant information proactively.
   

08/15:  
DinoV3  
Imagen 4  
Skild Brain
 
08/10:  
Prophet Arena  
[https://www.prophetarena.co/blog/welcome](https://www.prophetarena.co/blog/welcome)
 
08/07:  
GPT 5 comes out
 
08/05:  
[https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/](https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/)
 
08/02:  
AlphaEarth out  
Gemini 2.5 DeepThink out
 
08/01:  
olmOCRâ€¯v0.2.1From out for fast(er) OCR on OCR-bench.
 
07/30:  
[Warp's AI coding agent leaps ahead of Claude Code, hits #1 on Terminal-Bench](https://cc554gl0.r.us-east-2.awstrack.me/L0/https:%2F%2Flink.alphasignal.ai%2FAldwBy/1/010f01985dc3fe4a-c2f8079a-6b43-4cfe-a403-7fe00eb19516-000000/7KCCVLcAyRYijDlwi_qrUBe-gvA=222)  
That said, we have a real advantage on the UI side compared to other tools in the space like [Claude Code](https://thenewstack.io/claude-opus-4-with-claude-code-a-developer-walkthrough/), [Codex CLI](https://thenewstack.io/testing-openai-codex-and-comparing-it-to-claude-code/), or [Gemini CLI](https://thenewstack.io/gemini-cli-googles-challenge-to-ai-terminal-apps-like-warp/). Those products canâ€™t build a WYSWYG editor at all â€” let alone a great code review experience.

\> From \<[https://thenewstack.io/qa-how-warp-2-0-compares-to-claude-code-and-gemini-cli/](https://thenewstack.io/qa-how-warp-2-0-compares-to-claude-code-and-gemini-cli/)\>   
[Alibaba Qwen releases updated Qwen3-30B-A3B](https://cc554gl0.r.us-east-2.awstrack.me/L0/https:%2F%2Flink.alphasignal.ai%2FxiFi4r/1/010f01985dc3fe4a-c2f8079a-6b43-4cfe-a403-7fe00eb19516-000000/D_4dsAF2RVTrW4zm9Fb9PvRQsyc=222)
 
07/22:  
Gemini 2.5 offers conversational segmentation  
Qwen3 Coder out
 
07/18:  
[OpenAI launches ChatGPT Agent, automates tasks with app access](https://cc554gl0.r.us-east-2.awstrack.me/L0/https:%2F%2Flink.alphasignal.ai%2F7n4MCC/1/010f01981e087bd4-9834bc85-0a60-4410-8be7-e53c32aab7fa-000000/10giesVYxRy-On4Xi6SXMaE3Du0=220)  
[Anthropic announces a Claude tool for financial analysis](https://cc554gl0.r.us-east-2.awstrack.me/L0/https:%2F%2Flink.alphasignal.ai%2Ffj9m7O/1/010f01981e087bd4-9834bc85-0a60-4410-8be7-e53c32aab7fa-000000/f-hptg9hAkZJW4E3xuH6ew5RAr4=220)  
[OpenAI, DeepMind researchers propose Chain-of-Thought traces as safety tool](https://cc554gl0.r.us-east-2.awstrack.me/L0/https:%2F%2Flink.alphasignal.ai%2FwMmkJc/1/010f01981e087bd4-9834bc85-0a60-4410-8be7-e53c32aab7fa-000000/d9Mb426oSo9NZdBzgYmMEz6cAr0=220)
   

07/12:  
Grok-4 with agent swarm  
Deepmind hires windsurf
 
07/04:  
Gemma3n (Text, Audio, Vision) - Fine-Tuning  
Blog: [https://lnkd.in/ejsHieKD](https://lnkd.in/ejsHieKD)  
Gemma Recipes: [https://lnkd.in/eN9wdDy5](https://lnkd.in/eN9wdDy5)  
(4B and 2B)  
06/24:  
[ElevenLabs introduces Conversational AI 2.0](https://link.alphasignal.ai/lpzoYI), enabling AI agents to handle multiple languages and personas mid-call.  
**â€º** [ElevenLabs unveils 11.a voice assistant](https://cc554gl0.r.us-east-2.awstrack.me/L0/https:%2F%2Flink.alphasignal.ai%2Fq79NKP/1/010f0197a4dd2c31-225521b1-40e6-4c5f-8837-151c41e554de-000000/LwnZ0wJJ7Ppxs_sABWPRWtFIYt0=216). Lets you create tasks and summarizes your schedule.
 
[11.ai](http://11.ai) supports both voice and text in the same session and detects language automatically

- Plan your day and write tasks to Notion or Linear
- Search Slack messages or emails and summarize key updates
- Create and manage Linear tickets via voice
- Research topics using Perplexity and outline findings
 
**System Architecture**

- Runs on ElevenLabs' Conversational AI platform
- Detects almost 70+ languages automatically for multilingual interaction
- Uses RAG for context-aware responses
- Maintains conversational flow across multi-step interactions
 
Poutine:  
Our 3 B-parameter VLM Poutine scored 7.99 RFS on the official test setâ€”comfortably ahead of every other entry (see figure).  
2025 Waymo Vision-based End-to-End Driving Challenge!
   

06/16:  
Anthropic system detailed: [https://www.anthropic.com/engineering/built-multi-agent-research-system](https://www.anthropic.com/engineering/built-multi-agent-research-system)  
[Building a fully local "deep researcher" with DeepSeek-R1](https://www.youtube.com/watch?v=sGUjmyfof4Q)
 ![Embedded YouTube video](https://www.youtube.com/embed/sGUjmyfof4Q?feature=oembed&autoplay=true)