Excited to share our latest efforts toward increasing the reliability and trustworthiness of generative-AI-powered robots at [hashtag#CoRL](https://www.linkedin.com/feed/hashtag/?keywords=corl&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7250303640576356353) 2024 (upcoming) in Munich! We propose Sentinel, a runtime monitoring framework that detects unknown failures of generative robot policies during deployment.
 
ðŸ”— Quick links:  
arXiv: [https://lnkd.in/eH6VNbyD](https://lnkd.in/eH6VNbyD)  
Site: [https://lnkd.in/ezpyJ_Tg](https://lnkd.in/ezpyJ_Tg)
 
ðŸ“š Why monitor policies?  
Robot imitation learning has experienced a resurgence in recent years, driven by the curation of larger robot learning datasets and the adoption of generative modeling architectures that have shown remarkable success in robotics-adjacent fields (e.g., NLP, Vision). The training of generative robot policies is gradually becoming common practice in both industry and academic labs.
 
However, deploying generalist robots in the real world presents complexity far beyond controlled lab settings. There, even the most powerful generative policies will eventually encounter "out-of-distribution" scenariosâ€”situations outside the models' learned competenciesâ€”where their behavior may become unpredictable.
 
To facilitate scalable and safe deployment of AI-powered robots in the future, our paper advocates for the proactive monitoring of these generative policies, detecting failures as they occur and preventing their downstream consequences. This raises a follow-on question: What can we do with failures once they are detected? Weâ€™re excited to explore this further!
 \> From \<[https://www.linkedin.com/feed/update/urn:li:activity:7250303640576356353/](https://www.linkedin.com/feed/update/urn:li:activity:7250303640576356353/)\>