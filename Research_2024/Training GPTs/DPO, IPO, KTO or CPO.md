DPO, IPO, KTO or CPO? What should you use for RLAIF?ðŸ¤” A new paper compares the performance across three distinct scenarios: (1) keeping the Supervised Fine-Tuning (SFT) part, (2) skipping the SFT part, and (3) skipping the SFT part and utilizing an instruction-tuned model.
 
ð—œð—ºð—½ð—¹ð—²ð—ºð—²ð—»ð˜ð—®ð˜ð—¶ð—¼ð—»  
1ï¸âƒ£Â Models: Zephyr-sft-full, Mistral-7B-v0.1, Mistral-instruct-7B-v0.2  
2ï¸âƒ£Â Datasets: UltraChat for SFT training; UltraFeedback-binarized for alignment method training.  
3ï¸âƒ£Â Fine-Tuning: Apply alignment methods (DPO, IPO, KTO, CPO) to different models depending on the Scenario  
4ï¸âƒ£Â Evaluation: Test the models across 13 benchmarks (MT-Bench, Big Bench, and Open LLM Leaderboard).
 
ð—œð—»ð˜€ð—¶ð—´ð—µð˜ð˜€  
ðŸ’» Tested methods performed better with smaller datasets (Contrary to PPO) ðŸ§® KTO outperforms in tasks like mathematical problem-solving  
ðŸ“ None of the tested methods outperformed SFT in MMLU  
âœ… KTO and IPO outperformed SFT by 17.5% on TruthfulQA  
ðŸ’¬ CPO performed worse than SFT on MT-Bench, indicating weaker performance in dialogue.  
ðŸ§  Tested methods didn't significantly improve model reasoning  
ðŸ¤— Used [Hugging Face](https://www.linkedin.com/company/huggingface/) TRL for fine-tuning
 
Paper: [https://lnkd.in/e4WTwQVR](https://lnkd.in/e4WTwQVR)
 \> From \<[https://www.linkedin.com/feed/](https://www.linkedin.com/feed/)\>