Given a coding problem that’s specified in a prompt, the workflow for a coding agent typically goes something like this: Use a large language model (LLM) to analyze the problem and potentially break it into steps to write code for, generate the code, test it, and iteratively use any errors discovered to ask the coding agent to refine its answer. But within this broad framework, a huge design space and numerous innovations are available to experiment with. I’d like to highlight a few papers that I find notable:

- “[AgentCoder: Multiagent-Code Generation with Iterative Testing and Optimisation](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVHVHq5dsymgW3dY4Ly7x_glXW7HlLKX5gwrJ0N8Vtb7n3qgyTW6N1vHY6lZ3k_W3cpW3J22XN_NW3fDSzm4fHBC0W93c4V23JPqdXVNYG4q6HMMv6W9hDJwx75PW3zW1nV8z82HT49DW5PXRm03V4lmGW3pVsz91cB5WFW8z7XYp60H4VkW43hll25RFcK5VmdG2L7FPjyQW4kP9x36pq3_DVMhG302xDPsZW62ZXBl1hCW12W2S7TsY3xh_HPW1LcTDy4mYP_WW631bcS1K9BNpW9l6XR73wFyghW6nwkqn6Z2cC7W3fgmrq5PKS3zW4-RMdX6YlsChW8LGNWy6xNpGKf2tJg8j04),” Huang et al. (2024). 
- “[LDB: A Large Language Model Debugger via Verifying Runtime Execution Step by Step](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVHVHq5dsymgW3dY4Ly7x_glXW7HlLKX5gwrJ0N8Vtb7n3qgyTW6N1vHY6lZ3nZW2yvF2b6thxtVW8qvq8v8ZP-LLW3Q2tKX18n8XwW44QTWM2-fGPWW6bcxhb6XsHhgW4qnMvD8DQdbBW7YgKxZ2pL9Y6W372LcK8d9JStVSwQBZ6QBLggW1BvY2w1GGf72W8G-F8k8LMd_YW8ZHjQt7W19lSW7FVWGQ4x29K6N88vnjsCbZzWW9kBfry1jx8P1W2JwrYS3z7YwYW57S4Hj3QChB4W2LRtj68Z17tsW4lBGTC77Ch1JVMfSFT4yTyQCW8q5lxq1Y-mq9W4VdFhg3JQXSbf4Npc3b04),” Zhong et al., (2024). 
- “[SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVHVHq5dsymgW3dY4Ly7x_glXW7HlLKX5gwrJ0N8Vtb7n3qgyTW6N1vHY6lZ3krW2gqPCc3lNzN5W3ljfBt80-QwpW7K82rN3xhPhpW53W5JN2s93kGW2r1RPj71ySHqW1BfY4v8ZXBvQW8vLs_n11ytJJW6764h87w47qFV9bXNp8vx7M9W7TxMrY4vcbY9N6fQb_ytg4mxW58SXVZ68BgNBV3VbRs2ZDvvRW6THGYn8lH4XYW5DG3Gg6p2ClLW4fS3FK7xCskwN7dYYB0vN-2NW8vbrTQ385jlRW982tDf8J0KxrMN704BYZJn1N8hLBr9-WllbN8838sNMrQhTdBdkhF04),” Yang et al. (2024).
 \> From \<[https://mail.google.com/mail/u/0/#inbox/FMfcgzQVxHcqqGKfghKWgkQzGWbSnDGg](https://mail.google.com/mail/u/0/#inbox/FMfcgzQVxHcqqGKfghKWgkQzGWbSnDGg)\>     

How can we test the code without requiring the user to write test cases? In a multi-agent system, each “agent” is an LLM prompted to play a particular role. An interesting result from [AgentCoder](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVHVHq5dsymgW3dY4Ly7x_glXW7HlLKX5gwrJ0N8Vtb7n3qgyTW6N1vHY6lZ3kyW8RXchq3P_JyvW6HXt0X3mFlBTW4pZKlP9f6fRxW5RhlbF8zy6WwW1fLZWV5P2ghMW89PTKm1j75ZHW49D4H218TV1PW58Fp7Q5RVNFSW4blkHW1M62kBW1RMQGh1V8WrfW4sC5ZK10g0SkW6nwklq6LsbQ2W1FDf1F3SpkKhW65tbj27G3NpzW2LF28j393wgnW4Pd_H_3BKPgBW7v5jBz5vYtnHN6NJy5TZrPr2W9lzlLY8nFypSW3m3XT-2bkf5pW858PCG3MByvXW5rd8g05wyDQnd33h7Y04) shows that having separate agents for writing code and generating tests results in better performance than letting a single agent do both tasks. This is presumably because, if the agent writing the code is also responsible for writing the tests, the tests might be influenced by the code and fail to consider corner cases that the code does not cover.   
   
When people think of testing code, many initially think of output testing, in which we see if the code generates the correct outputs to a specific set of test inputs. If the code fails a test, an LLM can be prompted to reflect on why the code failed and then to try to fix it. In addition to testing the output, the LDB method is helpful. LDB steps through the code and presents to the LLM values of the variables during intermediate steps of execution, to see if the LLM can spot exactly where the error is. This mimics how a human developer might step through the code to see where one of the computational steps went wrong, and so pinpoint and fix the problem. 

![OPENDEVIN](Exported%20image%2020260222203135-0.png)

A lot of agentic workflows mimic human workflows. Similar to other work in machine learning, if humans can do a task, then trying to mimic humans makes development much easier compared to inventing a new process. However, the authors of [SWE-agent](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVHVHq5dsymgW3dY4Ly7x_glXW7HlLKX5gwrJ0N8Vtb7n3qgyTW6N1vHY6lZ3mSW6r4NNz1-DSJ6W9b80VP7vfByFW2fP2nD2-X7mBW6T1cM44g1cL0VGQctV6bqpLhW1Bwx6V743vvlW6t8ZTG9bSzWhW6513bm8bHlnQW5DcN435LDfCdW3WhvCm4KmjtYW524LN73Fj8qZW88N5x89c2cl6W6lVpMV910D0MN1jv1SjD00C4W2-b2b36DMtj9W6ZzHsN37mrQ_W1C_XBZ86ysDrW8vsfQ04ygG4TW7KR1kT7XF1vhW8Dwyjx6vrM2xM4rt5MRQJR_N7F3v7RpmB55f2FlZGF04) noticed that many tools that humans use for coding are very inefficient for agents. For example, giving an agent access to a bash shell and having it find a piece of code by executing numerous cd, ls, and cat commands is inefficient, even though humans can do this rapidly. Similarly, visual coding editors like VSCode, emacs, and vim are easy for humans to use, but hard for LLMs (or LMMs) to navigate. Because agents interact with computers differently than humans do, the authors found that building special-purpose tools (functions) to let an agent search, view, and edit codebases resulted in better performance. 
 
One reason research into coding agents is making rapid progress is that their performance can be evaluated automatically and reliably. With benchmarks like HumanEval, MBPP, and SWE-bench, researchers can try out an idea and automatically test how often it generates correct code. In contrast, even though there’s considerable activity on AI research agents that search the web and synthesize an article (I’ve enjoyed using the open-source [STORM](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVHVHq5dsymgW3dY4Ly7x_glXW7HlLKX5gwrJ0N8Vtb7n3qgyTW6N1vHY6lZ3mgW1rxphD6BjdDJW8dNwc258814TN9hhJSp541JrW1YrzBH4Y8zdGW66Mjbp7rq4s0VNzcgr7n3LX6W1wRp2D8vMP9LW1JXFTk2Gh6c_W3p8tlw7VxmstW5TWJ2h5X8VzdW71NhhB6rtjwcW2ym6xX7bq-WLW61t-2s5JYsr0W1Kjm2d1jTmfsW2MLvRr8d8D9tW55QHZn6fZb5hW67swyl7ZdyBwW2Z6wRj5hNlx-W79ps4V1pm-_4W5h1T5T6JTMZVN5fFt2-pmlwxW2n-DmG1pt-gZf47t29b04) system by Stanford's Yijia Shao et al.), they are [hard to evaluate](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVHVHq5dsymgW3dY4Ly7x_glXW7HlLKX5gwrJ0N8Vtb8g3qgyTW8wLKSR6lZ3nzW4j-l9W3kP4B1W25438n2nffRbW5KXPRZ5tRxdlN3YBgT0lRqk_W9fPfkG78chMHW4dvK-j55FgxxW2_JF7s4Dhx0CW74fgwy3Tn0yXW3Mw2HJ3VYY3KW3t9scz8cBw-5W179bdL7zSV8zN1wlmYSW83ycW7Y-1v91J-9s5W5M2zDQ2SzKyXW9bW_1s5B0JSqW1HzcdM8BgG7MV1dTRW1Cw9TgW5sRPwP76dDQhW17H5nM88s9MRVqp-1p9lgs8rN1qTSbJMNgTfW14Xwr91hsc5SW7wysTM4PH2ppW1c2fQK3sccxrW17mNMM8zljz1W7PSZ5q2mNPNWV7y9Fz3Qj21NW7_DTL82Q2mFDf13RV4x04) and this makes progress harder. 
 \> From \<[https://mail.google.com/mail/u/0/#inbox/FMfcgzQVxHcqqGKfghKWgkQzGWbSnDGg](https://mail.google.com/mail/u/0/#inbox/FMfcgzQVxHcqqGKfghKWgkQzGWbSnDGg)\>