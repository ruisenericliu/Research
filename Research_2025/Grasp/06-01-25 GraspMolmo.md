ğŸš€ Introducing GraspMolmo: Generalizable Task-Oriented Grasping via Large-Scale Synthetic Data
 
How should a robot grasp a knife? Well, it dependsâ€”to chop or to hand it over?
 
GraspMolmo bridges that semantic gap by fusing spatial reasoning and language understanding to predict task-appropriate grasps from natural language and a single RGB-D frame.
 
ğŸ›  How to Use GraspMolmo
 
1ï¸âƒ£ Build PRISM  
â€¢ 379k samples, 2,356 objects, 10k scenes  
â€¢ Uses ShapeNet-Sem + ACRONYM grasps  
â€¢ Tasks paired via GPT-4o + human filtering
 
2ï¸âƒ£ Fine-Tune Molmo  
â€¢ Trained on PRISM + TaskGrasp-Image  
â€¢ Grasp described using natural language + pixel-level grounding
 
3ï¸âƒ£ Run Inference  
â€¢ Input: RGB-D image + task prompt  
â€¢ Output: Task-aware pixel â†’ 6-DoF grasp proposal
 
4ï¸âƒ£ Deploy in Real World  
â€¢ Example: â€œdump flowers outâ€ â†’ correctly grasps vase edge, not body
 
Project: [https://lnkd.in/g94nsd2a](https://lnkd.in/g94nsd2a)