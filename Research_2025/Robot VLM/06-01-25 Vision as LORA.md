Bring Vision to Your Blind LLM! ðŸ‘€
 
Ever wished your text-only LLM could understand images?  
"Vision as LoRA (VoRA)" makes it possible, efficiently and affordably! ðŸš€
 
VoRA transforms any LLM into a Multimodal Large Language Model (MLLM) using only vision embeddings and LoRA layers, eliminating costly external vision encoders. The magic? Block-wise distillation transfers visual knowledge from a pre-trained ViT directly into LoRA, boosted by bi-directional attention masks that capture full image context.
 
âœ¨ Why VoRA stands out:  
â€£ Cost-effective: Only lightweight vision embeddings and LoRA are trained.  
â€£ Efficient distillation: Block-wise learning from a Vision Transformer (ViT).  
â€£ Context-aware: Bidirectional masking ensures that all image patches within the same input are simultaneously visible, enhancing global image understanding
 
ðŸš€ Results:  
â€£ Matches conventional multimodal models with significantly fewer resources.  
â€£ Seamlessly integrates visual capabilities without inflating inference costs or memory.
 
ðŸ”® What's Next?  
Why stop at vision? Imagine enabling your favorite LLM to process audio or even videos, turning it into a true multimodal marvel!
 
Dive deeper into the paper here ðŸ‘‰ [https://lnkd.in/gREbavfu](https://lnkd.in/gREbavfu)
 \> From \<[https://www.linkedin.com/my-items/saved-posts/](https://www.linkedin.com/my-items/saved-posts/)\>