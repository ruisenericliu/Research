Excited to introduce NORA (Neural Orchestrator for Robotic Autonomy) â€” a Vision-Language-Action (VLA) model tailored for embodied tasks. NORA is a small (~3B parameter) VLA model built on the Qwen2.5-VL backbone and trained on OXE. We developed multiple versions of NORA to evaluate which configurations perform best across different robot control frequencies, from low to high.
 
NORA achieves state-of-the-art performance on several real-world and simulated tasks.
 
Kudos to the entire team Chia Yu Hung Sun Qi, Peng Fei Hong U-Xuan Tan Navonil Mazumder. It was not possible without the huge support from the Lambda labs.
 
Project webpage: [https://lnkd.in/g55hyCBD](https://lnkd.in/g55hyCBD)  
We open-sourced the entire project here: [https://lnkd.in/gNU7ncvW](https://lnkd.in/gNU7ncvW)
 
Find NORA in action here ðŸ‘‡
 \> From \<[https://www.linkedin.com/my-items/saved-posts/](https://www.linkedin.com/my-items/saved-posts/)\>