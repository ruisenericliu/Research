**What's new:** Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov at Carnegie Mellon University proposed [Generating Images with Large Language Models](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VW138h4C2R6yW3zMpJV54DLs0W4PtzZh586rgvN3rtP6P3qgyTW6N1vHY6lZ3q5N6VY1mgm933sW5798cv6zYR9xW7fZp6d7kdJm-W2XSrSG9g5Z-2W4Q7ys83G-F07W7Glzws8sTdcPW989m1f4J6fjDW76sfrC7zGsDGW3LqQbx5268J1W8W2HYH3F-ZMjW550k5B7V6p3KW3qQMKp8NxfxHW7gR5P84cqG8_W7KvXbv15mX3yW7BHGmP93N3QQVswQK63MPknqW4flMtG6gyDr_W3p61GC80nXlvW99nZnt8bkWYBW3kL5M43FmLXlW7WG5rk6SfHVYW4TK0WY3BH8Xpdxw66x04)Â (GILL), a training method that enables a large language model and a text-to-image generator to use both text and images as either input or output. Given text and/or image input, it decides whether to retrieve existing images or generate new ones.  
**Key insight:**Models like CLIP and ImageBind map text and image inputs to a similar embedding space, so closely related text and images have similar embeddings. This approach enables a large multimodal model to process both data types. Text outputs, too, can be mapped to the same embedding space, so an image decoder, such as a diffusion model, can use them to produce images or an image retriever to retrieve images.
 
**How it works:**The authors used a pretrained [OPT](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VW138h4C2R6yW3zMpJV54DLs0W4PtzZh586rgvN3rtP6P3qgyTW6N1vHY6lZ3lcVWLMDN6Gl1d-W4RmX2B4nVGgLW4CWz4F2-bbJdW3vpS1b93HSB3W52F4c11PnL_zW2nGLYp1kN6wfN1hjMzNhn2tRVQghZ_9kTlDQW73x51g281mp2W1D6B_037z01DW8-nW0l8SyQWqW5P80l37MPyqyN1_3VYB4zpbmW919STj7g8r1wN5w9HcktRlMzW83kvxd6G9JSvW4RcBv78D8CPrW11kJcy5LlVPxW88fpC26bnqxFVYKzc96K05CqW13Lmnc7WsCHJW4JY8vf6cDLmWf6FKCvn04) large language model, [ViT-L](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VW138h4C2R6yW3zMpJV54DLs0W4PtzZh586rgvN3rtP6P3qgyTW6N1vHY6lZ3mZW9kdvzF3T-dvhW6lQF3R3_WbqvVKbzDf19R2J_W3CvG7R6tZQTXW2-5clj91ZzDgW3L3f9J9cYxSKW7wtx7C4BptXwW32VQNq79Z8LKN5c10kQBhBWTW54FT9-3pLr-CMMP4XzvJZZRW1xg5Sf8D_Wd5W3j27y454HT6QW1YD9x58lndrSW8XZHgQ55ww1tW2H29_q7sT5BhW3F-YGf4yYkH6W6KYcb54lSR87W3SkynG4bC9lVW4kb44n7jnFyFW1jtcHH8yL8wWW2mf8R26-NdwKf8nzg2d04) image encoder (taken from CLIP), and pretrained Stable Diffusion text-to-image generator. The authors trained ViT-L to map its embeddings to those produced by OPT. They trained OPT to recognize prompts that request an image and enabled the system to either generate or retrieve images. Finally, a separate linear classifier learned whether to retrieve or generate images.