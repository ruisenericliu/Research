[https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)
 
Deepseekv3 [https://www.linkedin.com/posts/philipp-schmid-a6a2bb196_yesterday-the-best-open-model-to-date-was-activity-7278313766679658498-6BCl?utm_source=share&utm_medium=member_ios](https://www.linkedin.com/posts/philipp-schmid-a6a2bb196_yesterday-the-best-open-model-to-date-was-activity-7278313766679658498-6BCl?utm_source=share&utm_medium=member_ios)
 
New small hybrid model from [NVIDIA](https://www.linkedin.com/company/nvidia/) has been announced! Hymba is a 1.5B hybrid Mamba x Attention Model that outperforms other small LLMs like [Meta](https://www.linkedin.com/company/meta/) 3.2 or SmolLM v2 being trained on only 1.5T Tokens. ðŸ¤¯
 
It uses a new hybrid architecture with Mamba and Attention heads running in parallel with additional meta tokens (learnable tokens prepended to every prompt), to improve the efficacy of the model. ðŸ‘€
 
It shares the KV cache between 2 layers and between heads in a single layer. It has 16 SSM states and 3 full attention layers; the rest are sliding window attention. It also uses FlexAttention from Pytorch 2.5! ðŸ”¥
 
Repository: [https://lnkd.in/e47xCPcS](https://lnkd.in/e47xCPcS)
 
The model weights are coming soon to [Hugging Face](https://www.linkedin.com/company/huggingface/), and it can be used commercially.
 
Hunyuan Large
   

LLM2CLIP from [Microsoft AI](https://www.linkedin.com/company/microsoft-ai/) - Leverage LLMs to train ultra-powerful CLIP models! Boosts performance over the previous SOTA by ~17%
   
- Nemotron
- [https://huggingface.co/collections/nvidia/llama-31-nemotron-70b-670e93cd366feea16abc13d8](https://huggingface.co/collections/nvidia/llama-31-nemotron-70b-670e93cd366feea16abc13d8) - Liquid AI Unveils [New Architecture forÂ Language Models,](https://link.alphasignal.ai/1KzXLP) outperforming Llama 3.2 in similar size classes. 
**LLM history:** [https://www.nytimes.com/2023/12/05/technology/ai-chatgpt-google-meta.html](https://www.nytimes.com/2023/12/05/technology/ai-chatgpt-google-meta.html)
 
MistralAI publishes a [Mutli-agent role-playing](https://y1mnw3w8.r.us-east-1.awstrack.me/L0/https:%2F%2Flink.alphasignal.ai%2Foohpde/1/01000191713629df-b64a1432-6b52-47c7-9ca0-b6798d38e3e1-000000/lTKSmhUVSEtaTaIzRcm9ED1pKu8=388) and knowledge graph construction example.
 
New paper [introduces "re-read" prompt method](https://link.alphasignal.ai/f9Qv5j), significantly enhancing LLM performance without model changes.
 
Bayesian oracle: [https://arxiv.org/abs/2408.05284](https://arxiv.org/abs/2408.05284)
 
New paper [introduces "re-read" prompt method](https://link.alphasignal.ai/f9Qv5j), significantly enhancing LLM performance without model changes.
 
Qwen 2.5: [https://qwenlm.github.io/blog/qwen2.5/](https://qwenlm.github.io/blog/qwen2.5/)
 
DataGemma: [https://blog.google/technology/ai/google-datagemma-ai-llm/](https://blog.google/technology/ai/google-datagemma-ai-llm/)
   

Better Phi: [https://huggingface.co/microsoft/Phi-3.5-mini-instruct](https://huggingface.co/microsoft/Phi-3.5-mini-instruct)  
MindSearch: [https://arxiv.org/abs/2407.20183](https://arxiv.org/abs/2407.20183)  
ADAS: [https://arxiv.org/pdf/2408.08435](https://arxiv.org/pdf/2408.08435)
 
LLM pruning example:  
[https://developer.nvidia.com/blog/how-to-prune-and-distill-llama-3-1-8b-to-an-nvidia-llama-3-1-minitron-4b-model/](https://developer.nvidia.com/blog/how-to-prune-and-distill-llama-3-1-8b-to-an-nvidia-llama-3-1-minitron-4b-model/)
 
Core ML: Is Inductive Bias necessary? [https://huggingface.co/papers/2406.09415](https://huggingface.co/papers/2406.09415)
 
MAD- Self Consuming Generative Models: [https://arxiv.org/pdf/2307.01850](https://arxiv.org/pdf/2307.01850)
      

Speech to speech  
[https://www.linkedin.com/posts/vaibhavs10_introducing-speech-to-speech-modular-ugcPost-7229794354976972801-pcvc?utm_source=share&utm_medium=member_ios](https://www.linkedin.com/posts/vaibhavs10_introducing-speech-to-speech-modular-ugcPost-7229794354976972801-pcvc?utm_source=share&utm_medium=member_ios)
   

A comprehensive overview of LLMs: [https://arxiv.org/pdf/2307.06435](https://arxiv.org/pdf/2307.06435)
 
Karpathy's [one-hour intro lecture](https://link.alphasignal.ai/TSqKTb) on Large Language ModelsÂ 
 
[Training LLMs from scratch in the wild: https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness](http://Training%20LLMs%20from%20scratch%20in%20the%20wild:%20https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness)
 
[https://medium.com/@plthiyagu/calculate-gpu-requirements-for-your-llm-training-7122a3700547](https://medium.com/@plthiyagu/calculate-gpu-requirements-for-your-llm-training-7122a3700547)
   

HippoRAG: [https://arxiv.org/abs/2405.14831](https://arxiv.org/abs/2405.14831) (long memory)
 
SmoLLM: [https://huggingface.co/blog/smollm](https://huggingface.co/blog/smollm)
 
QwenV2 Technical Report: [https://huggingface.co/papers/2407.10671](https://huggingface.co/papers/2407.10671)
 
[https://www.linkedin.com/posts/philipp-schmid-a6a2bb196_q-galore-released-q-galore-is-a-memory-efficient-activity-7217543741731975168-y7VA?utm_source=share&utm_medium=member_ios](https://www.linkedin.com/posts/philipp-schmid-a6a2bb196_q-galore-released-q-galore-is-a-memory-efficient-activity-7217543741731975168-y7VA?utm_source=share&utm_medium=member_ios)
 
Multi-token prediction: [https://arxiv.org/abs/2404.19737?utm_source=twitter&utm_medium=organic_social&utm_content=image&utm_campaign=research](https://arxiv.org/abs/2404.19737?utm_source=twitter&utm_medium=organic_social&utm_content=image&utm_campaign=research)
 
RankRAG: [https://arxiv.org/pdf/2407.02485](https://arxiv.org/pdf/2407.02485)
 
LORA for training instead of fine tuning? [https://minyoungg.github.io/LTE/](https://minyoungg.github.io/LTE/)
 
1 bit tokenization: [https://arxiv.org/abs/2402.17764](https://arxiv.org/abs/2402.17764)
 
Context and tokenization [https://www.linkedin.com/pulse/overcoming-limitations-context-size-language-models-solutions-lagana-chsmc?utm_source=share&utm_medium=guest_mobile_web&utm_campaign=copy](https://www.linkedin.com/pulse/overcoming-limitations-context-size-language-models-solutions-lagana-chsmc?utm_source=share&utm_medium=guest_mobile_web&utm_campaign=copy)
 
Grammar based prompting - [https://arxiv.org/abs/2305.19234](https://arxiv.org/abs/2305.19234)
 
**Thoughts on Network hallucinations:** [http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
 
Sparsity in LLMs WITH improvement? [https://arxiv.org/abs/2312.13558](https://arxiv.org/abs/2312.13558)
 
**LLM Rules - Hijacking SOTA LLMs:** [https://huggingface.co/papers/2311.04235?utm_source=digest-papers&utm_medium=email&utm_campaign=2023-11-09](https://huggingface.co/papers/2311.04235?utm_source=digest-papers&utm_medium=email&utm_campaign=2023-11-09)
 
Mixture of Experts:  
Papers to read:
 
- The Sparsely-Gated Mixture-of-Experts Layer (2017): [https://lnkd.in/gCVTzZWd](https://lnkd.in/gCVTzZWd)  
- GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding (2020) [https://lnkd.in/gTSK3GvA](https://lnkd.in/gTSK3GvA)  
- MegaBlocks: Efficient Sparse Training with Mixture-of-Experts (2022): [https://lnkd.in/gNYsYrKC](https://lnkd.in/gNYsYrKC)  
- Mixture-of-Experts Meets Instruction Tuning (2023): [https://lnkd.in/gBAZPJQu](https://lnkd.in/gBAZPJQu)
 
LLMs still can't plan  
[https://ar5iv.labs.arxiv.org/html/2206.10498](https://ar5iv.labs.arxiv.org/html/2206.10498)
 
But what about code as policies?ï¿¼[https://code-as-policies.github.io/](https://code-as-policies.github.io/)
 
**AlphaCode Technical Report -** [https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf](https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf)
 
**Chain of Code - Reasoning chain:** [https://arxiv.org/abs/2312.04474](https://arxiv.org/abs/2312.04474)
 
Lessons from Autonomous Driving that apply:  
[https://www.tidepool.so/2023/11/08/how-lessons-from-self-driving-can-improve-llms/](https://www.tidepool.so/2023/11/08/how-lessons-from-self-driving-can-improve-llms/)
 
**Graph meets LLM Survey:** [https://arxiv.org/pdf/2311.12399.pdf](https://arxiv.org/pdf/2311.12399.pdf)
   
            

LLaVA-01 (Let Vision Language Models Reason Step by Step  
Llama-3.2V(11B) + reasoning outperforms Gemini Pro 1.5, GPT 40 mini, Llam 3.2V(90B)
                  

Falcon Mamba: Fitting full attention on a standard scale GPU  
[https://huggingface.co/blog/falconmamba](https://huggingface.co/blog/falconmamba)
       
Chessformer â€“ Transcendence: [https://arxiv.org/html/2406.11741v1](https://arxiv.org/html/2406.11741v1)  
Is this any different than reinforcement learning?
   

Gemma 2-2B-it blog: [https://huggingface.co/blog/gemma2](https://huggingface.co/blog/gemma2)  
Should be on par with Phi-3-Medium-4k-Instruct  
Deployment on Nano: [https://www.linkedin.com/pulse/gemma-2-2b-little-gem-running-offline-your-pocket-asier-arranz-tc7xf/?trackingId=3jafVz%2BwSR2yZE9zkHd4IQ%3D%3D](https://www.linkedin.com/pulse/gemma-2-2b-little-gem-running-offline-your-pocket-asier-arranz-tc7xf/?trackingId=3jafVz%2BwSR2yZE9zkHd4IQ%3D%3D)
       
New Karpathy NanoGPT variant trains twice as fast, [achieving GPT-2 quality](https://link.alphasignal.ai/DfVSe6).  
Karpathy nano LLAMA: [https://github.com/karpathy/nano-llama31](https://github.com/karpathy/nano-llama31)
 
Are LLMs stochastic parrots?  
[https://www.linkedin.com/posts/timothy-nguyen-43b273138_excited-that-my-new-paper-on-understanding-activity-7217920715432341504-jXL3?utm_source=share&utm_medium=member_ios](https://www.linkedin.com/posts/timothy-nguyen-43b273138_excited-that-my-new-paper-on-understanding-activity-7217920715432341504-jXL3?utm_source=share&utm_medium=member_ios)
    
Memory requirements: [https://discuss.huggingface.co/t/llama-7b-gpu-memory-requirement/34323/4](https://discuss.huggingface.co/t/llama-7b-gpu-memory-requirement/34323/4)  
Can map: [https://discuss.huggingface.co/t/running-mistral-7b-instruct-v0-2-on-multiple-gpus/76804](https://discuss.huggingface.co/t/running-mistral-7b-instruct-v0-2-on-multiple-gpus/76804)  
[https://huggingface.co/docs/accelerate/usage_guides/big_modeling](https://huggingface.co/docs/accelerate/usage_guides/big_modeling)  
[https://stackoverflow.com/questions/75459172/loading-a-huggingface-model-on-multiple-gpus-using-model-parallelism-for-inferen](https://stackoverflow.com/questions/75459172/loading-a-huggingface-model-on-multiple-gpus-using-model-parallelism-for-inferen)  
- Llama 3.1 has been released with 8B, 70B and a MASSIVE 405B variant  
- You roughly need 16 GB GPU memory to run the 8B variant smoothly
 \> From \<[https://www.linkedin.com/feed/](https://www.linkedin.com/feed/)\>   
[https://kipp.ly/transformer-inference-arithmetic/](https://kipp.ly/transformer-inference-arithmetic/)
 
local-gemma v0.2 â†’ 150 tok/s GPT-3.5-quality model running locally on a consumer GPU (RTX4090) ðŸ˜Ž You just need two bash commands: one to install, another to run.
 
```  
pipx install local-gemma"[cuda]"  
local-gemma --model 2b --preset speed  
```
 
How did we get to 150 tok/s? ðŸ¤”
 
On an RTX4090, the 9B model has a throughput of ~25tok/s in BF16. Swapping to 2B gets us ~40tok/s, with a small penalty on modeling quality. 40tok/s is far from 150tok/s. The trick is to enable `torch.compile` on gemma 2, added in the latest ðŸ¤— transformers version (v4.44)
 
Using `torch.compile` with `mode="reduce-overhead"` will enable CUDA graphs, which is the single best thing you can do for the throughput of your LLM. Pair it up with a static-shaped KV cache to avoid recompilations et voilÃ , massive speedups ðŸš€
 
We believe we can further push the throughput numbers with compilation + quantization, stay tuned ðŸ«¡ BTW: the 2B model is perfect to be used as an assistant for the 27B model, increasing its throughput! (enabled in local-gemma v0.2)
   

From LLAMA 3.1 report:
 
1. Roberta-class models and Llama 2 were used to clean and assess the raw training data: to classify languages, to identify human language text and software code, to identify rare text vs. bland commonalities, etc.
   

2. Llama 3 was used to generate training data for supervised fine tuning. For example, for "rare" situations like code or mathematics; the generated code was then tested by execution. Techniques like chain of thought were used to generate extended examples, which could then be subsequently edited or simplified (cf Orca). Etc.
   

3. Reward models were trained and iteratively used to assess, select, and refine new supervised training data, cycled with DPO preference training in waves at very low learning rates.